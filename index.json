[{"categories":null,"content":"Just recently the network automation folks witnessed a great library to be ported from Python to Go - scrapligo. Been working on learning go a bit and have published scrapligo https://t.co/NDXQ6khxCr -- still a work in progress, but has been a fun learning experience! Check it out and let me know what ya think! ü§† ‚Äî Carl Montanari (@carlrmontanari) May 19, 2021 For me personally this was a pivotal point because with scrapligo the Go-minded netengs can now automate their networks with a solid and performant library. One of the things that scrapligo packs is, of course, the ability to reliably talk to the network devices using the same command line interface as a human would normally do. That means that scrapligo would send and receive the pieces of data that an operator would send/receive if they were connected with a terminal over SSH. As you may very well be aware, the typical output that a network device produces for a given command is unstructured, meaning that it is not presented in a way that can be effortlessly parsed by a machine. # output of a `show system information` command from Nokia SR OS =============================================================================== System Information =============================================================================== System Name : sros System Type : 7750 SR-1 Chassis Topology : Standalone System Version : B-20.10.R3 Crypto Module Version : SRCM 3.1 System Contact : System Location : System Coordinates : System Up Time : 8 days, 00:24:27.04 (hr:min:sec) If we were to send show system information command with scrapligo towards a Nokia SR OS device, we would have not be able to get, say, the device version right away, since the response is basically the unstructured blob of text as the program sees it. What can we do about it? ","date":"2021-06-09","objectID":"/2021/network-automation-options-in-go-with-scrapligo/:0:0","tags":["scrapli","textfsm","netconf"],"title":"Network automation options in Go with scrapligo","uri":"/2021/network-automation-options-in-go-with-scrapligo/"},{"categories":null,"content":"use NETCONF/gNMI/APIIn an ideal world, you would have stopped reading this post, because ALL your devices were equipped with some kind of programmatic interface that returns structured data. Like in the example above we connect to the SR OS node with scrapligo netconf subsystem and retrieve back a result of a NETCONF Get operation. We then can run a query on this XML document we received and get the data out just nicely. package main import ( \"fmt\" \"strings\" \"github.com/antchfx/xmlquery\" \"github.com/scrapli/scrapligo/driver/base\" \"github.com/scrapli/scrapligo/netconf\" \"github.com/scrapli/scrapligo/transport\" ) func main() { d, _ := netconf.NewNetconfDriver( \"clab-scrapli-sros\", base.WithAuthStrictKey(false), base.WithAuthUsername(\"admin\"), base.WithAuthPassword(\"admin\"), base.WithTransportType(transport.StandardTransportName), ) d.Open() r, _ := d.Get(netconf.WithNetconfFilter(` \u003cstate xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"\u003e \u003csystem\u003e\u003cversion\u003e\u003cversion-number/\u003e\u003c/version\u003e\u003c/system\u003e \u003c/state\u003e`)) doc, _ := xmlquery.Parse(strings.NewReader(r.Result)) ver := xmlquery.Find(doc, \"//version-number\") fmt.Println(ver[0].InnerText()) d.Close() } Output: ‚ùØ go run netconf.go B-20.10.R3 Unfortunately we are not yet there, we have thousands of access devices in Service Providers network which do not have any of the fancy interface. We have Enterprise networks running decade old gear. And we also live in a harsh world where even if the Network OS has one of those fancy interface, the level of information you can query via them is not on par with what you can do over CLI. ","date":"2021-06-09","objectID":"/2021/network-automation-options-in-go-with-scrapligo/:1:0","tags":["scrapli","textfsm","netconf"],"title":"Network automation options in Go with scrapligo","uri":"/2021/network-automation-options-in-go-with-scrapligo/"},{"categories":null,"content":"on-box JSON outputThe next best thing is to leverage the device‚Äôs ability to present the output as JSON. Then you can capture this output over SSH and let your JSON parser to do it‚Äôs thing. For example, on EOS every show command can be represented as a JSON blob: ceos\u003eshow inventory | json { \"fpgas\": {}, \"storageDevices\": {}, \"xcvrSlots\": {}, \"subcompSerNums\": {}, \"portCount\": 3, \"switchedBootstrapPortCount\": 2, \"managementPortCount\": 1, \"dataLinkPortCount\": 0, \"emmcFlashDevices\": {}, \"cardSlots\": {}, \"internalPortCount\": 0, \"powerSupplySlots\": {}, \"fanTraySlots\": {}, \"systemInformation\": { \"name\": \"cEOSLab\", \"description\": \"cEOSLab\", \"mfgDate\": \"\", \"hardwareRev\": \"\", \"hwEpoch\": \"\", \"serialNum\": \"\" }, \"unconnectedPortCount\": 0, \"switchedPortCount\": 0, \"switchedFortyGOnlyPortCount\": 0 } and with this tiny scrapligo program you can easily retrieve all the data from this output: package main import ( \"encoding/json\" \"fmt\" \"github.com/scrapli/scrapligo/driver/base\" \"github.com/scrapli/scrapligo/driver/core\" \"github.com/scrapli/scrapligo/transport\" ) func main() { d, _ := core.NewCoreDriver( \"clab-scrapli-ceos\", \"arista_eos\", base.WithAuthStrictKey(false), base.WithAuthUsername(\"admin\"), base.WithAuthPassword(\"admin\"), base.WithTransportType(transport.StandardTransportName), ) d.Open() // send show command and ask to output it as JSON r, _ := d.SendCommand(\"show inventory | json\") // imagine, that the structure that this output can be parsed into is unknown to us // thus we will use a map of empty interfaces to dynamically query data after var jOut map[string]interface{} json.Unmarshal(r.RawResult, \u0026jOut) fmt.Println(\"number of management ports:\", jOut[\"managementPortCount\"]) d.Close() } that produces the following output: ‚ùØ go run main_arista.go number of management ports: 1 That approach is a decent alternative to a missing programmatic interface and sometimes is the best option. But, as it usually happens, it is not universal. Many Network OS‚Äôes can not emit JSON for any given command, if at all. That means we need to resort to the parsing of the unstructured data ourselves. ","date":"2021-06-09","objectID":"/2021/network-automation-options-in-go-with-scrapligo/:2:0","tags":["scrapli","textfsm","netconf"],"title":"Network automation options in Go with scrapligo","uri":"/2021/network-automation-options-in-go-with-scrapligo/"},{"categories":null,"content":"good old parsingAnd we back to square 1, where we usually get after some reality check. That is where we need to parse the unstructured output ourselves and get the blob of text we receive from a device to be transformed to some data structure which we can use in a program. Of course, we can simply use Regular Expressions or even brute characters matching a loop, but when dealing with lengthy outputs (usually a product of a show command), we often resort to a framework that can simplify the parsing. For quite a long time the TextFSM python library was the answer to that particular task and since then a huge amount of textfsm templates were written to parse all sorts of outputs from various devices. Being a Go person myself I was wondering if TextFSM exists in Go, since once we have scrapli in Go, having a parsing library in Go was the key piece missing. Fortunately, some bright mind already ported TextFSM to Go - go-textfsm - and Carl integrated it into scrapligo the same day I notified him that go-textfsm exists. Let‚Äôs have a look how it is used within scrapligo. For that exercise we will take a show system information output from Nokia SR OS and use a textfsm template to create a structured data out of it. the original textFSM templates might need to be touched by you, since Go regexp differs from Python RE in some parts. To parse the output we will get from the Nokia SR OS device I will create a file with the textfsm template for this output under sysinfo.textfsm file name. Here is the template body: # System Value SysName (\\S+) Value SysType (.*) Value Version (\\S+) Value SysContact (.*) Value SysLocation (.*) Value SysCoordinates (.*) Value SysAtv (\\S+) Value SysUpTime (.*) Value ConfigurationModeCfg (.*) Value ConfigurationModeOper (.*) Start ^System Information -\u003e ReadData ReadData ^System Name\\s*:\\s*${SysName} ^System Type\\s*:\\s*${SysType} ^System Version\\s*:\\s*${Version} ^System Contacts+:\\s+${SysContact} ^System Location\\s*:\\s*${SysLocation} ^System Coordinates\\s*:\\s*${SysCoordinates} ^System Active Slot\\s*:\\s*${SysAtv} ^System Up Time\\s*:\\s*${SysUpTime} Now when the template is there, we can write the following scrapligo + gotextfsm program: package main import ( \"fmt\" \"github.com/scrapli/scrapligo/driver/base\" \"github.com/scrapli/scrapligo/driver/core\" \"github.com/scrapli/scrapligo/transport\" ) func main() { d, _ := core.NewCoreDriver( \"clab-scrapli-sros\", \"arista_eos\", base.WithAuthStrictKey(false), base.WithAuthUsername(\"admin\"), base.WithAuthPassword(\"admin\"), base.WithTransportType(transport.StandardTransportName), ) d.Open() r, _ := d.SendCommand(\"show system information\") parsedOut, _ := r.TextFsmParse(\"private/sysinfo.textfsm\") fmt.Printf(\"Version: %s\\nUptime: %s\", parsedOut[0][\"Version\"], parsedOut[0][\"SysUpTime\"]) d.Close() } Output: ‚ùØ go run main_fsm.go Version: B-20.10.R3 Uptime: 8 days, 03:07:43.25 (hr:min:sec) Easy-peasy! All thanks to the textFSM integration that scrapligo recently added! ","date":"2021-06-09","objectID":"/2021/network-automation-options-in-go-with-scrapligo/:3:0","tags":["scrapli","textfsm","netconf"],"title":"Network automation options in Go with scrapligo","uri":"/2021/network-automation-options-in-go-with-scrapligo/"},{"categories":null,"content":"PSRegardless which network/vendor/consulancy firm you employed with, you won‚Äôt be able to avoid CLI parsing activities at all times. The legacy gear is out there, with no other management interface but SSH/SNMP. Before scrapligo it was quite tedious (I‚Äôd claim not worth it even) to automate network activities over SSH. Now the module packs almost everything you need to efficiently get going and write some nice automation programs or CLI tools. ","date":"2021-06-09","objectID":"/2021/network-automation-options-in-go-with-scrapligo/:4:0","tags":["scrapli","textfsm","netconf"],"title":"Network automation options in Go with scrapligo","uri":"/2021/network-automation-options-in-go-with-scrapligo/"},{"categories":null,"content":"With the growing number of containerized Network Operating Systems (NOS) grows the demand to easily run them in the user-defined, versatile lab topologies. Unfortunately, container runtimes alone and tools like docker-compose are not a particularly good fit for that purpose, as they do not allow a user to easily create p2p connections between the containers. Containerlab provides a framework for orchestrating networking labs with containers. It starts the containers, builds a virtual wiring between them to create a topology of users choice and then manages a lab lifecycle. Containerlab focuses on containerized Network Operating Systems such as: Nokia SR-Linux Arista cEOS Azure SONiC Juniper cRPD FRR In addition to native containerized NOSes, containerlab can launch traditional virtual-machine based routers using vrnetlab integration: Nokia virtual SR OS (vSim/VSR) Juniper vMX Cisco IOS XRv Arista vEOS And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span contain all the needed variants of nodes: ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:0:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"The WHYAs it often happens, https://containerlab.srlinux.dev was created by engineers to address their needs. In containerlab‚Äôs case the need was simple - to be able to create networking topologies with containerized Network Operating Systems As you might know, the off-the-shelf tools like docker-compose are not really fit-for-purpose of defining a multi-interfaced containers, therefore many of us created the bespoke bash scripts ruling a web of veth pairs between containers. Containerlab solves this, and helps many other pain points you might have seen while running your labs. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:1:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"The WHATContainerlab is what docker-compose would be if it was created with networking topologies in mind. We use so-called clab files to define a topology that is then deployed by containerlab anywhere, where docker runs without any 3rd party dependencies. The clab file is a YAML in disguise, it offers a way to define your topology top-to-bottom. Balancing between the simplicity, conventionality and expressiveness it allows users to define topologies that are both easy to read/write and yet are not limited in features) name:srlceos01topology:nodes:srl:kind:srlimage:srlinux:20.6.3-145license:license.keyceos:kind:ceosimage:ceos:4.25.0Flinks:- endpoints:[\"srl:e1-1\",\"ceos:eth1\"] ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:2:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"The HOWThis clab file is all that is needed to spin up a lab of the two interconnected nodes - Nokia SR Linux and Arista cEOS. Yes, that is all that‚Äôs needed. No bulky emulators, no bespoke datapaths. A pure container-based lab powered by linux networking primitives. That is what you get: All the heavy lifting of launching the containerized NOS is abstracted by containerlab kinds. It knows how to start SR Linux and cEOS. Just tell it which kind you need and what image to use No need to keep handy those endless ENV vars or lengthy commands. Interconnecting the nodes is as easy as writing a string of text. Tell containerlab which interfaces you want to be interconnected, and it will create the veth pairs blazingly fast. And surely enough, that is just the tip of an iceberg, containerlab packs a ton of features which I won‚Äôt repeat here, as they are all mentioned in the docs site we carefully maintain. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:3:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Multivendor capabilities","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:4:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Arista cEOSAlthough containerlab was born in Nokia, it is now truly multivendor. Arista folks reading this? Here is a full blown support for cEOS Run cEOS as a first class citizen, it even makes cEOS to respect the docker assigned IP address. Although containerlab was born in Nokia, it is now truly multivendor. Arista folks, you there? @burneeed @flat_planet @TiTom73 @loopback1 https://t.co/N9OJQByszR Containerlab can run cEOS as a first class citizen, it even makes cEOS to respect the docker assigned IP address pic.twitter.com/HWFpMSyiAE ‚Äî Roman Dodin (@ntdvps) April 1, 2021 ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:4:1","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Juniper cRPDWe don‚Äôt pick sides in our choice of containerized NOS support, so Juniper cRPD is as welcome as any other NOS. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:4:2","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"SONiCYes, SONiC is there as well and we spent some time to make it beautifully integrated and start up just as any other NOS. A perfect candidate to be paired with Nokia SR Linux and see a modern DC interop use cases through and through. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:4:3","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"FRRComing from the free OSS NOS camp? FRR is also under containerlab umbrella. Basically, any Linux based NOS that you can image will be able to be run by containerlab, as it is agnostic to the packages inside the linux container. Containerlab is extensible, and if anything that is dear to your heart is missing it definitely can be added. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:4:4","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Network node + regular containersAlso remember that the same clab file can really be like docker-compose file. ‚úÖ Need to bind mount files/dirs to your network node ‚úÖ Want to expose a port to a container host ‚úÖ Maybe set ENV vars ‚úÖ Or change/augment the CMD the node runs I am repeating myself, but can‚Äôt stress this enough, containerlab clab files are a mix of a docker-compose and some networking stardust. That means that you can define a topology that will have both linux containers and network nodes. A perfect example - a telemetry lab. The above topology is defined in a single clab file that has your networking nodes and regular linux container defined. A single gittable, versionable lightweight text file defines a ready-made topology that is spinnable in 15 seconds. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:5:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"What about my VM-based routers?I can imagine how @ioshints says that this container-ish thingy can‚Äôt stand a chance vs real-deal qcow2 packaged VMs. Yes, a valid concern, but we are lucky that guys like @plajjan did some splendid work that we leveraged in containerlab. Watch my hands. Containerlab can run classic VMs like Nokia SR OS, Cisco IOS-XR, Juniper vMX, Arista vEOS in the container packaging. Yes, defined in the same clab file. This is possible by using our adapted version of vrnetlab project - https://github.com/hellt/vrnetlab name:vr04topology:nodes:srl:kind:srlimage:srlinux:20.6.3-145license:license.keyxrv9k:kind:vr-xrv9kimage:vr-xrv:7.2.1links:- endpoints:[\"srl:e1-1\",\"xrv9k:eth1\"] With this you can turn any EVE-NG or GNS lab that you have into a clab file. By packaging your routers into container images you can push them into a registry and enjoy your labs with a docker UX. Total control about reproducibility. In fact, in Nokia many engineers already transitioned from virsh/EVE/GNS to containerlab and they helped us refine containerlab to make it play nicely with classic VM-based products. The benefits of treating a router VM as a container are quite compelling. In fact, in Nokia many engineers already transitioned from virsh/EVE/GNS to containerlab and they helped us refine containerlab to make it play nicely with classic VM-based products. The benefits of treating a router VM as a container are quite compelling. pic.twitter.com/B5LnDpxDtX ‚Äî Roman Dodin (@ntdvps) April 1, 2021 ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:6:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Sharing lab accessOk, so in the same containerlab file we can run any linux container run most popular containerized NOSes run VM-based routers what else? Globe with meridians here goes the story about our collab with @atoonk and his @mysocketio service There is a thing about ‚Äòem labs. They usually run in a closed, isolated environments, with a handful of ppl having access to it. But quite often you find yourself in need to share access to this lab. And then it becomes a battle of a hundred SSH tunnels and exposed credentials. By integrating mysocketio service into containerlab we achieved an on-demand, stable \u0026 secure and lab access sharing. Check out this short video that explains the concepts: Just like this, adding a single line to your node definition, you make it available via Internet over the anycast network with optional strict OAuth rules for a fine grained access control. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:7:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"What are the use cases?The usecases where containerlab can shine are not only limited to labs. It is a perfect demo tool, you have a guarantee that your lab will run just like it always was thanks to strict versioning and immutable container images. With a very small footprint, requiring only Docker Another domain where we see containerlab be of a great help is CI/CD. Github Actions and Gitlab CI both have docker installed on their runners, so you can launch topologies and test them in your CI easily. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:8:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Any examples?Definitely, we also launched a satellite repo for containerlab based labs - https://clabs.netdevops.me This catalog is meant to be an open collection of labs built with containerlab. Anything you build with containerlab I will gladly feature there with full attribution to an author. You will likely find more use cases that fit your need, so give https://containerlab.srlinux.dev a nice spin and let us know how it goes. ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:9:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"Special thanksI want to thank @WHenderickx and @Karimtw_ who started this thing and created the core architecture. Then our internal users and contributors for always providing feedback and thus making containerlab better. It was a truly team work. A special kudos goes to @networkop1 who is always ahead of time and had a similar tool (docker-topo) created years ago. We took inspiration from it when were creating the containerlab topo file schema. Found this awesome, do not hesitate to star our repo - https://github.com/srl-labs/containerlab as a way of saying thanks. Want to contribute? That is awesome and appreciated! PS. The original announcement was made via this tweet-series. üö® I've been sitting on my hands for 3 months, but now the time has finally come... ü•ºWe are releasing containerlab - the open source CLI tool that may redefine the way you run networking labs.https://t.co/WZQGFWEttB It will be a long üßµbut I guarantee, you will dig it. ‚Äî Roman Dodin (@ntdvps) April 1, 2021 ","date":"2021-04-01","objectID":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/:10:0","tags":null,"title":"Containerlab - your network-centric labs with a Docker UX","uri":"/2021/containerlab-your-network-centric-labs-with-a-docker-ux/"},{"categories":null,"content":"I am a huge fan of a goreleaser tool that enables users to build Go projects and package/publish build artifacts in a fully automated and highly customizable way. We‚Äôve have been using goreleaser with all our recent projects and we couldn‚Äôt be any happier since then. But once the artifacts are built and published, the next important step is to make them easily installable. Especially if you provide deb/rpm packages which are built with NFPM integration. The ‚Äúchallenge‚Äù with deb/rpm packages comes to light when project owners want to add those packages to Apt/Yum repositories. Goreleaser doesn‚Äôt provide any integrations with 3rd party repositories nor there are Apt/Yum repositories which are free and provide an API to upload artifacts. Or are there? ","date":"2021-02-23","objectID":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/:0:0","tags":["fury.io","apt","yum","goreleaser"],"title":"Building and publishing deb/rpm packages with goreleaser and fury.io","uri":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/"},{"categories":null,"content":"Gemfury aka Fury.ioActually there is at least one - the gemfury.io project that does just that (and even more). Gemfury is a private package repository to help you easily reuse code without worrying about its hosting or deployment. It integrates directly with existing package management tools that you already use. Among other repositories, Fury provides a Yum/Apt repo for pre-built deb/rpm packages. It is free for public packages, which makes it a good choice for OSS projects. It also sports a hefty number of options to upload artifacts, from a simple curl to a push via its own CLI tool. Just register within the service and generate a push token, and you are good to go leveraging Goreleaser to push your artifacts to Fury. ","date":"2021-02-23","objectID":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/:1:0","tags":["fury.io","apt","yum","goreleaser"],"title":"Building and publishing deb/rpm packages with goreleaser and fury.io","uri":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/"},{"categories":null,"content":"Using Goreleaser with FuryStep 1: Adding Fury' tokenOnce you have a Fury' push token, it is a matter of a few lines of code on the Goreleaser side. I am using Goreleaser' Github action to build and publish artifacts, therefore I added push token to repo‚Äôs secrets and added it as another environment variable of a goreleaser action: # github action workflow file---name:Releaseon:push:tags:- v*jobs:goreleaser:runs-on:ubuntu-lateststeps:- name:Checkoutuses:actions/checkout@v2with:fetch-depth:0- name:Set up Gouses:actions/setup-go@v2with:go-version:1.15- name:Run GoReleaseruses:goreleaser/goreleaser-action@v2with:version:v0.155.0args:release --rm-distenv:GITHUB_TOKEN:${{ secrets.GITHUB_TOKEN }}FURY_TOKEN:${{ secrets.FURYPUSHTOKEN }} This will make our FURYPUSHTOKEN secret value to be available inside the Goreleaser' Env vars under the FURY_TOKEN name. Step 2: Add ID for NFPM buildsIn the nfpm section of your .goreleaser.yml file add id field. This identification string will be used in Step 3 to scope which artifacts will be pushed to Fury. Since Fury will be used exclusively for dep/rpm artifacts, by using the id related to them we will skip artifacts which are generated in the build section of goreleaser (aka archives). # .goreleaser.yml file\u003cSNIP\u003enfpms:- id:packages# here we say that artifacts built with nfpm will be identified with `packages` string.file_name_template:\"{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}\"\u003cSNIP\u003e Step 3: Add custom publisherNow we need to tell Goreleaser to actually push those deb/rpm files it produced to a Fury repo. This is easily done with the custom publishers feature. publishers:- name:fury.io# by specifying `packages` id here goreleaser will only use this publisher# with artifacts identified by this idids:- packagesdir:\"{{ dir .ArtifactPath }}\"cmd:curl -F package=@{{ .ArtifactName }} https://{{ .Env.FURY_TOKEN }}@push.fury.io/netdevops/ Look how easy it is. Now on every goreleaser' build, artifacts from nfpm will be concurrently uploaded to Fury and immediately available to the users of those Apt/Yum repositories. Do note, that by default pushed artifacts have a private scope, so don‚Äôt forget to visit Fury' account dashboard and make them public. Did I say that Goreleaser is a great tool? I bet I did, so consider supporting it if you have a chance. ","date":"2021-02-23","objectID":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/:2:0","tags":["fury.io","apt","yum","goreleaser"],"title":"Building and publishing deb/rpm packages with goreleaser and fury.io","uri":"/2021/building-and-publishing-deb/rpm-packages-with-goreleaser-and-fury.io/"},{"categories":null,"content":"Lately I have been consumed by an idea of running container-based labs that span containerized NOSes, classical VM-based routers and regular containers with a single and uniform UX. Luckily the foundation was already there. With plajjan/vrnetlab you get a toolchain that cleverly packages qemu-based VMs inside the container packaging, and with networkop/docker-topo you can run, deploy and wire containers in meshed topologies. One particular thing though we needed to address, and it was the way we interconnect containers which host vrnetlab-created routers inside. Vrnetlab uses its own ‚Äúoverlay datapath‚Äù to wire up containers by means of an additional ‚Äúvr-xcon‚Äù container that stitches the exposed sockets. Although this approach allows to re-wire containers in different topologies after the start, this was not something that we could use if we wanted use non-vrnetlab containers in our topology. Ideally I wanted to emulate p2p links between the routers (running inside containers) by veth pairs stretched between them, pretty much like docker does when it launches containers. And that is also the way docker-topo works. ","date":"2021-02-20","objectID":"/2021/transparently-redirecting-packets/frames-between-interfaces/:0:0","tags":["tc","ovs","lacp","vrnetlab"],"title":"Transparently redirecting packets/frames between interfaces","uri":"/2021/transparently-redirecting-packets/frames-between-interfaces/"},{"categories":null,"content":"1 Linux bridge and ‚Äúyou shall not pass‚ÄùMichael Kashin in his docker-topo project wanted to do the same, and he proposed to add a new connection type to vrnetlab which used linux bridges inside vrnetlab containers, thus allowing to interconnected vrnetlab containers in a docker-way: docker create --name vmx --privileged vrnetlab/vr-vmx:17.2R1.13 --meshnet docker network connect net1 vmx docker network connect net2 vmx docker network connect net3 vmx In a nutshell, this is what was proposed by Michael: The router VM sitting inside the container connects to container' data interfaces eth1+ by the means of Linux bridges. This approach is the most straightforward one, it doesn‚Äôt require any additional kernel modules, it is well-known and battle-tested and it has a native support in qemu/libvirt. But the elephant in the room is a Linux bridge' inability to pass certain Ethernet frames - specifically LACP and STP BPDUs. And apparently LACP support is something that is badly needed in nowadays labs, as people want to test/demo EVPN multihoming. So as easy as it gets, classical bridges can‚Äôt satisfy the requirement of emulating a p2p link between data interfaces. Off we go looking for alternatives. ADD: Apparently, there is a simple way to make LACP to pass over the linux bridge, another great person Vincent Bernard read the mailing list archives and found out that you can only restrict the MAC_PAUSE frames and leave LACP be. though tc solution is cleaner for the purpose of a point-to-point link. ","date":"2021-02-20","objectID":"/2021/transparently-redirecting-packets/frames-between-interfaces/:1:0","tags":["tc","ovs","lacp","vrnetlab"],"title":"Transparently redirecting packets/frames between interfaces","uri":"/2021/transparently-redirecting-packets/frames-between-interfaces/"},{"categories":null,"content":"2 MacvtapAnother approach that Michael tried when he was working on docker-topo was macvtap interface that looked promising on paper. And although this approach required to mount the whole /dev to a container namespace, it had no qemu native support so we had to play with opening file descriptors we still tried‚Ä¶ ‚Ä¶and we failed. Macvtaps in bridge mode worked, but they were not passing LACP still. No matter what we tried it became evident that path is a no go. ","date":"2021-02-20","objectID":"/2021/transparently-redirecting-packets/frames-between-interfaces/:2:0","tags":["tc","ovs","lacp","vrnetlab"],"title":"Transparently redirecting packets/frames between interfaces","uri":"/2021/transparently-redirecting-packets/frames-between-interfaces/"},{"categories":null,"content":"3 OpenvswitchMost of my colleagues use openvswitch bridges to interconnect classical libvirt/qemu VMs when they need to have support for LACP. With OvS all it takes is a single configuration command: ovs-vsctl set bridge $brname other-config:forward-bpdu=true The reason I didn‚Äôt want to start with OvS in the first place is that it is like using a sledge-hammer when all you need is to drive through a tiny nail. OvS is heavy in dependencies, it requires a kernel module and sometimes you simply can‚Äôt install anything on the host where you want to run containers. But with all other options exhausted, I decided to add this datapath option to my fork of vrnetlab to finally land LACP. And it worked as it should, until I started to hear complaints from users that sometimes they can‚Äôt install OvS for multiple reasons. But there was nothing else to try, or was there? We even wanted to explore eBPF path to see if it can help here‚Ä¶ ","date":"2021-02-20","objectID":"/2021/transparently-redirecting-packets/frames-between-interfaces/:3:0","tags":["tc","ovs","lacp","vrnetlab"],"title":"Transparently redirecting packets/frames between interfaces","uri":"/2021/transparently-redirecting-packets/frames-between-interfaces/"},{"categories":null,"content":"4 tc to the rescueThen all of a sudden Michael pinged me with the following message: @hellt have you seen this? ‚ÄúUsing tc redirect to connect a virtual machine to a container network ¬∑ GitHub‚Äù https://gist.github.com/mcastelino/7d85f4164ffdaf48242f9281bb1d0f9b This gist demonstrated how tc mirred function can be used to solve a task of port mirroring. Isn‚Äôt this brilliant? That was exactly what we needed, to transparently redirect all layer 2 frames between a pair of interfaces. Pretty much like veth works. And tc delivered! With a couple of lines and no external dependencies (tc is part if iproute2 which nowadays ubiquitous) tc made a perfect datapath pipe between VM and container interfaces: # create tc eth0\u003c-\u003etap0 redirect rules tc qdisc add dev eth0 ingress tc filter add dev eth0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev tap1 tc qdisc add dev tap0 ingress tc filter add dev tap0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev eth1 Back in 2010 some RedHat engineer was looking for a way to do port-mirroring on linux host and he explained how tc mirred works, maybe that inspired mcastelino to write that gist that Michael found, but whichever it was, that helped to solve my case of transparently wiring container interface to a tap interface of a VM. And it was super easy to make it integrated with qemu, since all you need is to create an ifup script for a tap interface: #!/bin/bash TAP_IF=$1 # get interface index number up to 3 digits (everything after first three chars) # tap0 -\u003e 0 # tap123 -\u003e 123 INDEX=${TAP_IF:3:3} ip link set $TAP_IF up # create tc eth\u003c-\u003etap redirect rules tc qdisc add dev eth$INDEX ingress tc filter add dev eth$INDEX parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev tap1 tc qdisc add dev $TAP_IF ingress tc filter add dev $TAP_IF parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev eth1 and then use this script in qemu: -netdev tap,id=XX,ifname=tap1,script=/etc/tc-tap-ifup,downscript=no ","date":"2021-02-20","objectID":"/2021/transparently-redirecting-packets/frames-between-interfaces/:4:0","tags":["tc","ovs","lacp","vrnetlab"],"title":"Transparently redirecting packets/frames between interfaces","uri":"/2021/transparently-redirecting-packets/frames-between-interfaces/"},{"categories":null,"content":"Running multiple VMs out of the same disk image is something we, network engineers, do quite often. A virtualized network usually consists of a few identical virtualized network elements that we interconnected with links making a topology. In the example above we have 7 virtualized routers in total, although we used only two VM images to create this topology (virtualized Nokia router and it‚Äôs Juniper vMX counterpart). Each of this VMs require some memory to run, for the simplicity, lets say each VM requires 5GB of RAM. So roughly, the above topology will claim 30-35GB of RAM in order to operate. Enriching the topology by adding more VMs of the same type will continue to push for more memory, thus running big topologies often becomes an exercise of hunting for RAM. Luckily, there are technologies like Kernel Same Merging (KSM) and it‚Äôs enhanced version Ultra-KSM (UKSM) that are able to lift the memory requirement for use cases like above. In a nutshell, they allow to merge mem pages of the same content, effectively reusing the same memory pages between virtual machines. from UKSM usenix paper Memory deduplication can reduce memory footprint by eliminating redundant pages. This is particularly true when similar OSes/applications/data are used across different VMs. Essentially, memory deduplication detects those redundant pages, and merges them by enabling transparent page sharing. Although UKSM is not a silver bullet for every application and use case, it tends to be a very good fit for hypervisors used to run virtualized networking topologies. For that reason the EVE-NG network emulation platform embeds UKSM in their product. So I decided to bring UKSM to my Ubuntu 20.04 VM that I use to launch virtualized routers and containers to witness the benefits/issues of having it. The results look promising. Running 6 VMs with a system memory footprint of one is a solid memory optimization, especially considering that performance penalty is something we can bare in a lab where we mostly play with control plane features. Now if you want to bring UKSM to your hypervisor you will need to jump through some hoops, as UKSM is a kernel feature that is not available as a module. This means that you need to build a kernel with UKSM enabled, and that might be a barrier too high for some of you. It was for me, until I spent a night trying multiple things until it worked, so let me share with you the process and the outcomes so that you can rip the benefits without having all the trouble of trial-and-error routine. 0 TL;DR Download UKSM patches Download kernel source Apply UKSM patch Build kernel Install kernel ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:0:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"1 Get UKSM patchesAs mentioned above, UKSM is a kernel feature and the way it is distributed nowadays is via patch files that are available in this Github repo. So our first step is cloning this repo to get the patches for recent (4.x and 5.x) kernels. Easy start. ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:1:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"2 Get the kernel source codeAs the UKSM patches need to be applied to a kernel source code, we need to get one. Here things can get a tad complicated. There are many different kernels out there: vanilla Linux kernels blessed by Linus himself distribution kernels (Debian, Ubuntu, Fedora, etc) third party kernels with the best hacks The UKSM patches were created against the vanilla Linux kernel, but my Ubuntu VM runs a kernel that was produced by Ubuntu team. # on Ubuntu 20.04 uname -a Linux kernel-build 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux Vanilla linux kernel uses X.Y.Z versioning. If anything is appended after X.Y.Z (like -48-generic) in my case, it indicates that the kernel comes from a distributor (Ubuntu in my case). Things that didn‚Äôt work: 1 At first I tried to download the original Linux kernel, but the build process failed without giving me a good explanation. 2 Download latest 5.4 kernel from Ubuntu - UKSM patch didn‚Äôt apply, as the code has changed apparently After multiple rinse-repeat iterations I found out that I can take the Ubuntu kernel 5.4.0-48.52 as UKSM patch applies to it no problem and the build succeeds. How did I get one? Oh, that is also something worth documenting, as the path to knowing it is paved with broken links and articles dated early 2000s. First, go here and check what tags/branches are available for Focal release of Ubuntu. Once the tag/branch name is found, pull this one only, to save on data transfer: # fetch single branch/tag only git clone --depth 1 --single-branch --branch Ubuntu-5.4.0-48.52 https://git.launchpad.net/\\~ubuntu-kernel/ubuntu/+source/linux/+git/focal Fast forward 180MB of kernel source code and you have it in focal directory. Next is patching. ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:2:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"3 Patch the sourceTo embed the UKSM code into the kernel code we need to use the patch utility. In the UKSM repo we cloned in step 1 we have patch files per kernel MAJOR.MINOR version. As we downloaded the Ubuntu kernel 5.4.0-something, let‚Äôs try and apply the patch from uksm-5.4.patch patch file. # assuming we cloned UKSM repo in ~ cd focal patch -p1 \u003c ~/uksm/v5.x/uksm-5.4.patch patch command must not return any failures. If it does, do not proceed! This is the most important step, the patch must apply cleanly, meaning that if you see any FAILURE strings in its output (or echo $? doesn‚Äôt return 0) it means the patch is not compatible with the kernel. The tricky part was to find the Ubuntu kernel+patch file combination that didn‚Äôt result in an error. For me the merry pair was Ubuntu-5.4.0-48.52 + uksm-5.4.patch. ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:3:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"4 Build the patched kernelOnce the patch is cleanly applied we build the kernel. Here I feel obliged to say that it was my first kernel build, so the explanations are surely not technically correct, but it works, so why not sharing my view on it. To build the kernel we fist need to create the build configuration file. As we use the kernel that we actually run (5.4.0-48) we can reuse the existing kernel configuration: # being in focal directory make oldconfig This command will run the config generation script and it prompted me that there is a UKSM config option added (as a result of a UKSM patch) and if I want to use it instead of the default KSM option. I typed 1 in the prompt confirming that I need UKSM to be an acting KSM feature. That is the only input that was needed. After the config is made, start the build process: # j8 is the number of cores I had on my machine make -j8 deb-pkg LOCALVERSION=-uksm 40 minutes later I had four debian packages created: ~/focal #Ubuntu-5.4.0-48.52 !15 ?16 root@devbox-u20 08:36:17 ‚ùØ ls -la ../*deb -rw-r--r-- 1 root root 11441428 Feb 17 20:50 ../linux-headers-5.4.60-uksm_5.4.60-uksm-1_amd64.deb -rw-r--r-- 1 root root 910558572 Feb 17 20:58 ../linux-image-5.4.60-uksm-dbg_5.4.60-uksm-1_amd64.deb -rw-r--r-- 1 root root 61261664 Feb 17 20:50 ../linux-image-5.4.60-uksm_5.4.60-uksm-1_amd64.deb -rw-r--r-- 1 root root 1071476 Feb 17 20:50 ../linux-libc-dev_5.4.60-uksm-1_amd64.deb ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:4:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"5 Install the kernelOut of these four files I needed to install all but *dbg* files: sudo dpkg -i ../linux-headers-5.4.60-uksm_5.4.60-uksm-1_amd64.deb sudo dpkg -i ../linux-image-5.4.60-uksm_5.4.60-uksm-1_amd64.deb sudo dpkg -i ../linux-libc-dev_5.4.60-uksm-1_amd64.deb Once this is done, update your grub config to have the new kernel load by default: sudo update-grub And reboot. Done! ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:5:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"6 Verify UKSM is workingAfter the reboot, ensure that your new kernel is running by examining uname -r output. It should match the new version. Launch some VMs, and check the memory consumption as well as the number of sharing pages with cat /sys/kernel/mm/uksm/pages_sharing. ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:6:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"7 Get the built kernelIf you don‚Äôt want to build a kernel yourself (and one night lost I can see why), I packaged the deb files into a bare container which you can pull and copy the files from to install the kernel on your Ubuntu machine: # pull the container and copy the deb files out of it docker pull ghcr.io/hellt/ubuntu-5.4.60-uksm:0.1 id=$(docker create ghcr.io/hellt/ubuntu-5.4.60-uksm:0.1 foo) docker cp $id:/uksm-kernel . All you need to do is to start with step 5 and you should be all good. Thanks for tuning in! ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:7:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"PS. KSM vs UKSMThere is a KSM kernel feature that allows you to achieve some memory sharing via a similar mechanisms. It can be that KSM will deliver a similar performance on your setup, and being included in your kernel by default it might be worth checking out. The following resources will help you start with KSM: https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-ksm#sect-KSM-The_KSM_tuning_service https://gist.github.com/mapuo/17e3b253222172c1659782eb14150c3a https://www.linux-kvm.org/page/KSM#Enabling_KSM https://openterprise.it/2019/03/enable-ksm-kernel-same-page-merging-on-fedora/ https://rotelok.com/enable-ksm-centos7-debian/ https://www.kernel.org/doc/Documentation/vm/ksm.txt ","date":"2021-02-18","objectID":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/:8:0","tags":["uksm","ubuntu"],"title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","uri":"/2021/how-to-patch-ubuntu-20.04-focal-fossa-with-uksm/"},{"categories":null,"content":"You slice and dice your files in a git repo like a pro and accidentally commit a binary file. It happened to you as well, don‚Äôt pretend it didn‚Äôt. Sooner or later you recognizes this file shouldn‚Äôt be there, it is clogging your git repo for no reason. OK, you delete the file and commit. But the repo size doesn‚Äôt get any smaller. Hm‚Ä¶ Indeed, next time you do git clone you are wondering why your repo is still megabytes in size, while it has just some source code files? The thing is, by just deleting the file from your working tree and committing this action you don‚Äôt make things any better. This large file still sits somewhere in .git directory waiting for you to rewind the history back and get it. The problem though is that you want this file gone for good. ","date":"2021-02-01","objectID":"/2021/remove-binaries-and-big-files-from-git-repo/:0:0","tags":["git"],"title":"Remove binaries and big files from git repo","uri":"/2021/remove-binaries-and-big-files-from-git-repo/"},{"categories":null,"content":"0 TLDRAll the tags, branches are preserved with this procedure, although I do not guarantee that the workflow will work in your case. Do a backup always. # clone a repo with --mirror flag and change into it git clone --mirror \u003crepo-url\u003e # launch cleanup process git filter-repo --strip-blobs-bigger-than 3M # run GC git reflog expire --expire=now --all \u0026\u0026 git gc --prune=now --aggressive # update the more git push ","date":"2021-02-01","objectID":"/2021/remove-binaries-and-big-files-from-git-repo/:0:1","tags":["git"],"title":"Remove binaries and big files from git repo","uri":"/2021/remove-binaries-and-big-files-from-git-repo/"},{"categories":null,"content":"1 Show me big filesTo see if your repo holds those monster files you can leverage some git commands, but I found this self-contained python script quite a good fit for the purpose: ‚ùØ ./lf.py -c 20 Finding the 20 largest objects‚Ä¶ Finding object paths‚Ä¶ All sizes in kB. The pack column is the compressed size of the object inside the pack file. size pack hash path 6769 6761 82d233ab6ff841f16bd17c2b5a6906ccdd8af8e5 rpm/tool-1.0.0.x86_64.rpm 13439 6723 dbd32fc21381cf1e4cb0ba964f53aff1ebcc8547 bin/tool 12437 6223 967237f169780b8660a771c6f478de1d93822157 bin/tool 12413 6211 dfd93506fa17401cc996223337b8372bf921887e bin/tool 11776 5917 f35577a72a2493b00c6e0520d1454d9fdaedb886 bin/tool 5646 5638 66cc7eb29577bb84aaa682dd1eb694fde1d9e399 rpm/tool-1.0.0.x86_64.rpm 5944 4073 360106b01776e4e7419ab414878d582747d7c945 bin/tool-test 5333 3899 53ef404d20a09db9040696eeb5df5bebf10ecf52 bin/tool 4985 3569 1d81eafd70736f568526b7e5221478b5b3e67c6d bin/tool 4111 3224 acfd2077e642272c2ab09cbfaf435b4fc91ac012 bin/tool 4018 3205 f331cec6b0e599dfbef7361c947a14beea7ce4c2 bin/tool 3849 3111 8bb4fdaeccecf1ef0a91fc780c243cb89109597a bin/tool 655 456 393dadfa6f5957f60a42287ed2c6e7ddcd5688cc bin/tool Nice and easy we get 20 largest files which I have no intention to keep and they make the size of the repo to be in 70MB range with compression. No good. ","date":"2021-02-01","objectID":"/2021/remove-binaries-and-big-files-from-git-repo/:0:2","tags":["git"],"title":"Remove binaries and big files from git repo","uri":"/2021/remove-binaries-and-big-files-from-git-repo/"},{"categories":null,"content":"2 Removing large files2.1 Beware of consequencesNow to the fun part, lets remove those files, making our repo fit again! One problem, though, it‚Äôs not that easy. First of all, this operation is very intrusive. As Git stores the commits in a graph fashion, you can‚Äôt change some commit in the middle, without rewriting the commits after it. So be prepared, that all the commits will eventually have new hashes. Evaluate the consequences and implications of it. 2.2 ProcedureIf you start searching, you will find many workflows, dating back to early 2000s and git 1.x. I tried them and I regret. Eventually I found the working combination that I tested on two of my repos and they worked flawlessly. 2.2.1 Make a backupDo a backup of your original repo 2.2.2 Clone a mirrorNow clone the repo with a --mirror option. That step is very important. You will have your repo cloned under \u003crepo-name\u003e.git directory, but you won‚Äôt see actual files, instead you will have the git database of this repo. 2.2.3 Install git-filter-repoThe actual tool that does the job is called git-filter-repo. It is a successor to git-filter-branch and BFG Repo Cleaner. There is the Install document, but it is written somehow complex. The easy way to install for me was copying the raw script and copying it under the directory that git --exec-path command outputs. 2.2.4 Run the cleanupThen you can read about the options this script supports, for me I chose the easiest path possible ‚Äì delete every file that is bigger than X Megabytes. So I entered the directory that appeared after I did git clone --mirror and executed the following command: git filter-repo --strip-blobs-bigger-than 3M For my no-so-big repo with 500 commits, it finished in under a second. It removed all the files bigger than 3Megs and re-wrote all the commits that were affected by that change. 2.2.5 Garbage collectWe are not done yet. Although the files were removed for good, we still need to tell git to run a garbage collection procedure to forget about those missing files: git reflog expire --expire=now --all \u0026\u0026 git gc --prune=now --aggressive 2.2.6 Update the remoteNow the final part. We are ready to update the remote with our new git history. Interesting enough it is done with a simple git push no force is needed ¬Ø_(„ÉÑ)_/¬Ø ","date":"2021-02-01","objectID":"/2021/remove-binaries-and-big-files-from-git-repo/:0:3","tags":["git"],"title":"Remove binaries and big files from git repo","uri":"/2021/remove-binaries-and-big-files-from-git-repo/"},{"categories":null,"content":"I am a firm believer that documentation is an integral part of the project. A terse, twisted, incomplete or sometimes even missing documentation penalizes your projects success. At the same time clean, concise and comprehensive documentation is not only something worth being proud of, but an opening to a users' appreciation and fame. I am sharing the way I build, publish and host documentation sites for my projects via this live-example site - projectdocs.netdevops.me ","date":"2020-10-30","objectID":"/2020/how-i-create-documentation-sites-for-my-projects/:0:0","tags":["documentation","mkdocs-material"],"title":"How I create documentation sites for my projects?","uri":"/2020/how-i-create-documentation-sites-for-my-projects/"},{"categories":null,"content":"gnmic was the first opensource project that I‚Äôve been part of that got widely adopted. As the maintainers of a public project, Karim and I were wondering when would we get the first external contribution. To our surprise, the very first external contribution laid out the foundation to one of the most exciting features of gnmic - YANG-Completions. I thought that the best way to describe what YANG-completions is showing you a quick demo augmented with some comments. This resulted in this twitter-series: üíªFellow Network Automation engineers, today is a special day for YANG-based automation as we release the \"YANG-completions\" feature with gnmic v0.4 I have only 1m30s to convince you that it is a dream turned into reality (or a black magic üíÄ)https://t.co/Nylyay1pl4 pic.twitter.com/IOnumRCOZi ‚Äî Roman Dodin (@ntdvps) October 21, 2020 ","date":"2020-10-22","objectID":"/2020/gnmic-got-better-with-yang-completions/:0:0","tags":["gnmi","openconfig","yang"],"title":"gNMIc got better with YANG-completions","uri":"/2020/gnmic-got-better-with-yang-completions/"},{"categories":null,"content":"We were pleasantly surprised by the way community appreciated gNMIc release. Thank you üôè! That solidifies the fact that a well-formed, documented and easy to use gNMI tool was needed. Now with gNMIc available to everybody its easy like never before to test gNMI implementation of different routing OSes. And in this post we will get our hands on Arista vEOS. For this journey we pack: vEOS router (either physical or virtual) gNMIc documentation and a gNMI-map to navigate through the gNMI realm. Arista vEOS-for-labs is freely distributed and you can download the vmdk image from the official software portal. Table of contents vEOS configuration gNMI Capabilities Getting to know Arista YANG models gNMI Get gNMI Set gNMI Subscribe Sample subscriptions ON_CHANGE subcriptions ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:0:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"vEOS configurationOnce your vEOS starts with a blank config (credentials: admin and an empty pass) we ought to add a minimal config to it before gNMI fun starts: username admin privilege 15 secret admin ! interface Ethernet1 no switchport ip address 10.2.0.21/24 ! management api gnmi transport grpc default With this config snippet we do a few things important from the gNMI standpoint: enabling password for admin to authenticate with a router configuring IP address for the Ethernet1 interface to let gNMIc reach the router enabling gnmi management interface with the default transport config default transport doesn‚Äôt enforce TLS usage and uses 6030 port That is all it takes to configure vEOS to start replying to our first gNMI RPCs, ridiculously easy! ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:1:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"gNMI CapabilitiesWith gNMIc installed, our first stop would be trying out the gNMI Capabilities RPC. The Capabilities RPC is quite instrumental as it uncovers which gNMI version the device runs, what models it is loaded with and which encoding it understands. # 6030 - the default gNMI port on vEOS # credentials are admin:admin # --insecure mode is used to not enforce the TLS transport $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure capabilities gNMI version: 0.7.0 supported models: - openconfig-rib-bgp, OpenConfig working group, 0.7.0 - arista-qos-augments, Arista Networks, Inc., - arista-srte-deviations, Arista Networks, Inc., \u003cCLIPPED\u003e - openconfig-platform-linecard, OpenConfig working group, 0.1.1 - openconfig-if-tunnel, OpenConfig working group, 0.1.1 supported encodings: - JSON - JSON_IETF - ASCII Judging by the output returned we see that vEOS 4.24.1.1F in my lab runs the latest gNMI version 0.7.0 it is configured with both openconfig and native models it supports two variants of JSON encoding with a useless ASCII ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:2:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"Getting to know Arista YANG modelsBefore we can dive into the rest RPCs of gNMI service we have to get to know the YANG models vEOS listed as supported in Capabilities response. Arista publishes its YANG models in the aristanetworks/yang repo and by the looks of it it seems they are OpenConfig believers. For the vEOS 4.24.1.1F release that I am running the list of YANG models is definitely angled towards OpenConfig models with native YANG models marked as experimental. Browsing the source OC YANG files in this repo is one way to understand the structure of the models, or we can use pyang or goyang to generate a tree view. Michael Kashin shows here how to use goyang to quickly generate tree views of Arista models. Once we know the structure of the OC YANG models vEOS is equipped with we can finally get to more advanced RPCs fetching, setting and subscribing. If YANG transformation topic is hard on you, ping me in comments and I will expand on this. ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:3:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"gNMI GetNow that we know which models our gear runs we can easily issue a gNMI Get RPC with get command. Lets pretend that we would like to know the configured IP addresses on the vEOS. All it takes is to carefully walk through the OC model to the right leaf: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\ --path \"/interfaces/interface[name=*]/subinterfaces/subinterface[index=*]/ipv4/addresses/address/config/ip\" { \"source\": \"10.2.0.21:6030\", \"time\": \"1970-01-01T02:00:00+02:00\", \"updates\": [ { \"Path\": \"/interfaces/interface[name=Ethernet1]/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=10.2.0.21]/config/ip\", \"values\": { \"interfaces/interface/subinterfaces/subinterface/ipv4/addresses/address/config/ip\": \"10.2.0.21\" } } ] } The returned output indicates that router has only one IPv4 address 10.2.0.21 configured and it is contained within the /interfaces/interface[name=Ethernet1]/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=10.2.0.21]/config/ip path. The paths in its turn shows that the interfaces that has this IP is Ethernet1. gNMI Get ALLOne particular trick that might come very handy is getting the entire config/state of the router with gNMI. That will likely output a lot of data but it will enable you to search through it and find the right path in the model for a more precise get/set/subscribe queries. The trick is to specify the root / path for your Get RPC that will dump all the data from the router. Its better to redirect the output to a file: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get --path / \u003e /tmp/arista.all.json Here is the resulting JSON that I fetched from my lab router. This way you can ‚Äúreverse engineer‚Äù the models tree view by letting a router send you all its state and config. ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:4:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"gNMI SetOur next RPC will change the configuration on a vEOS router. This is done with the gNMIc set command. Updating configurationA quick example would be to add a description to a port. But first lets ensure that its not set: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\ --path \"/interfaces/interface[name=Ethernet1]/config/description\" { \"source\": \"10.2.0.21:6030\", \"time\": \"1970-01-01T02:00:00+02:00\", \"updates\": [ { \"Path\": \"/interfaces/interface[name=Ethernet1]/config/description\", \"values\": { \"interfaces/interface/config/description\": \"\" } } ] } All good, the description is empty, lets set it to gnmic-example value: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure set \\ --update-path \"/interfaces/interface[name=Ethernet1]/config/description\" \\ --update-value \"gnmic-example\" { \"source\": \"10.2.0.21:6030\", \"timestamp\": 1595749808169752510, \"time\": \"2020-07-26T10:50:08.16975251+03:00\", \"results\": [ { \"operation\": \"UPDATE\", \"path\": \"/interfaces/interface[name=Ethernet1]/config/description\" } ] } With the implicit-type kind of a set operation gNMIc will use the JSON encoding for the value specified with --update-value flag. The result we get back from the box confirms that the UPDATE operation has been applied. Now we can check if the value is indeed set by repeating the get command: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\ --path \"/interfaces/interface[name=Ethernet1]/config/description\" { \"source\": \"10.2.0.21:6030\", \"time\": \"1970-01-01T02:00:00+02:00\", \"updates\": [ { \"Path\": \"/interfaces/interface[name=Ethernet1]/config/description\", \"values\": { \"interfaces/interface/config/description\": \"gnmic-example\" } } ] } Now we see the description set to the value we specified. gNMIc supports many ways to provide the configuration values, check set command docs for all the options. Deleting configurationgNMI Set RPC allows not only to update/replace the configuration but also to delete it. Lets remove the description we set before with the delete flag of the set command.: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure set \\ --delete \"/interfaces/interface[name=Ethernet1]/config/description\" { \"source\": \"10.2.0.21:6030\", \"timestamp\": 1595750232015131720, \"time\": \"2020-07-26T10:57:12.01513172+03:00\", \"results\": [ { \"operation\": \"DELETE\", \"path\": \"/interfaces/interface[name=Ethernet1]/config/description\" } ] } ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:5:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"gNMI SubscribeAnd we managed to get to the end of it. The crown jewel of gNMI service - Subscribe RPC. To demonstrate gNMI subscriptions done with the corresponding subscribe command we will solve the following tasks: subscribe to interface counters and see the effect of SAMPLE subscriptions with different sampling intervals subscribe to a protocol admin status and see the effect of ON_CHANGE mode of subscription The subscribe command has a lot of options which are instrumental to tailor the command behavior to your needs. Sample subscriptionsFor the sampled subscriptions we expect to receive the value of the subscribed data node with each sampling interval; doesn‚Äôt matter if the data changes in-between the sampling timestamps or not, we will get it with the cadence specified by the sample-interval. Samples subscriptions are useful for rapidly changing data, like interface counters. Lets subscribe to our only interface Ethernet1 with 2 seconds sampling interval. Note, we had to disable QoS marking by setting it to 0, since vEOS does not support marking for gNMI messages. $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure subscribe \\ --path \"/interfaces/interface[name=Ethernet1]/subinterfaces/subinterface/state/counters/in-octets\" \\ --stream-mode sample --sample-interval 2s \\ --qos 0 _sampled subscriptions arriving with 2s interval_ On Change subscriptionsThe other popular subscription mode is ON_CHANGE where the router pushes the data towards the collector when the data changes. With such a mode you don‚Äôt push the data unnecessarily. A popular use case for ON_CHANGE subscriptions is to subscribe to oper/admin state of control plane protocols to get notified when the state changes (aka trap). To demonstrate this behavior we will configure BGP process on vEOS subscribe with ON_CHANGE mode to the BGP AS leaf effectively watching its value. Our trivial BGP configuration: ! ip routing ! router bgp 2 Now lets subscribe to the AS number with: $ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure subscribe \\ --path \"/network-instances/network-instance[name=default]/protocols/protocol[identifier=BGP][name=BGP]/bgp/global/config/as\" \\ --stream-mode on_change \\ --qos 0 { \"source\": \"10.2.0.21:6030\", \"subscription-name\": \"default\", \"timestamp\": 1595753683536455210, \"time\": \"2020-07-26T11:54:43.53645521+03:00\", \"updates\": [ { \"Path\": \"network-instances/network-instance[name=default]/protocols/protocol[identifier=BGP][name=BGP]/bgp/global/config/as\", \"values\": { \"network-instances/network-instance/protocols/protocol/bgp/global/config/as\": 2 } } ] } Our subscription will stand still, as the AS number doesn‚Äôt change, the router will not update it, unless the value changes. Lets remove the BGP process from the router and see what happens: _on change subscriptions arrive when data changes_ Here we receive a notification update about the deletion of the data we subscribed to immediately when the deletion happens. ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:6:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"SummaryIn this post we put gNMIc to a good use against Arista vEOS. All of the gNMI service RPCs have been successfully tested against vEOS, we have identified which encoding vEOS supports, found out that it uses OpenConfig models mostly and it can‚Äôt use QoS markings. On the bright side, vEOS supports ON_CHANGE subscriptions and is capable of delivering subsecond updates. ","date":"2020-07-25","objectID":"/2020/arista-veos-gnmi-tutorial/:7:0","tags":["gnmi","openconfig","arista","gnmic","yang"],"title":"Arista vEOS gNMI Tutorial","uri":"/2020/arista-veos-gnmi-tutorial/"},{"categories":null,"content":"Despite the fact that gNMI is defacto the go-to interface for a model-driven telemetry collection, we, as a community, had no gNMI tool that was easy to install, pleasure to use, documented and pre-built for common platforms. Until now. I am excited to announce the public release of gnmic - a CLI client and a collector that talks gNMI to your devices. ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:0","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"Problem statementI am not exaggerating, there is a shortage of open source gNMI clients one can find. And when I say gNMI clients I mean the CLI clients that allow you to invoke gNMI service RPCs. Earlier this year I bragged about it, in hope that my google-foo is just broken and the community knows of a gNMI client that I could download and use right away without jumping through hoops: So coming back to the OpenConfig/gNMI and what a hot mess it is when it comes to the tooling. You would probably think that there is a top-notch gNMI CLI (or even a shell) for you to query your routers like a pro. ‚Äî Roman Dodin (@ntdvps) February 18, 2020 But that was not my google-foo, unfortunately. For the sake of completeness allow me to summarize the landscape of gNMI clients in a pre-gnmic era: OpenConfig gNMI CLI client - thats the google search top result one gets when looking for gNMI client. A reference implementation which lacks some essential features: no documentation, no usage examples - you really better know how to read Go code to understand how to use it. Get requests will require you to write in proto syntax instead of a simple get command with a path. additional options like Encoding, Models are not exposed via flags. no ready-made binaries - you need to have a Go tool chain to build the tool. no insecure support - you can kiss goodbye your lab installations without PKI. Google gnxi - Googles gNxI tools that include gNMI, gNOI. the gNMI RPCs are split to different CLI tools which is not convenient a list of flags is all you got when it comes to documentation no releases to download, Go toolchain is needed cisco-gnmi-python - a Cisco Innovative Edge project that is quite decent and complete, good job! But a few improvements could have been made: client doesn‚Äôt allow to use insecure gRPC transport, PKI is mandatory. Set requests can‚Äôt set values specified on the command line. CLI structure is not consistent across the commands No option exposed to set the Subscription mode. Telegraf and Ansible gNMI module are not qualified to be considered as CLI tools. ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:1","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"What makes gNMI tool nice to use?Looking at this landscape, the following essential features a nice gNMI client should have come to mind: provide a clean and vendor independent interface to gNMI RPCs expose all configuration options the gNMI RPCs have via flags or file-based configurations allow multi-target operations: i.e. a subscription made to a number of the devices implement both TLS enabled and non-secure transport support different output formats (JSON, proto) and destinations (stdout, file, streaming/messaging buses) be documented provide an easy way to install the tool without requiring a dev toolchain to be present. With these essential features in mind we started to work on gnmic. ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:2","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"gNMIc and its featuresThe work on gnmic started with analysis of the existing tools shortcomings coupled with collecting requirements from our fellow engineers and our past user experience. For the gnmic features run down go to our beautiful documentation portal - https://gnmic.kmrd.dev. In this post I will go a bit deeper on some core features and design choices we made, so please refer to the documentation if you are looking for a basic usage or command reference guide. Consistent command line interfaceIt is easy to spot a CLI tool that got some love from its developers by looking at the way it is composed. Since most of the gnmic users will use it as a CLI tool we took an extra step and wrote it with a Cobra framework that adds a great layer of consistency to the command line applications. With Cobra gnmic gets extra powers such as consistent global and local flags, multi-tiered subcommands, auto-generated and accurate help and overall a ‚Äúproper‚Äù CLI behavior. $ gnmic get --help run gnmi get on targets Usage: gnmic get [flags] Flags: -h, --help help for get --model strings get request model(s) --path strings get request paths --prefix string get request prefix -t, --type string the type of data that is requested from the target. one of: ALL, CONFIG, STATE, OPERATIONAL (default \"ALL\") Global Flags: -a, --address strings comma separated gnmi targets addresses --config string config file (default is $HOME/gnmic.yaml) -d, --debug debug mode -e, --encoding string one of [json bytes proto ascii json_ietf]. Case insensitive (default \"json\") Alignment to gNMI specificationFor a tool to be generic it must not deviate from a reference specification. Adhering to that promise, we made gnmic commands modelled strictly after the gNMI RPCs. Each RPC has a command with a clear and concise name, and each command‚Äôs flags are named after the fields of the corresponding proto message. No ambiguous flag names or questionable subcommands, it is clear and guessable what each command and flag does without looking at the documentation: $ gnmic -h \u003csnipped\u003e Available Commands: capabilities query targets gnmi capabilities get run gnmi get on targets help Help about any command listen listens for telemetry dialout updates from the node path generate gnmi or xpath style from yang file set run gnmi set on targets subscribe subscribe to gnmi updates on targets version show gnmic version \u003csnipped\u003e Moreover, we tried to expose every configuration knob gNMI specification has to offer. Again, a generic tool should not limit your capabilities, so if you want to, say, restrict the YANG models the gNMI target should use when replying back to the client - there is a flag for that! TLS and non-TLS transportsWe allowed ourselves to step away from the specification to add one additional generic purpose feature - a insecure transport fo gRPC connection. The need for the non-secured connections is quite reasonable, its cumbersome in many cases to deal with certificates and keys generation if all one is up to is a quick gNMI test. gnmic -a 10.1.0.11:57400 -u admin -p admin --insecure capabilities gNMI_Version: 0.7.0 supported models: - nokia-conf, Nokia, 19.10.R2 - nokia-state, Nokia, 19.10.R2 - nokia-li-state, Nokia, 19.10.R2 - nokia-li-conf, Nokia, 19.10.R2 \u003c\u003c SNIPPED \u003e\u003e supported encodings: - JSON - BYTES Flexible configuration optionsDue to a sheer amount of configuration options gnmic has, it can sometimes be tedious to specify all of them as CLI flags. For such cases we leveraged viper and added support for file-based configuration that is consistent with both local and global flags. Its up to a user to choose the configuration file format: YAML, JSON, HCL - all are welcome! $ cat ~/gnmic.ymladdress:\"10.0.0.1:57400\"username:adminpassword:admininsecure:true # now gnmic can read this cfg file and get the params from it $ gnmi get --path /configure/system/name Automation friendly outputIts quite common to use gnmic in a setting where the output it provides is used ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:3","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"SummaryAt the end of the day, I tend to believe that gnmic will successfully fill the void of standalone gNMI tools available to the public. Starting from a consistent CLI layer with all the gNMI RPCs nicely exposed and finishing with the proper docs and easy installation it checks all the marks I had in mind for a decent gNMI client, and hope it will be to community‚Äôs satisfaction as well. Oh, and gnmic also has collection capabilities allowing you to export the metrics collected via gNMI to Kafka, NATS, Influx, Prometheus. But that is for another post. ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:4","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"AuthorsThe team behind gnmic consists of Karim Radhouani and Roman Dodin, but we are welcome contributors of all sorts. Be it code, documentation, bug reports or feature requests! ","date":"2020-07-08","objectID":"/2020/gnmic-gnmi-cli-client-and-collector/:0:5","tags":["gnmi","openconfig","go"],"title":"gNMIc - gNMI CLI client and collector","uri":"/2020/gnmic-gnmi-cli-client-and-collector/"},{"categories":null,"content":"If you pick a random NetEng and ask them if they love NETCONF they would likely say ‚ÄúNah‚Äù. The hate-hate love-hate kind of relationship with NETCONF mostly roots in its XML layer that one can‚Äôt swap out. But if we set the XML-related challenges aside, it will become clear that NETCONF is a very well designed management interface with lots of capabilities. In this topic we will touch on the NETCONF‚Äôs subtree filtering capabilities. NETCONF‚Äôs RFC 6241 defines two methods for filtering contents on the server (router) side: Subtree filtering - mandatory for a NETCONF-enabled device to support XPATH filtering - an optional capability Subtree filtering is powered by the following components: Namespace Selection Attribute Match Expressions Containment Nodes Selection Nodes Content Match Nodes They are very well explained in the RFC, so I won‚Äôt bother with copy-pasting the definition and the rules these filtering components follow. Instead we will focus on the practical examples and put Selection and Content Match nodes to work in different scenarios. ","date":"2020-07-03","objectID":"/2020/netconf-subtree-filtering-by-example/:0:0","tags":["netconf"],"title":"NETCONF subtree filtering by example","uri":"/2020/netconf-subtree-filtering-by-example/"},{"categories":null,"content":"1 Selection nodesSelection node allow us to get a node and all its nested elements. Our simple examples will revolve around interactions with local users configuration on a Nokia SR OS which is modelled with the following YANG model: module: nokia-conf +--rw configure +--rw system | +--rw security | | +--rw user-params | | +--rw local-user | | +--rw user* [user-name] | | +--rw user-name types-sros:named-item | | +--rw password types-sros:hashed-leaf | | +--rw access | | | +--rw console? boolean | | | +--rw ftp? boolean | | | +--rw snmp? boolean | | | +--rw netconf? boolean | | | +--rw grpc? boolean | | | +--rw li? boolean | | +--rw console +--rw member* -\u003e../aaa/local-profiles‚Ä¶ If we want to filter all the configuration information related to the local users we could use Selection node \u003clocal-user/\u003e in our get-config RPC: \u003cget-config\u003e \u003csource\u003e \u003crunning /\u003e \u003c/source\u003e \u003cfilter\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user/\u003e \u003c!-- selection node --\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/filter\u003e \u003c/get-config\u003e Hint #1: Nokia-yangtree is a beautiful way to explore Nokia YANG models. Hint #2: I recommend netconf-console to talk NETCONF to your routers. If we translate this get-operation command to plain English it would sound like: Dear router, can you please return everything you have under local-user node in the running configuration datastore? And that is what router replies back: \u003crpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:f00ec433-17b3-4bcb-9d83-c3557794e56e\"\u003e \u003cdata\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003cpassword\u003e$2y$10$Ro5MzyBZ18eVve/aTIYt..fSBbyJar11QGcQbixrVPfxLcpXeZ4eu\u003c/password\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003cgrpc\u003etrue\u003c/grpc\u003e \u003c/access\u003e \u003cconsole\u003e \u003cmember\u003eadministrative\u003c/member\u003e \u003c/console\u003e \u003c/user\u003e \u003cuser\u003e \u003cuser-name\u003eroman\u003c/user-name\u003e \u003cpassword\u003e$2y$10$xkqn46jNHBUJWit446j2o.Yu3E9zWOg44yRGjRK2YjRZE4p5xFjmG\u003c/password\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003c/access\u003e \u003cconsole\u003e \u003cmember\u003edefault\u003c/member\u003e \u003c/console\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/data\u003e \u003c/rpc-reply\u003e The answer satisfies the request we specified. Router dumps everything it has under local-users. 1.1 Multiple selection nodesBut what is we don‚Äôt want to get back all that information about the local users and just interested in the account names and their access methods? That is as well the work for Selection nodes. But instead of referencing a container or a list with the Selection node, we will pinpoint the nodes of interest - user-name and access: \u003cget-config\u003e \u003csource\u003e \u003crunning /\u003e \u003c/source\u003e \u003cfilter\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name /\u003e \u003caccess /\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/filter\u003e \u003c/get-config\u003e Pay attention that it doesn‚Äôt matter what type of node we are referencing with a Selection node. It can be a container, a list, a leaf. If a selected node happens to have nested elements they will be returned as well. In the example above we reference the user-name leaf and the access container, as a result we receive back a concrete data stored as the user-name node and everything that exists under the access container: \u003crpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:4c646ef4-601b-465e-ba35-f90953527a73\"\u003e \u003cdata\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003cgrpc\u003etrue\u003c/grpc\u003e \u003c/access\u003e \u003c/user\u003e \u003cuser\u003e \u003cuser-name\u003eroman\u003c/user-name\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003c/access\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/","date":"2020-07-03","objectID":"/2020/netconf-subtree-filtering-by-example/:0:1","tags":["netconf"],"title":"NETCONF subtree filtering by example","uri":"/2020/netconf-subtree-filtering-by-example/"},{"categories":null,"content":"2 Content Match nodesIn many cases it is needed to filter not only on the node itself (what Selection node does), but also on the value of the referenced leaf. That is a work for Content Match nodes. Using our local users examples that translates to a need to filter the information of a single user only. Let‚Äôs get the configuration of the admin user only by using the Content Match node semantics: \u003cget-config\u003e \u003csource\u003e \u003crunning /\u003e \u003c/source\u003e \u003cfilter\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003c!-- Content Match node --\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/filter\u003e \u003c/get-config\u003e Content Match nodes filtering is only applicable to the leafs, in our example that was user-name which we set to admin. As a result, we got back the configuration related to the admin user only: \u003crpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:5c1da160-69e1-466a-97c0-541f3add8f2d\"\u003e \u003cdata\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003cpassword\u003e$2y$10$Ro5MzyBZ18eVve/aTIYt..fSBbyJar11QGcQbixrVPfxLcpXeZ4eu\u003c/password\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003cgrpc\u003etrue\u003c/grpc\u003e \u003c/access\u003e \u003cconsole\u003e \u003cmember\u003eadministrative\u003c/member\u003e \u003c/console\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/data\u003e \u003c/rpc-reply\u003e 2.1 Multiple Content Match nodesBy adding multiple Content Match nodes in your filter request you add an implicit AND operand between them. Lets say we want to list the configured users who both have access to netconf and grpc. We can craft such a filter request by using two Content Match nodes expressions: \u003cget-config\u003e \u003csource\u003e \u003crunning /\u003e \u003c/source\u003e \u003cfilter\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003caccess\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003cgrpc\u003etrue\u003c/grpc\u003e \u003c/access\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/filter\u003e \u003c/get-config\u003e In the end we get our single user - admin - who has access to the subsystems we put in a filter, cool! \u003crpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:33d1666e-de58-4414-8c4d-374bd73d8ef2\"\u003e \u003cdata\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003caccess\u003e \u003cconsole\u003etrue\u003c/console\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003cgrpc\u003etrue\u003c/grpc\u003e \u003c/access\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/data\u003e \u003c/rpc-reply\u003e ","date":"2020-07-03","objectID":"/2020/netconf-subtree-filtering-by-example/:0:2","tags":["netconf"],"title":"NETCONF subtree filtering by example","uri":"/2020/netconf-subtree-filtering-by-example/"},{"categories":null,"content":"3 Content Match and Selection nodesAnother interesting filtering technique is combining Selection and Content Match nodes. Quite often you want to filter on the content, but at the same time limit the amount of data that router replies back. That might be very expensive for a router to return every sibling when only Content Match node is used, therefore its a good practice to craft a filter that will contain only the needed information. Talking our local users database me might want to know if admin user has access to netconf subsystem and we don‚Äôt care at all about any other configuration that user has. Thats a perfect candidate for a combination of Content Match and Selection nodes: \u003cget-config\u003e \u003csource\u003e \u003crunning /\u003e \u003c/source\u003e \u003cfilter\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003c!-- content match --\u003e \u003caccess\u003e \u003cnetconf/\u003e \u003c!-- selection --\u003e \u003c/access\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/filter\u003e \u003c/get-config\u003e And look at what a concise and clear response we got back. It has only the information we cared about. \u003crpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:02b36787-e805-4370-ac7b-b569a14d2e64\"\u003e \u003cdata\u003e \u003cconfigure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003caccess\u003e \u003cnetconf\u003etrue\u003c/netconf\u003e \u003c/access\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/configure\u003e \u003c/data\u003e \u003c/rpc-reply\u003e In the simplified local users database example that might not seem critical, but on a real network element you might filter through hundreds of configuration elements while only cared about a single one. Then it makes all the sense to combine Content Match nodes with Selection nodes to minimize the payload sizes and computation times. ","date":"2020-07-03","objectID":"/2020/netconf-subtree-filtering-by-example/:0:3","tags":["netconf"],"title":"NETCONF subtree filtering by example","uri":"/2020/netconf-subtree-filtering-by-example/"},{"categories":null,"content":"SummaryNETCONF Subtree filtering is a powerful mechanism that is both easy to use and reason about. By using Contaiment, Selection and Content Match nodes one can easily filter anything, while maintaining efficiency and cleanliness of the filter construct. Remember that using Selection nodes with Content Match nodes allow you to follow the beast practices and request only the information that you need, without clutter. ","date":"2020-07-03","objectID":"/2020/netconf-subtree-filtering-by-example/:0:4","tags":["netconf"],"title":"NETCONF subtree filtering by example","uri":"/2020/netconf-subtree-filtering-by-example/"},{"categories":null,"content":"Lately I‚Äôve been involved in project that required quite a deep understanding of OpenConfig gRPC Network Management Interface (gNMI). Going over the gNMI specification multiple times made me realize that I can‚Äôt fully build a mental map of all the messages and encapsulations without having a visual representation of it. So I‚Äôve made one, lets see what it has to offer. gNMI Map is essentially a visual guide to the gNMI service. It lays out the protobuf data types that compose the gNMI service and provides the references to the relevant sections of the reference guide and code definitions. For example, if you wondered what are the messages the client sends when it needs to query the Capabilites of the remote gNMI target, you can easily zoom into the Capabilities RPC and identify all the messages and types involved in this RPC: The visual connectors help you unwrap the nested messages and collect the whole picture. Moreover, each message and type ‚Äúcard\"0 has a link to a relevant documentation piece of the reference along with the link to its definition in the gnmi.proto file: allowing you to quickly jump either to the explanation paragraph of the spec or dive into the proto definition code piece. Currently the latest gNMI version (0.7.0.) has been ‚Äúmapped‚Äù, my intention is to release another map when a new version of the gNMI will be available, keeping the old ones versioned. That will allow having a map for each release after 0.7.0. The map comes in a PDF format and is stored at hellt/gnmi-map, you can quickly access the latest version with a shortcut: https://bit.ly/gnmi-map. Happy mapping! ","date":"2020-06-28","objectID":"/2020/gnmi-map/:0:0","tags":["gnmi","openconfig"],"title":"gNMI Map","uri":"/2020/gnmi-map/"},{"categories":null,"content":"The power of a packet capture is boundless‚Ä¶ Sometimes its indeed a pcap that can save you nights of troubleshooting, so being able to get one quickly and easily is an ace up a neteng sleeve. In this post I‚Äôll show you how I use Wireshark‚Äôs remote capture ability to sniff on packets running in EVE-NG without being need to install any custom plugins or packages from EVE. EVE-NG provides some integration packs with wrappers around Wireshark‚Äôs remote capture feature to make capturing a one-click task. The integration pack has all the needed software and some Duct tape to make it all work: plink 0.73 (for wireshark) all necessary wrappers It will modify windows registry files for proper work I would rather want to keep my registry untouched for a simple task like sniffing the packets from a remote location, therefore I always use Wireshark remote capture without installing any client packs from Eve. It feels more ‚Äúappropriate‚Äù, though I wouldn‚Äôt mind to install the pack in a VM that I don‚Äôt care about much. So, you are perfectly capable of sniffing on packets running in EVE by having Wireshark alone. Thats the procedure: Install wireshark In the EVE lab view grep the link name of an interface you want to capture from 2.1 right click on the device you want to capture from 2.2 select ‚ÄúCapture‚Äù menu 2.3 move mouse over the interface you want to capture from 2.4 get the interface name (vunl0_1_0 in my example) Open Wireshark and choose remote capture in the list of the capture interfaces Enter the address of your EVE hypervisor (can use names of your systems from ssh_config) Type down the interface name you got in step 2 (the capture filter statement generates automatically) Start capturing! It might look like a lot of manual steps from the first sight, but it takes actually 10 seconds, since you only need to memorize the link name and type it once in the wireshark interface. ","date":"2020-05-17","objectID":"/2020/using-wireshark-remote-capture-with-eve-ng/:0:0","tags":["wireshark","eve-ng"],"title":"Using Wireshark remote capture with EVE-NG","uri":"/2020/using-wireshark-remote-capture-with-eve-ng/"},{"categories":null,"content":"Automation Is as Good as the Data Models is a chapter‚Äôs name in the great book titled ‚ÄúNetwork Programmability With YANG‚Äù. These days you won‚Äôt bedazzle anyone by just providing the set of YANG models for the flagship network products. The models alone, albeit a great step forward, do not guarantee that programmability will start flourish. The automation tools leveraging YANG is often a missing link and in this post I am talking about the Nokia YANG tree and Path Browser tools which help both our internal automation squad and our customers to be more effective working with our YANG models. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:0:0","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"1 Models for machinesAt Nokia we distribute the YANG models via our nokia/7x50_YangModels repository. This enables us to allow users to simplify the way they get the models. The challenge with these models, or any models provided in .yang format for that matter, is that its extremely hard for a naked eye to browse/evaluate these models when doing network automation. They are great for compilers, and not as much for us - automation engineers. // first lines of ietf-interfaces.yang module module ietf-interfaces { yang-version 1.1; namespace \"urn:ietf:params:xml:ns:yang:ietf-interfaces\"; import ietf-yang-types { prefix yang; } revision 2018-02-20; container interfaces { description \"Interface parameters.\"; list interface { key \"name\"; leaf name { type string; } leaf description { type string; } leaf enabled { type boolean; default \"true\"; } Likely, browsing the ietf-interfaces.yang file won‚Äôt make you sweat, yet it shouldn‚Äôt led you to a false conclusion that YANG code representation is easy. The reality hits hard when YANG exposes its features such as groupings and uses, custom typedefs and multiple identityrefs, solid layer of XPATH here and there, twisted imports and a composition with dozens of submodules. For example, our combined model for the configuration-only data (nokia-conf-combined.yang) is 15MB in size and has 331000 lines. That is like the opposite of easy. But why is it important to peer inside the models in the first place? ","date":"2020-04-29","objectID":"/nokia-yang-tree/:1:0","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"1.1 Why browsing models is important?Truth is that every model driven (MD) interface you have in mind such as NETCONF, gNMI, RESTCONF operates on the data that is modelled in YANG. Thus every single operation you make with these interfaces eventually aligned with the underlying YANG model to access the data. And unless your automation suite leverages some advanced orchestrator-provided abstractions or code generated classes, you literally need to look at the YANG modules when using those management interfaces. NETCONF operations must have the XML envelopes created in conformance with the YANG model (example) gNMI paths are XPATH-like paths modelled after the underlying YANG model (example) RESTCONF URL embeds a model path as dictated by the YANG model (example) ","date":"2020-04-29","objectID":"/nokia-yang-tree/:1:1","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"2 YANG representationsMake no mistake: regardless of the interface you pick, you end browsing YANG models and as you can imagine, scrambling through the raw YANG model representation is not an easy task. Luckily, the better looking representations of the very same models exist. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:2:0","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"2.1 TreeThe RFC8340 YANG tree representation is the one you see everywhere in the documentation and blogs. By passing the same ietf-interfaces.yang snippet through the pyang tool we transform the module to a better looking tree-like view: +--rw interfaces | +--rw interface* [name] | +--rw name string | +--rw description? string | +--rw type identityref | +--rw enabled? boolean Compared to the .yang raw view, the tree makes it way easier to glance over the model and understand the parent-child relationships of the data entry nodes and their types. Still, it has some serious UX drawbacks an engineer will face: the path information is missing. By looking at a certain leaf/container/list of a tree you can‚Äôt easily say what is the path of that element starting from the root? Yet it is very important to have this information, since it enables you to create XPATH filters for xCONF or paths to subscribe to with gNMI. it is hard to navigate the large models. Since its the text file you are looking at, you can‚Äôt expand/collapse the data nodes on request. Its flushed to you in its entirety, and if the model is big enough you will easily loose the sense of where you are. Consider the following snippet of a single screen of text I captured from a real YANG module; is it easy to understand where are you standing at? ","date":"2020-04-29","objectID":"/nokia-yang-tree/:2:1","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"2.2 HTML treeFortunately for us, pyang supports many output formats and one of them - jstree - is a mix of the model‚Äôs tree structure with HTML features. The outcome of this mixture is the self-contained, offline HTML page that crosses off the drawbacks outlined in the previous section. In this mode we are having the full control on which part of the model we want to explore and which branches we want to leave collapsed to not clutter the view. This might sound like a small thing, but actually it boosts the user experience quite substantially. Another improvement over the textual tree view is the path information that is provided for each element of the model. As explained above, these paths are essential to have for the following reasons: understand the parental path of the element of interest to, say, create the NETCONF XML envelope. use these paths in gNMI subscribe paths. use these paths with the tools that can generate data based on it (like XML skeleton). And HTML tree delivers on that promise by providing the path information for every element: Its does not really strike like a needed feature when you have a compact module like ietf-interface, but consider a heavier model where a certain leaf is sometimes 10 levels deep from the root: On a model like this its dead obvious that a textual tree won‚Äôt be of help due to the progressively increased nesting of the elements, thus ‚ÄúHTML tree‚Äù seems like a reasonable view to use. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:2:2","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"3 Nokia YANG tree repositoryNokia distributes the YANG models for 7x50 routers in two forms: combined models: all the submodules are grouped under the respective top level roots and the following combined YANG modules are produced: nokia-conf-combined.yang and nokia-state-combined.yang individual models: the submodules are kept in their own YANG files. The combined modules provide a unique one-stop shop for the configuration and state YANG view, therefore I always use the combined models as they have all the elements nicely grouped under a single root. Due to the substantial size of the combined models it takes quite some time for pyang to generate the tree views; I quickly got tired of generating the tree views for each new minor release of SR OS myself. So I decided to generate them automatically for each new set of YANG modules Nokia pushes to the nokia/7x50_YangModels repo. That is how hellt/nokia-yangtree repo was born. The repository features: various views of the Nokia combined YANG models (text tree, xml, html tree) as well as stores the XPATH paths extracted from the models. A user can clone the repo and gain access to all of these formats YANG Browser that serves the ‚ÄúHTML tree‚Äù views of the combined models so that our users could consume these models online without a need to generate them Path Browser that enables search functionality over the extracted model paths repository directory layout The repo navigation is built on the basis of git tags, that means that a certain set of YANG views will be shown to a user when they select a certain tag that matches the SR OS release number: ","date":"2020-04-29","objectID":"/nokia-yang-tree/:3:0","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"3.1 YANG BrowserAs briefly explained before, the YANG Browser is merely an HTTP server that serves the HTML tree views of the combined models generated with pyang. I foresee this to be the main interface for the SR OS automation engineers to consume the YANG models, it is always available, easy to navigate, free and requires just a browser. How YANG Browser works: A user selects an SR OS release as shown in animation above Once the release is selected the HTML tree links in the section 2 for the relevant datastores will point to the right URLs. Clicking on a link will open a new tab with the HTML Tree view (note, it might take a few minutes to a browser to load and render this big HTML file). HTML tree view for the nokia-state-combined module Using this page a user can answer most of the questions related to the YANG modules used by the Nokia 7750 router. The always on HTML tree view is amazing, but it still has some flaws. One particular case that can‚Äôt be solved with YANG Browser is filtering the model‚Äôs paths with a keyword. To answer that request we created the Path Browser. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:3:1","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"3.2 Path BrowserImagine a request comes in asking to identify all the leaves that relate to the alarm status of the port/chassis/fan/optics/etc. Quite a standard task for every monitoring activity, which is not easy to answer without the proper tooling. How would you use a YANG Browser if you don‚Äôt know which containers have or haven‚Äôt the alarm related leaves inside? Opening all of them will become a nightmare, as well as expanding all the elements and perform a full-text search. What would be nice to have is a search actions on the paths that have some keywords inside them, like alarm. For that particular set of the use cases we created the Path Browser. The Path Browser links are in the section 2 of the repo readme. First, my colleague wrote a tool that extracts a list of XPATH compatible paths for a given model. The text file with the list of paths is part of the hellt/nokia-yangtree repo. $ head -10 sros_20.2.r2-nokia-state-combined-paths.txt nokia-state | /state/aaa/radius/statistics/coa/dropped/bad-authentication | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/dropped/missing-auth-policy | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/dropped/invalid | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/dropped/missing-resource | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/received | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/accepted | yang:counter32 nokia-state | /state/aaa/radius/statistics/coa/rejected | yang:counter32 nokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/bad-authentication | yang:counter32 nokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/missing-auth-policy | yang:counter32 nokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/invalid | yang:counter32 The format of the path entries follows the pattern of module_name | path | type. And having this file alone allows you to leverage CLI tools magic to filter on this massive data set: $ grep \"/port.*alarm\" sros_20.2.r2-nokia-state-combined-paths.txt | head -5 nokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/temperature/high-alarm | decimal64 nokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/temperature/low-alarm | decimal64 nokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-bias-current/high-alarm | decimal64 nokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-bias-current/low-alarm | decimal64 nokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-output-power/high-alarm | decimal64 The paths are XPATH and gNMI compatible. You can paste it to the telemetry collector and they would work. The next step was to build a web service with the same functionality, so I added datatables to the mix and generated the HTML pages with the filtering capabilities built-in. With a service like that you can efficiently and plain easy search through the Nokia modules for the leaves having certain keywords. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:3:2","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"4 SummaryBy leveraging the opensource tools and by writing our own paths extractor we have created a DIY YANG browsing set of instruments that greatly help network automation engineers working with Nokia gear. Understanding the utter importance of YANG, it was imperative for me to make these models more convenient to consume and, at the same time, keeping it open and free. As a result of that effort, the community now can use YANG Browser to breeze through the Nokia YANG modules and Path Browser comes to help when the users need to perform a search for the certain leaves. ","date":"2020-04-29","objectID":"/nokia-yang-tree/:4:0","tags":["nokia","sros","yang"],"title":"Nokia YANG tree and Path Browser","uri":"/nokia-yang-tree/"},{"categories":null,"content":"I bet every one of you was in a situation when you bloody needed to expose some local resource over internet. Letting a remote colleague to look at your work, delivering a demo being off-VPN, or any other reason to have your service be reachable over Internet. And it was never easy; corporate firewalls stand on-guard ensuring you can‚Äôt be agile and productive üòâ In this post I‚Äôll share with you how I glue ngrok and fwd tools together to make my routers management interfaces exposed over Internet in a few clicks for free. My story will be based on the following ‚Äúnetwork-automation engineer‚Äôs‚Äù requirement: Make router‚Äôs management interfaces available over internet (SSH, Netconf, gNMI) Make this ‚Äúexposure‚Äù process quick to set up/tear down Make it free of charge In my case I am talking about management interfaces of a router, but it can be any application that relies on TCP/UDP transport that you can expose that way. By the end of this post you will see that we deliver on all of these requirements. ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:0:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"You shall (not) passYou have a router with these shiny management interfaces. But this poor thing is so locked up‚Ä¶ Most of the times there are zero chances you can configure ingress access due to the myriads of elements out of your control (corp firewall being the most tricky one). There might even not be any EVE-NG or Openstack in the picture, but this BFF9000 fella will be there, and we are about to penetrate it. ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:1:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"Enter ngrokSince opening the ports or configuring the port forwarding is not gonna help us much, we will use the reverse technique: opening the connections in the egress direction. Sending traffic in the egress direction is usually not a problem, we can leverage ngrok and expose the needed ports through it: If you never heard of ngrok, I suggest you go through their website, its a very powerful tool to setup tunnels. That way we can expose any TCP/UDP port of a machine that runs ngrok client via publicly accessible sockets (tcp://xx.ngrok.io:\u003cport\u003e) for free. That looks like a nice idea and quite a realistic one, but not that many routers allow users to install ELF binaries and run them. This means that we can‚Äôt run ngrok client on the router itself and establish tunnels. But we can definitely install ngrok on a machine that can reach our routers. In my case that will be the EVE-NG hypervisor/VM, since it can reach all the routers that run inside of it. ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:2:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"Enter fwdSince ngrok is only capable of exposing the ports of the host the client runs on, we would end up with ports of the Linux host exposed, but not the router‚Äôs. The missing piece is the forwarder process that will stitch the ngrok-exposed ports of the linux host with the respective ports of the remote router. Let‚Äôs dissect this two stage process of setting the tunnels up: expose the local ports of the linux VM with ngrok. This linux VM can reach the router. run fwd tool that will forward the traffic appearing on the local ports we exposed with ngrok in step 1 That way we bridge the linux ports exposed with ngrok with the ports on the router. If that sounds confusing, lets go through the example. ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:3:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"socat vs fwdIf you (as me) will experience some issues with fwd reporting broken pipes there is an old-school alternative - socat. As with fwd, you can concatenate connections in the following way: # requests coming to localhost:11122 will be forwarded to 10.2.0.11:22 socat tcp-listen:11122,reuseaddr,fork tcp:10.2.0.11:22 ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:4:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"Practical exampleWe start first with exposing the ports of the machine that runs ngrok client and has IP reachability with our router. To expose multiple ports in a quick and easy way, I suggest you leverage the ngrok configuration file which can resemble smth like this: # cat ngrok.cfgauthtoken:\"MYTOKEN\"log_level:warnlog:/tmp/ngrok.logregion:eutunnels:gnmi_r1:addr:57401proto:tcpnc_r1:addr:11831proto:tcpssh_r1:addr:11122proto:tcp With this configuration we command ngrok to expose the TCP ports 57401, 831, 11122 on the linux VM. ngrok start -config ngrok_cfg.cfg --all And voil√°, these ports are now Internet-reachable: ngrok by @inconshreveable Session Status online Account cats_admin (Plan: Free) Version 2.3.35 Region Europe (eu) Web Interface http://127.0.0.1:4040 Forwarding tcp://0.tcp.eu.ngrok.io:13621 -\u003e localhost:11831 Forwarding tcp://0.tcp.eu.ngrok.io:16704 -\u003e localhost:57401 Forwarding tcp://0.tcp.eu.ngrok.io:19968 -\u003e localhost:11122 Connections ttl opn rt1 rt5 p50 p90 0 0 0.00 0.00 0.00 0.00 But this won‚Äôt work yet as we wanted. ssh -p 19968 admin@0.tcp.eu.ngrok.io ssh_exchange_identification: Connection closed by remote host As we clarified before, nothing listens on these ports, since they are not of router' but of the linux machine running ngrok client. There is one step left. Start the fwd processes and stitch the local ports with the router' ports. Since fwd cant read (yet) the configuration file, I created a dumb script: fwd --from localhost:57401 --to 10.1.0.11:57400 \u0026 fwd --from localhost:11831 --to 10.1.0.11:830 \u0026 fwd --from localhost:11122 --to 10.1.0.11:22 \u0026 Thats the missing piece to propagate our tunnels all the way to the router with IP address of 10.1.0.11 in my example. And now we‚Äôre in business! ssh: $ ssh -p 19968 admin@0.tcp.eu.ngrok.io admin@0.tcp.eu.ngrok.io's password: SR OS Software Copyright (c) Nokia 2019. All Rights Reserved. [] A:admin@R1# netconf: $ ssh -p 13621 admin@0.tcp.eu.ngrok.io -s netconf admin@0.tcp.eu.ngrok.io's password: \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003chello xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"\u003e \u003ccapabilities\u003e \u003ccapability\u003eurn:ietf:params:netconf:base:1.0\u003c/capability\u003e \u003ccapability\u003eurn:ietf:params:netconf:base:1.1\u003c/capability\u003e gNMI: $ myAwesomegNMIClient -a 0.tcp.ngrok.io:16704 -u admin -p admin --insecure cap gNMI_Version: 0.7.0 supported models: - nokia-conf, Nokia, 19.10.R2 - nokia-state, Nokia, 19.10.R2 - nokia-li-state, Nokia, 19.10.R2 - nokia-li-conf, Nokia, 19.10.R2 Now to stop this we simply kill the fwd processes and ngrok: pkill fwd \u0026\u0026 pkill ngrok ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:5:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"Summary With ngrok and fwd we have been able to expose the local router' ports with two clicks in the terminal. The tunnels are persistent and free to use. Tear down process is as simple as pkill fwd \u0026\u0026 pkill ngrok you can monitor established tunnels with ngrok console (free) you can try inlets or argo tunnels for similar capabilities What are your ways to reach your routers in a lab, share in the comments? ","date":"2020-04-25","objectID":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/:6:0","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"],"title":"Easily exposing your local resources with ngrok and fwd","uri":"/2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/"},{"categories":null,"content":"We can praise YANG as long as we want, but for an end user YANG is useful as the tooling around it and the applications leveraging it. Ask yourself, as a user of any kind of NETCONF/YANG application what was the last time you looked at a *.yang file content and found something that was needed to consume that application? In a user role I personally never look at a YANG source, though, I look at the tree or HTML representation of YANG all the time; Thats is the YANG human interface for me. And even in these human friendly formats you can‚Äôt find all the answers; for example, looking at the YANG tree, how do you get the XML data sample of a given leaf? Thats what we will discover in this post. Problem statementGetting the XML data sample of a given leaf? What is this, why might I need it? Lets work through a real-life example that should make a perfect point. Suppose you need to get a list of configured users from a given network element (NE). You would normally do this by leveraging \u003cget-config\u003e operation, but in order to get only the users portion of the configuration, you would need to augment your request with a filter. NETCONF defaults to subtree filtering when it comes to filters. \u003c!-- subtree filter example from RFC6241 --\u003e \u003crpc message-id=\"101\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"\u003e \u003cget-config\u003e \u003csource\u003e \u003crunning/\u003e \u003c/source\u003e \u003cfilter type=\"subtree\"\u003e \u003ctop xmlns=\"http://example.com/schema/1.2/config\"\u003e \u003cusers/\u003e \u003c/top\u003e \u003c/filter\u003e \u003c/get-config\u003e \u003c/rpc\u003e Now the question comes: how do one know how to craft the subtree filter XML data for a given NE? If you will send the above XML envelope to any NE you will receive an error, because the subtree filter provided is not how the users are modelled in the underlying YANG model. Yes, it boils down to an underlying model used by a given NE; you would need to consult with this model and derive the right nodes to get to the configuration block in question. Actually, that post is a feedback to the question that popped up in my twitter recently: Hello @nickrusso42518 @ntdvps whats the best way to transform a YANG file/section into a XML filter to use in a NETCONF get message? I used to export YANG file with pyang to html format, but it bothers me put the exact tree by hand. Are there an easy way? ‚Äî Rafael Ganascim (@rganascim) January 31, 2020 Solving the problem with PYANGRafael asked a very practical question that every NETCONF user encounters; ours example follows the same question by asking how do I know which XML data to use in my subtree filter to get users config, are there aby tools for that? It didn‚Äôt take Rafael long to come up with a solution to his own question, which he explained in the same thread: I found an \"easy\" way to get the xml tree, instead of writing by hand ‚Äî Rafael Ganascim (@rganascim) January 31, 2020 As you can see, he leveraged PYANG and solved the problem with a grain of sed salt. The steps he took can be categorized with 4 major steps: Generated HTML view of a YANG model (jstree output format) Copy the path of a node in question Remove the prefix from that path Generate XML skeleton data for that cleaned path Lets solve our example question following this method and using Nokia SR OS router running 19.10.r2. First, lets enjoy the generated HTML views of Nokia SR OS models provided in the nokia-yangtree repo, no need to generate anything yourself, we value your time and here we got you covered. Few clicks away and you drill down to the user list of the configuration model. Thats where our configured local users live. To our grief, PYANG cant digest the path that it produces in its Path column of the HTML view, therefore we need to sanitize it and remove the path prefix (conf in our case) from it: # path in the HTML: /conf:configure/conf:system/conf:security/conf:user-params/conf:local-user/conf:user /configure/system/security/user-params/local-user/user SR OS PRO TIP that makes competition angry You can get the model path right out from the box when you","date":"2020-02-01","objectID":"/2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/:0:0","tags":["Netconf","Docker","YANG"],"title":"Getting XML data sample for a given leaf in a YANG model","uri":"/2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/"},{"categories":null,"content":"Its an engineers core ability to decompose a complex task in a set of a smaller, easy to understand and perform sub-tasks. Be it a feature-rich program that is decomposed to classes, functions and APIs or a huge business operation captured in steps in a Methods Of Procedure document. In a network automation field where the configuration protocols such as NETCONF or gRPC are emerging, it is always needed to have a quick way to validate an RPC or Notification feature before implementing this in a code or a workflow. This blog post is about a handy tool called netconf-console which allows you to interface with your network device using NETCONF quick and easy. And, of course, I packed it in a smallish container so you can enjoy it hassle-free on every docker-enabled host. netconf-console is a tool from Tail-f that basically gives you a NETCONF client for your console. That is exactly the packaging that I appreciate to have when I need to play with NETCONF. Cold-starting a python project with ncclient is much slower and you need ensure that you have all the RPCs coded, meh. With the console client you have almost anything you need to start tinkering with NETCONF enabled device. # netconf-console --host=example.com --db candidate --lock --edit-config=fragment1.xml \\ --rpc=commit-confirmed.xml --unlock --sleep 5 --rpc=confirm.xml Moreover, you can have an interactive NETCONF console to your device: # netconf-console --host=example.com -i netconf\u003e lock netconf\u003e edit-config fragment1.xml --db candidate netconf\u003e rpc commit-confirmed.xml netconf\u003e unlock netconf\u003e get-config netconf\u003e rpc confirm.xml ","date":"2020-01-28","objectID":"/2020/netconf-console-in-a-docker-container/:0:0","tags":["Netconf","Docker"],"title":"NETCONF console in a docker container","uri":"/2020/netconf-console-in-a-docker-container/"},{"categories":null,"content":"ContainerizeI‚Äôve been spoiled by Rust/Go tools that are self-contained, dependency-free and almost platform-agnostic. To achieve the same level of hassle-free for python tool I practice containerization. So I decided to put netconf-console in a whale protected cage by building a multi-stage docker image. Even though there are some images for the netconf-console, they are all outdated, based on an old version of the tool and use python2 under the hood. Its 2020 here, so I wanted to create a fresh, small image based off of the recent netconf-console code and running with python3. Of course you can install the netconf-console with pip as usual: pip install netconf-console So here it is, a multi-stage build Dockerfile that builds netconf-console in Alpine linux with python3.7. The result of this build can be found at the relevant docker hub page. ","date":"2020-01-28","objectID":"/2020/netconf-console-in-a-docker-container/:1:0","tags":["Netconf","Docker"],"title":"NETCONF console in a docker container","uri":"/2020/netconf-console-in-a-docker-container/"},{"categories":null,"content":"TagsThe docker image will be tagged in accordance with the release version numbers of the netconf-console; at the time of this writing, the latest version is 2.2.0, hence you will find the image with the corresponding tag. Also, the latest tag will point to the most recent version. ","date":"2020-01-28","objectID":"/2020/netconf-console-in-a-docker-container/:1:1","tags":["Netconf","Docker"],"title":"NETCONF console in a docker container","uri":"/2020/netconf-console-in-a-docker-container/"},{"categories":null,"content":"InstallationAs with any other docker image, all it takes is to make a pull: docker pull hellt/netconf-console ","date":"2020-01-28","objectID":"/2020/netconf-console-in-a-docker-container/:2:0","tags":["Netconf","Docker"],"title":"NETCONF console in a docker container","uri":"/2020/netconf-console-in-a-docker-container/"},{"categories":null,"content":"Usage examplesThe entry point of the docker image is the netconf-console itself, therefore you can run it almost in the same way as you‚Äôd do with a standalone installation - by providing the arguments to the callable. # verify that the tool is properly working, docker run --rm -it hellt/netconf-console --help usage: netconf-console [-h] [-s [{plain,noaaa} [{plain,noaaa} ...]]] [--db DB] [--timeout TIMEOUT] [--with-defaults {explicit,trim,report-all,report-all-tagged}] [--with-inactive] [-x XPATH] [-t {test-then-set,set,test-only}] [-o {merge,replace,create}] [--del-operation {remove,delete}] [-v VERSION] [-u USERNAME] [-p [PASSWORD]] [--host HOST] [-r REPLY_TIMEOUT] [--port PORT] [--privKeyFile PRIVKEYFILE] [--raw [RAW]] [--tcp] [-N [NS [NS ...]]] [--debug] [--hello] [--get [GET]] [--get-config [GET_CONFIG]] [--kill-session SESSION_ID] [--discard-changes] [--lock] [--unlock] [--commit [{confirmed}]] [--validate [VALIDATE]] [--copy-running-to-startup] [--copy-config [COPY_CONFIG]] [--edit-config [EDIT_CONFIG [EDIT_CONFIG ...]]] [--set [SET [SET ...]]] [--delete [DELETE [DELETE ...]]] [--create [CREATE [CREATE ...]]] [--get-schema GET_SCHEMA] [--create-subscription [CREATE_SUBSCRIPTION]] [--rpc [RPC]] [--sleep SLEEP] [-e EXPR] [--dry] [--interactive] [filename] The interactive console mode, of course, also works: docker run -it --rm hellt/netconf-console --host=10.1.0.11 --port=830 -u admin -p admin -i netconf\u003e hello \u003c?xml version='1.0' encoding='UTF-8'?\u003e \u003cnc:hello xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\"\u003e \u003cnc:capabilities\u003e \u003cnc:capability\u003eurn:ietf:params:netconf:base:1.0\u003c/nc:capability\u003e \u003cnc:capability\u003eurn:ietf:params:netconf:base:1.1\u003c/nc:capability\u003e \u003c\u003cSNIPPED\u003e\u003e Now a more real-life example would be to throw a NETCONF RPC to the target device: $ cat test.xml \u003crpc message-id=\"113\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"\u003e \u003cget\u003e \u003cfilter\u003e \u003cstate xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser/\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/state\u003e \u003c/filter\u003e \u003c/get\u003e \u003c/rpc\u003e $ docker run -it --rm -v $(pwd):/rpc hellt/netconf-console --host=10.1.0.11 --port=830 -u admin -p admin test.xml \u003c?xml version='1.0' encoding='UTF-8'?\u003e \u003crpc-reply xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"113\"\u003e \u003cdata\u003e \u003cstate xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"\u003e \u003csystem\u003e \u003csecurity\u003e \u003cuser-params\u003e \u003clocal-user\u003e \u003cuser\u003e \u003cuser-name\u003eadmin\u003c/user-name\u003e \u003cattempted-logins\u003e13\u003c/attempted-logins\u003e \u003cfailed-logins\u003e0\u003c/failed-logins\u003e \u003clocked-out\u003efalse\u003c/locked-out\u003e \u003cpassword-changed-time\u003e2020-01-24T23:27:33.0Z\u003c/password-changed-time\u003e \u003c/user\u003e \u003cuser\u003e \u003cuser-name\u003egrpc\u003c/user-name\u003e \u003cattempted-logins\u003e0\u003c/attempted-logins\u003e \u003cfailed-logins\u003e0\u003c/failed-logins\u003e \u003clocked-out\u003efalse\u003c/locked-out\u003e \u003cpassword-changed-time\u003e2020-01-24T23:27:35.0Z\u003c/password-changed-time\u003e \u003c/user\u003e \u003c/local-user\u003e \u003c/user-params\u003e \u003c/security\u003e \u003c/system\u003e \u003c/state\u003e \u003c/data\u003e \u003c/rpc-reply\u003e Note that the WORKDIR of the container image is set to /rpc, therefore mounting the directory with your RPCs to that mountpoint will allow to refer to the file names directly. And you can create pretty complex ad-hoc RPCs with locking the datastore, committing and discarding the changes effortlessly. ","date":"2020-01-28","objectID":"/2020/netconf-console-in-a-docker-container/:3:0","tags":["Netconf","Docker"],"title":"NETCONF console in a docker container","uri":"/2020/netconf-console-in-a-docker-container/"},{"categories":null,"content":"Not a single day goes by without me regretting I haven‚Äôt mastered any front-end technology like React/Angular or the likes. Why would a network engineer want to step into the game that seems orthogonal to its main area of expertise, one might ask? Truth be told, I wasn‚Äôt born with an urge to learn anything that has javascript under the hood, but over the years, working within the network/backend silos, I realized, that being able to create a simple front-end service is a jewel that fits every crown, no matter what title you wear. This tutorial is based on the task real task of building up a web interface (pycatjify.netdevops.me) for the pycatjify REST API service deployed as a serverless function. The end result is a simple, completely free and reusable Bootstrap based front-end boilerplate which can be used as a foundation for a similar task. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:0:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"1 Benefits of knowing how to front-end?Lets me first explain why I think that even a basic experience with any front-end technology is beneficial to virtually anyone. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:1:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"1.1 Get your tool a web interfaceWe often start with an idea of a tool and work it to a completion by publishing a command line interface to it, sometimes the CLI is all the tool needs, it is just best consumed that way. Other times even the CLI is not needed, as the tool is only used as a plugged-in library. But quite often the tool can benefit greatly by having its own web interface. You can broaden the horizons of your project audience vastly by simply creating a web service out of it. I can name a handful number of tools that I consume via web instead of using their CLI counterparts, it is just more convenient to me and so might think the users of your tools. The pycatj is a perfect example of a CLI-first tool that can be conveniently consumed via web as well. Thus I set myself on a journey to create a web facade for it and at the same time reinforcing my very basic web skills. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:1:1","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"1.2 Take your pitch or demo to a next levelNot everyone of us is working in an environment where the bosses have engineering background and can equally enjoy a demo of a new service by looking at the terminal full of curl requests. Even if your bosses are the ones who contribute to the cutting edge technologies, your customers can easily be made of a different dough. Therefore it might be equally important to supplement your neat idea with a proper visualization; my experience says that a great tool or a service attracts audience way better when it is wrapped in a shiny package. So having a prototyped web UI might give you some bonus points even if it is not going to be consumed via the Web UI after all. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:1:2","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"1.3 Learn how they do it on the other side of a fenceA classic, book-pictured network automation engineer is an all Python-shop customer. Although Python is a natural fit for the network automation activities, it is also important to not less yourself be constrained by a singe language or a toolchain. Educating yourself on a different technology with a different set of the instruments and/or the views means a lot. Even by scratching the surface of the Javascript, its package managers and the front-end frameworks could make you better understand the pros and cons of the ecosystem you are in. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:1:3","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"2 Front-end \u0026 Javascript frameworks So how do one start if they want to learn any of that shiny front-end witchery given that there are so many frameworks around? In spite to answer this question I compiled the following list of options that when I approached the task of making a pycatjify web service: Frameworkless: bare HTML/CSS/JS This is the most straightforward way of creating a web service. You basically write everything by yourself without relying on any framework. On the pros side this is the most lightweight and bloat-less approach, as you are in the full control of what contributes to the end result. The cons side is substantial though, you need to be well experienced in the HTML/CSS/JS to create something less minimalistic than a blank page with the elements on it. Front-end frameworks Front-end frameworks provide a shortcut for a web service creation drastically reducing time to create one. Also known as CSS frameworks they come across with the lego-like blocks (components implemented with CSS/JS/HTML) that you use to build a web service from. Dozens of front-end frameworks have been created over the time, from the minimalistic ones to the monstrous software bundles. Bootstrap, Foundation, Skeleton, Materialize are one of the few that one can find in the numerous ‚Äútop front-end frameworks‚Äù charts. A major benefit that all above mentioned frameworks share is that they don‚Äôt need to be compiled and can be run by all modern browsers. All it takes is to put the framework' HTML/CSS/JS files along with your project and open the index.html. Javascript frameworks and libraries: React/Angular/Vue/etc These are the modern age Javascript frameworks (often referred as libraries) that enable you to build modern web/mobile applications with a feature-rich logic. With the great power, though, comes the great size and complexity; installing a sample React app can easily add thousands of JS packages that framework depends on. The learning curve for these frameworks is steep as opposed to the front-end frameworks listed in [2]. But mastering one of them would enable you to create versatile and breathtaking Web UIs as well as mobile applications. Notable frameworks in that category are React, Vue, Angular, Ember. Since I am not a front-end developer the sweet spot for me lies with the front-end frameworks that I can install/run without a specific harness. They are lightweight, easy to work with, and all it takes to start is the basic HTML/CSS/JS knowledge. At the same time they provide just enough features to handle not overly complicated tasks a network engineer might encounter in a small size projects. For the pycatjify.netdevops.me I decided to use a ‚ÄúMaterial Design‚Äù flavored Bootstrap based framework called mdboostrap. Also I had some past experience with the Bootstrap 3 framework when I worked on a Web UI for the python scripts quite some time ago. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:2:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"3 Mdbootstrap Mdbootstrap (MDB) offers the Material-UI themed components for the various JS frameworks such as Bootstrap/JQuery, Angular, React and Vue. For the reasons outlined in section 2, I decided to go with a Bootstrap/JQuery version of the MDB framework as this is the easiest way to put up a simple front-end service for me. MDB offers a free quick start guide as well as a full-length tutorial if you want to refresh the bootstrap basics or follow an authored paid course. Bootstrap popularity also makes it extremely easy to find a lot of guides and tutorials that tremendously help to understand the basics of this framework. MDB, being based on the Bootstrap 4, obviously follows its ancestors paradigms when it comes to the Grid system, CSS, components, etc. If you worked with the Bootstrap before then the MDB won‚Äôt be a problem at all. Moreover, the elements I used in this project are not MDB specific, the same components are available in the original Bootstrap library. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:3:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"3.1 Install the frameworkIts extremely easy to ‚Äúinstall‚Äù mdbootstrap/bootstrap framework, its hardly an installation even, as all you need is to download the archive and extract the framework' files. Once done, the framework contents is nothing more than a small number of the folders and files: . ‚îú‚îÄ‚îÄ [drwxr-xr-x] css ‚îú‚îÄ‚îÄ [drwxr-xr-x] font ‚îú‚îÄ‚îÄ [drwxr-xr-x] img ‚îú‚îÄ‚îÄ [drwxr-xr-x] js ‚îú‚îÄ‚îÄ [drwxr-xr-x] scss ‚îî‚îÄ‚îÄ [-rwxr-xr-x] index.html 5 directories, 4 files Yes, thats literally all you need, no packages installation no dependency management, just static files, classy! You can open the index.html with your browser and it‚Äôll just work. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:3:1","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"3.2 Framework structureThe framework comes with the following important components that make it all work in a unison to display the web page built with it: CSS files in the css folder that define the styling of the framework elements and controls Javascript files in the js folder that comprise the dynamic logic that the framework relies on Static images in the img folder as well as the fonts in the font directory Index HTML file that in a simple case will have all the website contents Take a look at the index.html file that comes with a framework: In the \u003chead\u003e section of this HTML file the CSS files are being loaded. These CSS files comprise a big portion of the framework itself, as they govern the styling that the components have. Then the \u003cbody\u003e section of the HTML file holds the default web page‚Äôs content. The index.html file that comes with a template has a div with a few headers and a paragraph of text. You will replace then with the framework components like Navigation bars, input forms, tables, text elements, modal dialogs when you start to build your front-end service. In the ending of the \u003cbody\u003e section you would find the \u003cscript\u003e elements that load the Javascript code the framework relies on. The custom JS code that your service most likely will have will also be added in the body‚Äôs tail section (see section 4 for an example). ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:3:2","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"3.3 Bootstrap components are your lego blocksThe framework‚Äôs library has a lot of components that might be treated like the lego blocks with which you build the web facade. The benefit of having the pre-created components is huge; you don‚Äôt need to create these common components yourself from the ground up, just browse the library and pick the right ones. Example of the tabs \u0026 pills components To understand which components I‚Äôd need for the pycatjify I imagined what layout would I want my page to have. Since pycatj is a tool that works on an input JSON/YAML data and produces a multi line output, the simple layout could consist of a navigation bar with the project logo, the two input fields for input and output data and the modal dialog with cards. Knowing the needed basic building blocks we can now browse the framework' documentation section in search for the right ones. The MDB docs are just great for that - lots of examples on how to use the various components in different kinds of shapes and sizes. Basically you copy the example from the docs, paste it to your HTML file and tune it as per the components options which are explained in the docs. When building pycatjify front-end I just removed the default contents of the \u003cbody\u003e section of the index.html file and started to throw in the components as per the layout I had in my head. Thats what the index.html for the pycatjify.netdevops.me started to look like when I added all the components I talked above. It looks like a lot of lines of code, but everything was just pasted from the examples section. First time it takes some time to get to know the components and their behavior, but do it once and the next project would be an effortless task. pycatjify web ui ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:3:3","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"4 Hooking up the back-endAs implied by the name of this post, the communication between the front-end and the back-end is happening using the REST API. In the previous post I wrote about the way I packaged the pycatj tool into a Google Cloud Function which exposes a single API endpoint. Now it is time to make our front-end to be a REST API client that talks to the back-end and displays the results it receives back. This is a breakdown of a communication logic between the front and back ends: Capture the user input (which is a YAML or JSON formatted text) from the input field Send it via HTTP POST request to the back-end API endpoint in a JSON format Back-end to receives a request and does the transformation of the received data It then sends the transformed data back as a string packed in a JSON body as well. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:4:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"4.1 REST API client with JQuery/AJAXThere are several ways of making an asynchronous HTTP request from within the front-end service. One approach would be by using the JQuery‚Äôs AJAX function. Since MDB framework has JQuery as its dependency and this library is already loaded into our page we can use it right away. Lets add a Javascript file at the js/pycatjify.js path that will implement the logic of a REST client. This little unnamed function is bound to the button with id convert_btn by the means of the #convert_btn selector. Specifically to its click action. That means that when a click action occurs on the convert_btn button, this JS code kicks in. In the very beginning the code reads the data from the input element text area into the data[\"pycatj_data\"] object. Next, it serializes the variable value into the JSON string since we chose to use JSON payload with our POST request. And then the actual AJAX request (which is essentially a JQuery name for the async HTTP call) comes into play: $.ajax({ url: \"https://us-central1-pycatj.cloudfunctions.net/pycatjify\", contentType: \"application/json\", data: body, dataType: \"json\", type: 'POST', success: function (response) { $('#out_form').val(response.data) } }); With the url parameter we say what is the URL of our REST API endpoint contentType set to application/json narrows down the type of the content we will convey through HTTP messages the data that we send with this specific request is contained in the body variable computed before dataType: \"json\" allows us to treat the returned response as the JSON object, and since our pycatj serverless function returns the JSON it is exactly what we need. the request type is POST If our POST request succeeds and we receive a response, we call a function that displays the results received as a JSON. Because of our serverless function returns the data in a data field, we select this field with the response.data selector in the $('#out_form').val(response.data) expression. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:4:1","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"5 Hosting the web applicationSince our back-end code is hosted by a GCP Function, the front-end itself is nothing more than a bunch of static files (CSS, JS, HTML), and that means that it can easily be hosted for free with the beautiful Gitlab Pages service. For that I added a .gitlab-ci.yml file that has a single pages job responsible for copying the web service related files to the public directory which, in its turn, tells Gitlab to start serving these files over HTTP. Now with every push to the master branch Gitlab will restart the web server to ensure that the most recent files are being served. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:5:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"6 SummaryThis pretty much concludes the Minimum Viable Product of the web front-end for the simple REST API service: by leveraging the Google Cloud PLatform Functions we deployed a python code that implements a back-end REST API service - $0 the front-end is built with a simple Bootstrap/JQuery based MDB framework and hosted with Gitlab Pages - $0 the wildcard TLS certificate is provided by Cloudflare - $0 As you see, the process of putting a simple front-end service is simple and completely free. It goes without saying, that the example presented in this topic uses a very basic layout and a straightforward design - hence the overall simplicity. For instance the it does not handle any errors and does not perform input validation. Adding the spinner element to the UI to indicate the processing time would also enhance the UX. You can imagine, that adding all of these features increases the complexity of the code base and might require additional components and/or libraries. I hope this ‚Äúhow to create a front-end being not a front-ender‚Äù post helps you with the basics of a simple front-end machinery. Remember, its important to start, and its easier to start small, you can always grow later. And I think the Bootstrap-like frameworks are a good choice for that. Checkout the project‚Äôs source code and leave the comments or ask questions below if I missed something. ","date":"2019-07-28","objectID":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/:6:0","tags":["javascript","frontend","bootstrap"],"title":"Creating a Bootstrap based front-end for your simple REST service","uri":"/2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/"},{"categories":null,"content":"Two years ago I shared my experience on building the AWS Lambda function for a python project of my own. And a few days ago I stumbled upon a nice opensource CLI tool that I immediately wanted to transform in a web service. Naturally, a simple, single-purpose tool is a perfect candidate for function-as-a-service (FaaS), and since I had past experience with AWS Lambda, this time I decided to meet its Google‚Äôs sibling - Google Cloud Function. In this post we‚Äôll discover how to take a python package with 3rd party dependencies, make a GCP Function from it and deploy it without a single click in the UI - all without leaving the IDE. [Project‚Äôs source code] The python tool I considered a natural fit for a Cloud Function is a pycatj by David Barroso that he released just recently. For those with problems accessing yaml/json data: https://t.co/IbbS3x05bq ‚Äî David Barroso (@dbarrosop) June 26, 2019 This tool helps you to map a JSON/YAML file to a Python dictionary highlighting the keys you need to access the nested data: $ cat tests/data/test_1.json { \"somekey\": \"somevalue\", \"somenumber\": 123, \"a_dict\": { \"asd\": \"123\", \"qwe\": [1, 2, 3], \"nested_dict\": { \"das\": 31, \"qwe\": \"asd\" } } } $ pycatj --root my_var tests/data/test_1.json my_var[\"somekey\"] = \"somevalue\" my_var[\"somenumber\"] = 123 my_var[\"a_dict\"][\"asd\"] = \"123\" my_var[\"a_dict\"][\"qwe\"][0] = 1 my_var[\"a_dict\"][\"qwe\"][1] = 2 my_var[\"a_dict\"][\"qwe\"][2] = 3 my_var[\"a_dict\"][\"nested_dict\"][\"das\"] = 31 my_var[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\" I felt like having a single-page web service that would do these transformations leveraging the pycatj would be helpful to somebody sometime. Probably the easiest way would be to rewrite the same code with JavaScript and create a static page with that code without any backend, but does it spark joy?. Not even a bit. And as a starting point I decided to create a serverless function that will rely on pycatj and will be triggered later by a Web frontend with an HTTP request carrying the content for pycatj-ifying. In a nutshell, the function should behave something like that: curl -X POST https://api-endpoint.com -d '{\"data\":{\"somekey\":\"value1\"}}' # returns my_var[\"somekey\"] = \"value1\" To add some sugar to the mix I will leverage the serverless framework to do the heavy lifting in a code-first way. The plan is set, lets go see it to completion. AgendaThe decomposition of the service creation and deployment can be done as follows: Google Cloud Platform Create a GCP account (if needed) and acquire the API credentials Create a project in GCP that will host a Function and enable the needed APIs for serverless to be able to create the Function and its artifacts Function creation and testing create the code in conformance with the GCP Function handlers/events rules Manage code dependencies Function deployment leveraging serverless framework to deploy a function to GCP Add a frontend (in another blog post) that will use the serverless function. 1 Google Cloud PlatformFollowing the agenda, ensure that you have a working GCP account (trial gives you $300, and GCP Function is perpetually FREE with the sane usage thresholds). Make sure that you have a billing account created, this is set up when you opt in the free trial program, for example. Without a linked billing account the Functions won‚Äôt work. Once you have your account set, you should either continue with a default project or create a new one. In either case you need to enable the APIs that will be leveraged by serverless framework for a function deployment process. Go thru this guide carefully on how to enable the right APIs. API credentials Do not forget to download your API credentials, as nothing can be done without them. This guide‚Äôs section explains it all. The commands you will see in the rest of this post assume that the credentials are stored in ~/.gcould directory. 2 Function creationSince we are living on the edge, we will rely on the serverless framework to create \u0026 deploy our function. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:0:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.1 Serverless service templateThe way I start my serverless journey is by telling the serverless to generate a service template in the programming language of my choice. Later we can tune bits and pieces of that service, but if you start from a zero-ground, its easier to have a scaffolding to work on. # Create service with `google-python` template in the folder ~/projects/pycatj-web docker run --rm \\ -v ~/projects/pycatj-web:/opt/app \\ amaysim/serverless:1.45.1 \\ serverless create --template google-python --path pycatj-serverless The result of the serverless create --template \u003ctemplate\u003e command will be a directory with a boilerplate code for our function and a few serverless artifacts. # artifacts created by `serverless create --template` $ tree pycatj-serverless/ pycatj-serverless/ ‚îú‚îÄ‚îÄ main.py ‚îú‚îÄ‚îÄ package.json ‚îî‚îÄ‚îÄ serverless.yml We need to take a closer look at the generated serverless.yml template file where we need to make some adjustments: the project name should match the project name you have created in the GCP the path to your GCP credentials json file should be valid Given that the project in my GCP is called pycatj and my credentials file is ~/.gcloud/pycatj-d6af60eda976.json the provider section of the serverless.yml file would look like this: # serverless.yml file# with project name and credentials file specifiedprovider:name:googlestage:devruntime:python37region:us-central1project:pycatjcredentials:~/.gcloud/pycatj-d6af60eda976.json As to the main.py generated by the framework, then its a simple boilerplate code with a text reply to an incoming HTTP request wrapped in a Flask object. # main.py def http(request): \"\"\"Responds to any HTTP request. Args: request (flask.Request): HTTP request object. Returns: The response text or any set of values that can be turned into a Response object using `make_response \u003chttp://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response\u003e`. \"\"\" return f'Hello World!' Lets test that our modifications work out so far by trying to deploy the template service. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:1:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.2 Testing function deploymentBefore we start pushing our function and its artifacts to the GCP, we need to tell serverless how to talk to the cloud provider. To do that, we need to install the serverless-google-cloudfunctions plugin that is referenced in the serverless.yml file. Install the google cloud functions plugin with the npm install command using the directory with a generated serverless service files: docker run --rm \\ -v ~/.gcloud/:/root/.gcloud \\ -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\ amaysim/serverless:1.45.1 npm install Note, here I mount my GCP credentials that are stored at ~/.gcloud dir to a containers /root/.gcloud dir where serverless container will find them as they are referenced in the serverless.yml file. And secondly I bind mount my project‚Äôs directory ~/projects/pycatj-web/pycatj-serverless to the /opt/app dir inside the container that is a WORKDIR of that container. Now we have a green flag to try out the deployment process with serverless deploy: docker run --rm \\ -v ~/.gcloud/:/root/.gcloud \\ -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\ amaysim/serverless:1.45.1 serverless deploy If the deployment fails with the Error Not Found make sure that you don‚Äôt have stale failed deployments by going to Cloud Console -\u003e Deployment Manager and deleting all deployments created by Serverless Upon a successful deployment you will have a Cloud Function deployed and reachable by the service URL: Deployed functions first https://us-central1-pycatj.cloudfunctions.net/http curl-ing that API endpoint will return a simple ‚ÄúHello world‚Äù as coded in our boilerplate main.py function: # main.py def http(request): return f'Hello World!' curl -s https://us-central1-pycatj.cloudfunctions.net/http Hello World! You can also verify the resources that were created by this deployment by visiting the Deployment Manager in the GCP console as well as navigating to the functions page and examine the deployed function and its properties: ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:2:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.3 Writing a FunctionThat was a template function that we just deployed with the HTTP event acting as a trigger. Lets see how the the actual Python function is coupled to a service definition inside the serverless file. How about we give our function a different name by first changing the functions section of the serverless.yml file: # changing the function name and handler to `pycatjify`functions:pycatjify:handler:pycatjifyevents:- http:path Since we changed the function and the handler name to pycatjify we should do the same to our function inside the main.py file: def pycatjify(request): return f\"We are going to give pycatj its own place on the web!\" Deploying this function will give us a new API endpoint aligned to a new function name we specified in the serverless.yml: Deployed functions pycatjify https://us-central1-pycatj.cloudfunctions.net/pycatjify # testing $ curl https://us-central1-pycatj.cloudfunctions.net/pycatjify We are going to give pycatj its own place on the web! ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:3:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.3.1 Managing code dependenciesUp until now we played with a boilerplate code with a few names changed to give our function a bit of an identity. We reached the stage when its time to onboard the pycatj package and make our function benefit from it. Since the Functions are executed in the sandboxes on the cloud platforms, we must somehow tell what dependencies we want these sandbox to have when running our code. In the AWS Lambda example we packaged the 3rd party libraries along the function (aka vendoring). In GCP case the vendoring approach is also possible and is done in the same way, but it is also possible to ship a pip requirements.txt file along your main.py that will specify your function dependencies as pythonistas used to. Read more on GCP python dependency management Unfortunately, the PIP version that GCP currently uses does not support PEP 517, so it was not possible to specify -e git+https://github.com/dbarrosop/pycatj.git#egg=pycatj in a requirements file, thus I continued with a good old vendoring technique: # executed in ~/projects/pycatj-web/pycatj-serverless pip3 install -t ./vendored git+https://github.com/dbarrosop/pycatj.git This installs pycatj package and its dependencies in a vendored directory and will be considered as Function‚Äôs artifact and pushed to GCP along the main.py with the next serverless deploy command. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:3:1","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.3.2 EventsEvery function should be triggered by an event or a trigger that is supported by a cloud provider. When serverless is used the event type is specified for each function in the serverless.yml file: # pycatjify function is triggered by an event of type `http`# note that they key `path` is irrelevant to the serverlessfunctions:pycatjify:handler:pycatjifyevents:- http:path With this configuration we expect our function to execute once an HTTP request hits the function API endpoint. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:3:2","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.3.3 Writing a functionYes, a thousand words later we finally at a milestone where we write actual python code for a function. The template we generated earlier gives us a good starting point - a function body with a single Flask request argument: def pycatj(request): return f\"We are going to give pycatj its own place on the web!\" The logic of our serverless function that we are coding here is: parse the contents of an incoming HTTP request extracting the contents of a JSON file passed along with it transform the received data with pycatj package and send back the response With a few additions to access the pycatj package in a vendored directory and being able to test the function locally, the resulting main.py file looks as follows: This code has some extra additions to a simple two-step logic I mentioned before. I stuffed a default data value that will be used when the incoming request has no body, then we will use this dummy data just for demonstration purposes. To let me test the function code locally I added the if __name__ == \"__main__\": condition and lastly I wrote some print functions for a trivial logging. Speaking of which‚Ä¶ ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:3:3","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"2.4 LoggingLogging is a bless! Having a chance to look what happens with your function in a cloud platform sandbox is definitely a plus. With GCP the logging can be done in the simple and advanced modes. A simple logging logs everything that is printed by a function into stdout/stderr outputs -\u003e a simple print() function would suffice. In a more advanced mode you would leverage a GCP Logging API. The logs can be viewed with the Web UI Logging interface, as well as with the gcloud CLI tool. 3 Function deploymentWe previously already tried the deployed process with a boilerplate code just to make sure that the serverless framework works. Now that we have our pycatj package and its dependencies stored in a vendored folder and the function body is filled with the actual code, lets repeat the deployment and see what we get: docker run --rm \\ -v ~/.gcloud/:/root/.gcloud \\ -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\ amaysim/serverless:1.45.1 serverless deploy All goes well and serverless successfully updates our function to include the vendored artifacts as well as the new code in the main.py. Under the hood, the deployment process took the code of our Function and packaged it into a directory, zipped and uploaded to the deployment bucket. As demonstrated above, the serverless framework allows a user to express the deployment in a code, making the process extremely easy and fast. 4 Usage examplesTime to give our Function a roll by bombing it with HTTP requests. In this section I will show you how you can use the pycatjify service within a CLI and in a subsequent post we will write a simple Web UI using the API that our function provides. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:4:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"4.1 empty GET request curl -s https://us-central1-pycatj.cloudfunctions.net/pycatjify | jq -r .data # returns my_dict[\"data\"] = \"test_value\" my_dict[\"somenumber\"] = 123 my_dict[\"a_dict\"][\"asd\"] = \"123\" my_dict[\"a_dict\"][\"qwe\"][0] = 1 my_dict[\"a_dict\"][\"qwe\"][1] = 2 my_dict[\"a_dict\"][\"qwe\"][2] = 3 my_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31 my_dict[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\" With an empty GET request the function delivers a demo of its capabilities by taking a hardcoded demo JSON and making a transformation. The returned string is returned in a JSON object accessible by the data key. ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:5:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"4.2 POST with a root and pycatj_data specifiedGetting a demo response back is useless, to make use of a pycatjify service a user can specify the root value and pass the original JSON data in a POST request body using the pycatj_data key: curl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\ -H \"Content-Type: application/json\" \\ -d '{\"root\":\"POST\",\"pycatj_data\":{\"somekey\":\"somevalue\",\"a_dict\":{\"qwe\":[1,2],\"nested_dict\":{\"das\":31}}}}' \\ | jq -r .data # returns POST[\"somekey\"] = \"somevalue\" POST[\"a_dict\"][\"qwe\"][0] = 1 POST[\"a_dict\"][\"qwe\"][1] = 2 POST[\"a_dict\"][\"nested_dict\"][\"das\"] = 31 ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:6:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"4.3 POST without root, with pycatj_dataIt is also allowed to omit the root key, in that case a default root value will be applied: curl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\ -H \"Content-Type: application/json\" \\ -d '{\"pycatj_data\":{\"somekey\":\"somevalue\",\"a_dict\":{\"qwe\":[1,2],\"nested_dict\":{\"das\":31}}}}' \\ | jq -r .data # returns my_dict[\"somekey\"] = \"somevalue\" my_dict[\"a_dict\"][\"qwe\"][0] = 1 my_dict[\"a_dict\"][\"qwe\"][1] = 2 my_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31 ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:7:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"4.4 POST with json file as a bodyMy personal favorite is dumping a JSON file in a request. In that case a lengthy curl is not needed and you can specify a path to a file with a @ char. This example leverages the logic embedded in a function that treats the whole body of an incoming request as a data for pycatj transformation. $ cat test/test1.json { \"somekey\": \"localfile\", \"a_dict\": { \"asd\": \"123\", \"qwe\": [ 1, 2 ], \"nested_dict\": { \"das\": 31, \"qwe\": \"asd\" } } } curl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\ -H \"Content-Type: application/json\" \\ -d \"@./test/test1.json\" \\ | jq -r .data # returns my_dict[\"somekey\"] = \"localfile\" my_dict[\"a_dict\"][\"asd\"] = \"123\" my_dict[\"a_dict\"][\"qwe\"][0] = 1 my_dict[\"a_dict\"][\"qwe\"][1] = 2 my_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31 my_dict[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\" What‚Äôs next?Having pycatj functionality available withing a HTTP call reach makes it possible to create a simple one-page web frontend that will receive the users input and render the result of the pycatj-web service we deployed in this post. I will make another post covering the learning curve I needed to climb on to create a modern Material UI frontend that leverages the serverless function. [Project‚Äôs source code] ","date":"2019-06-30","objectID":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/:8:0","tags":["GCP","GCP Function","Python","Serverless"],"title":"Creating Google Cloud Platform Function with Python and Serverless","uri":"/2019/creating-google-cloud-platform-function-with-python-and-serverless/"},{"categories":null,"content":"I love the tech conferences that share the recordings of the sessions without hiding behind the registration or a pay wall. Luckily the trend to share the knowledge with a community openly is growing, yet you still can find a nice talk hidden behind the above mentioned walls. Sometimes an easy registration is all it takes, but then, how do you watch it offline? For example, I do love to catch up with with the recent trends and experiences while being on a plane, meaning that I just cant afford to be hooked up to the Internet. If a talk is published on the YouTube, you are good to go and download it with any web service that pops up in the google search by the term ‚ÄúYoutube download‚Äù. But what do we do when the video is hosted somewhere in the CDN and is served as a dynamic playlist of *.ts files? Here I share with you an easy way to download the videos from an m3u/m3u8 playlist. The dynamic playlist format - M3U/M3U8 - is a way to tell the browser how to download the pieces of the video that will comprise the whole recording. So the download process is actually as easy as: Get the m3u8 link Download every file from that playlist and glue them into a single video. ","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:0:0","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"Getting the playlist URLNow the first part is easy, you go to the page where a vide player is rendered and search for the m3u8 file using the developers tools console of your browser. Make sure to get the master playlist request url and copy it in your clipboard. ","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:1:0","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"Video qualityIn the master playlist body you can see the different versions of the playlists, typically they differ with the quality settings. Consider the following m3u8 file contents: #EXTM3U #EXT-X-VERSION:4 #EXT-X-STREAM-INF:PROGRAM-ID=0,BANDWIDTH=180400,CODECS=\"mp4a.40.2,avc1.4d001e\",RESOLUTION=720x294,AUDIO=\"audio-0\",CLOSED-CAPTIONS=NONE https://manifest.prod.boltdns.net/... #EXT-X-STREAM-INF:PROGRAM-ID=0,BANDWIDTH=335500,CODECS=\"mp4a.40.2,avc1.4d001f\",RESOLUTION=1200x490,AUDIO=\"audio-1\",CLOSED-CAPTIONS=NONE https://manifest.prod.boltdns.net/... The first cropped link is for the playlist with 720x294 resolution, whereas the second one is a HQ version with ‚Äú1200x490‚Äù stream. If you see that for some reason you are downloading the low quality stream, extract the HQ stream URL and use it instead of the master playlist URL. ","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:2:0","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"Downloading the files","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:3:0","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"with VLCThe title of this post says ‚Äú‚Ä¶ with no special software‚Äù, yet we will use the VLC player here which I deliberately categorize as a software that everyone can get on every platform, so its not a special software. What you need to do next is to choose File -\u003e Open Network dialog and paste the URL of the m3u8 playlist from the prev. step. Now you can either play it in the VLC right away, or check the Stream Output checkbox and click Settings. This will open a new dialog where you can choose: the path to a resulting video file the video container format and, optionally, the audio/video codecs if you want to do transcoding Click Ok and the files will start to download and encode in your resulting video container by the path you specified. This is not a particularly fast process, so just wait till the progress bar reaches its end and enjoy the video! ","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:3:1","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"with youtube-dlThe VLC-way is good for a one-time quick download, but if you have a list of playlists you want to download, then youtube-dl python tool is just unmatched. Judging by the name, the tool was developed for youtube downloads originally but outgrew it quickly enough to be a swiss knife for online video downloads. You can install it as a python package or as a pre-compiled binary, so the installation is really a breeze and won‚Äôt take long. Additionally, the tool brings an endless amount of features: automatically detect playlist URL by crawling the HTML page (no need to manually look for m3u8 URL) cli interface to scriptify bulk downloads extensive encoding support via ffmpeg and aconv filtering and sorting for videos in the playlists (if the playlist has more than one vide, i.e. Youtube playlist) and many more. For example, to download a video that you would normally watch at http://example.com/vid/test a single CLI command is all it takes: # name the output file test.\u003ccontainer_format\u003e youtube-dl -o 'test.%(ext)s' --merge-output-format mkv http://example.com/vid/test and the rest is handled by the marvelous youtube-dl: [download] Downloading playlist: Search query [Search] Downloading search JSON page 1 [Search] Downloading search JSON page 2 [Search] Downloading search JSON page 3 [Search] playlist Search query: Downloading 75 videos [download] Downloading video 1 of 75 [author:new] 6047188571001: Downloading webpage [author:new] 6047188571001: Downloading JSON metadata [author:new] 6047188571001: Downloading m3u8 information [author:new] 6047188571001: Downloading m3u8 information [author:new] 6047188571001: Downloading MPD manifest [author:new] 6047188571001: Downloading MPD manifest [hlsnative] Downloading m3u8 manifest [hlsnative] Total fragments: 245 [download] Destination: test.fhls-430-1.mp4 [download] 69.0% of ~81.31MiB at 577.84KiB/s ETA 01:57 Sometimes, though, you can‚Äôt just specify the URL of a page where the player is normally loaded in your browser, due to the cookies presented in your browser and who knows what black magic this frontenders invented while we were not watching. Then you still need to manually fetch the m3u8 link and feed it to the youtube-dl. The rest stays the same, the tool will handle the download/encoding process in the most effective and pleasant way. Note, you also might need to download the ffmpeg for youtube-dl to merge the different streams in a single container. Anyway, youtube-dl will tell you if its the case for you. ","date":"2019-06-20","objectID":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/:3:2","tags":["m3u8","video","youtube-dl"],"title":"How to download an M3U/M3U8 playlist (stream) without a special software","uri":"/2019/how-to-download-an-m3u/m3u8-playlist-stream-without-a-special-software/"},{"categories":null,"content":"While working on the Ipanema Wan Opt VNF integration with Nuage Networks I stumbled upon an interesting case which required to max out the network with FTP traffic. The tricky point there was to create the FTP connection which won‚Äôt be limited by the disk IO performance. Especially, considering that the disks were kind of slow in the setup I had. It turns out, you can use the in-memory devices in the FTP communication path /dev/zero -\u003e /dev/null, ruling out the slowliness that could have been added by the disks. Lets figure out how to do that! Software-wise my setup consisted of a single FTP server vsftpd and the FTP client ftp all running on Centos7-based VMs. These VMs were equipped with a network namespace ns-data which host the datapath interface eth1. ","date":"2018-11-23","objectID":"/2018/saturating-the-network-with-ftp/:0:0","tags":["vsftpd","FTP"],"title":"Saturating the network with FTP","uri":"/2018/saturating-the-network-with-ftp/"},{"categories":null,"content":"/dev/zero -\u003e /dev/nullI found this ‚Äúftp to dev null to test bandwidth‚Äù blog post explaining how to use /dev/zero as a source file and /dev/null as a destination within a running FTP session. The example there (executed on the FTP client side) demonstrates the following technique: #!/bin/bash /usr/bin/ftp -n \u003cIP address of machine\u003e \u003c\u003cEND verbose on user \u003cusername\u003e \u003cpassword\u003e bin put \"|dd if=/dev/zero bs=32k\" /dev/null bye END So this example did not work for me right out of the box so let me augment it with the few findings I came across while trying to make this one work. ","date":"2018-11-23","objectID":"/2018/saturating-the-network-with-ftp/:1:0","tags":["vsftpd","FTP"],"title":"Saturating the network with FTP","uri":"/2018/saturating-the-network-with-ftp/"},{"categories":null,"content":"vsftpd configurationThe put \"|dd if=/dev/zero bs=32k\" /dev/null command is the transfer operation from the client to the server. On the server side the data that comes from the client is saved in /dev/null device. First thing to check there is that your FTP server configuration allows a client to use the dev/null device as the destination. I used the vsftpd as a server, so the config that worked for me (using the local user authentication) is as follows: # cat /etc/vsftpd/vsftpd.conf anonymous_enable=NO local_enable=YES write_enable=YES anon_upload_enable=YES anon_mkdir_write_enable=YES anon_other_write_enable=YES anon_world_readable_only=YES connect_from_port_20=YES hide_ids=YES pasv_min_port=40000 pasv_max_port=60000 local_umask=022 dirmessage_enable=YES xferlog_enable=YES xferlog_std_format=YES listen=YES xferlog_enable=YES ls_recurse_enable=NO ascii_download_enable=NO async_abor_enable=YES one_process_model=NO idle_session_timeout=120 data_connection_timeout=300 accept_timeout=60 connect_timeout=60 pam_service_name=vsftpd tcp_wrappers=YES This enables me to authenticate using the local user credentials on the server and write the data to the /dev/null device. ","date":"2018-11-23","objectID":"/2018/saturating-the-network-with-ftp/:2:0","tags":["vsftpd","FTP"],"title":"Saturating the network with FTP","uri":"/2018/saturating-the-network-with-ftp/"},{"categories":null,"content":"500 OOPS: ftruncateOnce I found the workable vsftpd config, I run the script and received 500 OOPS: ftruncate error from the server. This problem, as it seems, only affects RHEL-based distros, and as explained here the workaround is to use append command instead of a put. This brings me to the final version of the script I used: # cat ./ftp.sh #!/bin/bash ip netns exec ns-data /usr/bin/ftp -n 192.168.99.101 \u003c\u003cEND verbose on user myuser mypassword bin append \"|dd if=/dev/zero bs=32k\" /dev/null bye END And the result I got on the 10Mbps uplinks: $ sudo bash ftp.sh Verbose mode on. 331 Please specify the password. 230 Login successful. 200 Switching to Binary mode. local: |dd if=/dev/zero bs=32k remote: /dev/null 227 Entering Passive Mode (192,168,99,101,222,205). 150 Ok to send data. ^C277+0 records in 276+0 records out 9043968 bytes (9.0 MB) copied, 7.06165 s, 1.3 MB/s send aborted waiting for remote to finish abort 226 Transfer complete. 8986624 bytes sent in 7.03 secs (1278.00 Kbytes/sec) 221 Goodbye. And this saturates my uplinks completely. ","date":"2018-11-23","objectID":"/2018/saturating-the-network-with-ftp/:3:0","tags":["vsftpd","FTP"],"title":"Saturating the network with FTP","uri":"/2018/saturating-the-network-with-ftp/"},{"categories":null,"content":"Shortly after I passed AWS CSA exam I went on a hunt for the next certification to claim. Decided to tackle the Openstack COA certification first saving the Docker/Kubernetes certification for a later occasion. There is a common joke floating around: ‚ÄúOh, is Openstack still a thing?\" - yes, its pretty much still a thing, especially in the Telecom area where VMs are the only viable option for the most of the cases (think VNFs). Openstack also powers our teams public SDN lab that allows to provision a fully functional Nuage environment in a matter of minutes. So I wanted to get a better operational knowledge of Openstack to be able to support and tune the platform if necessary. Disclaimer: I took the recently updated COA exam which is based on the Openstack Pike release, although I did not face any Pike-specific questions during the exam. This does not mean that the exam content will stay the same throughout the course evolution, so watch out for the updates. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:0:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Key takeawaysBefore going into detailed explanation of the preparation steps and the questions I found most challenging to prepare for, check out this list of COA key takeaways: Although it is a practice, scenario-based exam, its an easy one. You can prepare for it without being exposed to a production-grade Openstack operations. Focus on the questions that can‚Äôt be done via Horizon, as they would eat most of the exam time. Use the Horizon all the time (unless the task assumes CLI approach); if you are not a CLI jockey I ensure you, you will loose the precious time. Make use of the embedded notebook to mark the questions left unfinished; you can get back to them later if time permits. Verify the connection requirements, as the exams virtual-room environment is sensible to a browser version and connection quality. Think of a backup internet connection. Clean the desk completely! ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:1:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"PreparationThe exam is a relatively easy one; compared to AWS CSA it has far less material to learn and being a scenario-based exam it doesn‚Äôt expect you to memorize things as you can wander in the Horizon dashboard working out a correct solution for the task. I actually liked the scenario-based approach more, it tests your hands-on skills rather than the ability to hold a lot of theory in your head (which wears off really fast anyway). So how did I prepare for it? Being a LinuxAcademy user I first tried their COA courses, and they were completely useless. The course author spent 80% of the time in the CLI basically reading out load the CLI commands. Add on top the poor explanation of the basics and you get a perfect example of a bad course. So I do not recommend it to anyone. When LinuxAcademy failed me on that front I went looking for a preparation book, remembering that AWS CSA book was rather awesome. And found the one that I read cover-to-cover and can recommend to anyone looking for a single source of preparation: Preparing for the Certified OpenStack Administrator Exam by Matt Dorn. Here are the links where you can read/buy it: Amazon Safaribooks Packt In contrast to the online course I referred above, this book gives a really good theoretical background on the Openstack services that one would need to configure during the exam, supplementing it with the step-by-step configuration explanation. And yes, it also comes with a VirtualBox-based Openstack installation with pre-configured scenarios for each chapter. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:2:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Horizon all the wayAlthough every exam study guide will most likely present you with the GUI and CLI ways of solving a particular task my recommendation is to solve everything in the Horizon, leaving CLI-specific tasks to the CLI. The reason is simple, a regular user will configure things much faster within Horizon dashboard, rather than copy-pasting UUIDs in an unfriendly CLI emulator. There are tasks that can only be done within the CLI and these are the only ones that I recommend to solve with it. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:2:1","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"CLI-only tasksThere are tasks that has no way around but using the CLI to crack them. Tasks that require to interact with the objects like: domains endpoints, Downloading glance images Managing swift ACL rules and expiration dates will require you to open the CLI, but these are the only tasks that will require it. The rest can be done much faster within the Horizon dashboard. On the other hand I really encourage you to focus on these tasks, especially on the Swift‚Äôs ACLs and object expiration tasks. Swift is the only service that will require you to use the swift CLI client instead of a common openstack CLI client. And to make your life harder there is no built-in help for swift commands to manage ACL and expiration, so you need to memorize the exact commands. I also strongly suggest to pay additional attention to the tasks that test your ability to work with and analyze the Openstack Logs. You might make a mistake skipping it over, or paying a little attention to it. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:2:2","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Troubleshooting tasksAcross the 40 scenarios that you would see during your exam there would be a troubleshooting section. As explained in the book I shared above you (most likely) will not see anything harder than the communication problem for a set of VMs. And the golden there rule is to check the Security Group rules to see if the rule is there and the protocol that must be working is set in the policy. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:2:3","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Taking the examNow this is very important, if you dont want to be derailed by the proctor, make sure of the following: ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:3:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Desk stays cleanClean the desk completely. I mean completely, nothing is allowed to be on the desk. The proctor gave me hard time asking me to clean the room and the desk. For the moment I thought that the proctor is actually my wife. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:3:1","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Backup your internet linkIf possible, have a backup internet connection. During the exam you would need to share your desktop, if you have \u003e1 monitors you would need to share them all. For some mysterious reason, my perfect internet connection had been flagged as slow by the LinuxFoundation proctor and they asked me to provide another one, since the desktop stream was lagging. Now this could caught you off-guard, I ended up setting up a mobile hot spot, so think about the backup. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:3:2","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Intermittent connectivity is OKConnection drops happened to me more than once; dont be scared, this is OK. You will be able to get back where you stopped, although the time could be only adjusted by the proctor. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:3:3","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Using the integrated notebookThe exam environment allows you to use the integrated notebook. I used it to mark the questions that I left/skipped so I could come back to them later. Yes, you got it right, the exam environment does not mark which questions were unanswered, so you need to write down the question numbers for those which you unfinished. ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:3:4","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"PSGo and pursue the COA certification, its not the easiest one out there, but certainly its not one of the toughest. Depending on your exposure to the Openstack basics you can prepare only for the rare things like swift expiration configuration and maybe some other CLI-only tasks and sit the exam. Since it sports one free re-take, you can safely get a taste of it and probably pass it from the first attempt. Good luck! ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:4:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"Useful links Preparing for the Certified OpenStack Administrator Exam Preparing to COA Git repo ","date":"2018-11-11","objectID":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/:5:0","tags":["Openstack","Certification"],"title":"Prepping up for and passing the Certified Openstack Administrator Exam","uri":"/2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/"},{"categories":null,"content":"On May 11th I passed the AWS Certified Solution Architect Associate exam which was harder then I expected. In this memo I will outline how I prepared to this exam, what topics you better pay more attention to and some tips and hints I could give to anyone going for AWS CSA exam. Disclaimer: I took the original AWS CSA exam, not the one that was launched in Feb 2018; this older version is only available to schedule till August 2018. After that date the newer version of this exam will be the only one available. Watch out, it has a new set of objectives. ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:0:0","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"PreparationIts worth to mention that prior the preparation to this exam I had a little hands on experience with AWS and almost no theoretical knowledge. So I approached the preparation course as an AWS noob. Q: Do I need to pay for AWS services if I want to pass this exam? A: no, you can pass it without having a real hands on, for instance by reading the AWS Study Guide alone. Though, the Free Tier offering by AWS will make the costs for AWS practice really close to $0. Without laying your hands on basic configuration stuff it could be challenging to pass the exam. ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:1:0","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Main courseSince I have an active Linux Academy subscription I took theirs certification prep course which I found rather complete and well composed. Its the 20hrs length course with interactive quizzes and the real Hands On Labs. Labs are a great way to gain practice by having the access to AWS Console provided by the Linux Academy. Each section will test you on the outlined concepts. Most of them have real Labs Quizzes really help you to check that the material you‚Äôve just listened to was actually absorbed by your memory. I went through all the videos on 1.5x speed and cleared all quizzes rather easily. At the very end of the course you can simulate the real exam by answering 60 questions in 80 minutes time frame. While the questions I had during the exam were harder, this simulation can help you to feel the real exam atmosphere. While Hands On Labs are super useful to get the practical knowledge about the AWS Console and the configuration of the various services, I did not complete them all for the sake of time. But in general, I would highly suggest to clear Hands On Labs, most of the training providers offer them. Linux Academy also had an assessment service called Cloud Assessments, now when LA merged with A Cloud Guru this is not available anymore. I completed the offered set of practice tasks and loved the gamification part they embedded. Solving the real practical tasks within the AWS Console can help you if you had no previous AWS management activities. Cloud Assessments challenge you to solve different practice tasks for each major AWS service Main course alternativesOf course, Linux Academy is not the only service who made these AWS CSA prep courses, the most popular one I saw on the Internet was the A Cloud Guru course. Their students praise this course rather highly, so you can take that one as well. Check out the pricing and the course offerings to pick up the right provider for you. ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:1:1","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Additional resourcesI hate to break it, but my experience showed that the course alone won‚Äôt make you pass the exam easily. And not because the courses are not good enough, they are good, but they do not cover all the aspects or do not explain all the details of some service, which you might encounter during the exam. WhitepapersAs a recommended supplement many courses suggest to go over the AWS Whitepapers to get more knowledgeable on the various AWS concepts. I‚Äôve peered into the two of the papers and left the rest untouched. While the whitepapers are good and useful, they are ~70 pages long and reading them takes quite some time. But if time allows, you better read them, since they cover a lot of the concepts you will be tested against. I hadn‚Äôt that much time, so I went reading the Study Guide instead. AWS CSA Official Study GuideThe Study Guide offers the right amount of information to prepare you for the exam. I personally did not read the book, since I had a pretty good understanding of AWS services by finishing the LinuxAcademy course but if you don‚Äôt want to pay for video course and Labs access, the Guide will do just fine. I highly recommend to get your hands on this guide since it has a brilliant set of the Exam Essentials chapters and the straight-to-the-point quizzes after each chapter. All I did with this book was that I cleared all the quizzes after each chapter and read few ‚ÄúExam essentials‚Äù chapters the day before the exam. And again, go over the quizzes, they are very good and if you‚Äôll cover them all, I would say you‚Äôd pass the exam with 80%+ score. The quizzes in this Study Guide are better/harder than the ones I solved in the LinuxAcademy course, at the same time they are a good addition to the quizzes in the LinuxAcademy course. I must say that the quizzes had the most positive impact for me to clear the exam, since they helped me to discover my weak spots and focus on the topics where I made a lot of mistakes from the first attempt. AWS FAQsNow few posts on the Internet suggested to go over the FAQ section for each AWS service that is tested in AWS CSA exam. Thats a very good suggestion, since the FAQ section actually quite resembles the questions you might encounter during the exam. I breezed over two or three FAQs for VPC, RDS and SQS to see whats there; due to the time constraints I left other FAQs unread. Catch up with a communityIts a wise move to explore how others mastered the exam and pick theirs preparation practices that might work for you as well. I found this two articles quite good and comprehensive, with lots of useful links and suggestions: My AWS Solution Architect Associate exam experience by Viktorious AWS CSA discussion board on A Cloud Guru ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:1:2","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Analyze the weak spotsAs I said, quizzed helped me to discover my weak spots. So I watched over some videos again and read few more topics, then I went over the quizzes again to make sure that I understood the problem well enough. ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:1:3","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Taking the exam","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:2:0","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"What topics should I pay attention to most?As I mentioned in the beginning, the actual questions I encountered during the exam were not easy-peasy. While, of course, I had a lot of basic questions that tested my ability to identify the key application area for the various AWS services, there were questions that required practical experience or detailed knowledge on the topic. My point here, is that you will a fair share of questions that you could easily prepare to by reading the ‚ÄúExam Essentials‚Äù topics of the above mentioned Study Guide, but will it be enough to safely pass the exam? Hard to tell. Exam blueprint states that you will be tested for the following topics: Topic Level Scoring: (with my marks) 1.0‚ÄàDesigning highly available, cost-efficient, fault-tolerant, scalable systems:‚Äà79% 2.0‚ÄàImplementation/Deployment:‚Äà100% 3.0‚ÄàData Security:‚Äà77% 4.0‚ÄàTroubleshooting:‚Äà80% No doubts, the most popular services will be tested the most: EC2 S3 RDS DynamoDB AutoScaling VPC At the same time I was caught off guard by hitting quite a few questions dedicated to detailed knowledge of the following services: API Gateway ECS Lambda CloudFormation My advice would be to pay a bit more attention to these topics, since they seem to appear frequently in the current versions of the CSA exam. And when I say ‚Äúdetailed knowledge‚Äù, this means that questions were testing some configuration steps or your ability to identify the details about some service. Below I mention a few AWS concepts across various services that I did not master and was tested against, you better make sure to get familiar with these topics, since they are not normally stressed enough in the various prep courses. IAM I had a false sense of simplicity when read IAM chapters. It seemed like everything is obvious with IAM. But then I got a question about the AWS STS service and I was unprepared. Check this one out, don‚Äôt be like me. So questions like temporary access are possible, and you need to know something about it. If you are not coming from Systems Administration background you might shiver when you see AD/LDAP abbreviations in the question (I do). With this said, check that you know how AD policies can be integrated with AWS IAM. Its very important to understand the huge importance of IAM Roles, I got the impression that I‚Äôve been tested on this more than once. A tricky question was how can you let users from one AWS account access resources in another AWS account? EC2 Auto Scaling Elasticity and High Availability are the corner stones of a modern cloud-based application, therefore you will face lots of Auto Scaling questions. Most of them are basic, but the interesting one happened to appear to me. How does AS terminate the instance? In what order? Does it pick up a VM to terminate randomly or does it take into account active connections to the instance or maybe number of instances in the AZ? This is all about Default Termination Policy and my bad I did not pay attention to that specific angle of the EC2 Auto Scaling. S3 Normally S3 should come easy, aspects like Life-cycle policies and default data replication across AZs in a selected region must be emphasized by every course material. But ensure that you know what is S3 Cross-Region replication and why it might be needed. Don‚Äôt fool yourself that S3 Encryption questions will avoid you. Be ready to answer few questions on that. EBS Do note, even though EBS in general might appear to you as an easy topic, AWS CSA tests you hard on EBS data encryption and protection. Spend few more hours on learning encryption options for EBS and the snapshots handling. Databases Yes, fair share of DB related questions, mostly about RDS and DynamoDB. Be prepared to answer the multi AZ deployment questions. And do keep in mind that AWS RDS does not let you to access the underlying Operating System. VPC Networks is my background, therefore VPC never got me much trouble. But for an Average Joe it would be nice to ensure that topics like NAT G","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:2:1","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Passing score for AWS CSA examAWS does not disclose what is the passing score, I hit 81% and passed, but seems like the threshold varies quite significantly, check this topic for various reports of a pass/fail marks. Some passed with 60%, others failed with 70%‚Ä¶ ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:2:2","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Watch out, time could be pressingApparently, 80 minutes for 55 questions could be a problem. I had 20 minutes left when I finished the last question, but I saw lots of comments, when others failed to finish in time. Some questions have lengthy explanation, so you loose time by reading it once/twice, then you could loose some more time on ruling out the right options. So my suggestion would be to skip the questions you can‚Äôt answer in 1.5-2 mins interval. You can come back to skipped questions when you will deal with the rest of them. Good luck with your exam! ","date":"2018-05-11","objectID":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/:2:3","tags":["AWS","Certification"],"title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","uri":"/2018/prepping-up-for-and-passing-the-aws-certified-solution-architect-associate/"},{"categories":null,"content":"Have you ever tried to upload thousands of small/medium files to the AWS S3? If you had, you might also noticed ridiculously slow upload speeds when the upload was triggered through the AWS Management Console. Recently I tried to upload 4k html files and was immediately discouraged by the progress reported by the AWS Console upload manager. It was something close to the 0.5% per 10s. Clearly, the choke point was the network (as usual, brothers!). Comer here, Google, we need to find a better way to handle this kind of an upload. To set a context, take a look at the file size distribution I had (thanks to this awk magic): Size, KB Num of files 256 2 512 2 1024 8 2048 1699 4096 1680 8192 579 16384 323 32768 138 65536 34 131072 6 262144 1 1048576 1 2097152 1 4194304 1 My thought was that maybe there is a way to upload a tar.gz archive and unpack it in an S3 bucket, unfortunately this is not supported by the S3. The remaining options were (as per this SO thread): You could mount the S3 bucket as a local filesystem using s3fs and FUSE (see article and github site). This still requires the files to be downloaded and uploaded, but it hides these operations away behind a filesystem interface. If your main concern is to avoid downloading data out of AWS to your local machine, then of course you could download the data onto a remote EC2 instance and do the work there, with or without s3fs. This keeps the data within Amazon data centers. You may be able to perform remote operations on the files, without downloading them onto your local machine, using AWS Lambda. Hands down, these three methods could give you the best speeds, since you could upload tar archive and do the heavy lifting on the AWS side. But none of them were quite appealing to me considering the one-time upload I needed to handle. I hoped to find kind of a parallel way of the multiple uploads with a CLI approach. So what I found boiled down to the following CLI-based workflows: aws s3 rsync command aws cp command with xargs to act on multiple files aws cp command with parallel to act on multiple files TL;DR: First option won the competition (# of cores matters), but lets have a look at the numbers. I created 100 files 4096B each and an empty test bucket to do the tests: # create 100 files size of 4096 bytes each seq -w 1 100 | xargs -n1 -I% sh -c 'dd if=/dev/urandom of=file.% bs=1 count=4096' $ find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -n 4096 100 ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:0","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"1. AWS Management ConsoleAs a normal human being I selected all these 100 files in the file dialog of the AWS Management Console and waited for 5 minutes to upload 100 of them. Horrible. The rest of the tests were run on an old 2012 MacBook Air with 4vCPUs. ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:1","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"2. aws s3 syncA aws s3 sync command is cool when you only want to upload the missing files or make the remote part in sync with a local one. In case when a bucket is empty a sequential upload will happen, but will it be fast enough? time aws s3 sync . s3://test-ntdvps real 0m10.124s user 0m1.470s sys 0m0.273s 10 seconds! Not bad at all! ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:2","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"3. aws s3 cp with xargs ls -1 | time xargs -I % aws s3 cp % s3://test-ntdvps 294.05 real 68.76 user 9.27 sys 5 mins! As bad as the AWS Management Console way! ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:3","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"4. aws s3 cp with parallelparallel is a GNU tool to run parallel shell commands. # parallel with 60 workers ls -1 | time parallel -j60 -I % aws s3 cp % s3://test-ntdvps --profile rdodin-cnpi 39.32 real 108.41 user 14.46 sys ~40 seconds, better than xargs and worse than aws s3 sync. With an increasing number of the files aws s3 sync starts to win more, and the reason is probably because aws s3 sync uses one tcp connection, while aws s3 cp opens a new connection for an each file transfer operation. ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:4","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"5. What if I had some more CPU cores?You can increase the number of the workers, and if you have a solid amount of threads available you might win the upload competition: # 48 Xeon vCPUs, same 100 files 4KB each aws s3 sync: 6.5 seconds aws s3 cp with parallel and 128 jobs: 4.5 seconds # now 1000 files 4KB each aws s3 sync: 40 seconds aws s3 cp with parallel and 252 jobs: 21.5 seconds So you see that the aws s3 cp with parallel might come handy if you have enough of vCPUs to handle that many parallel workers. But if you are sending your files from a regular notebook/PC the aws s3 sync command will usually be of a better choice. ","date":"2018-03-25","objectID":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/:0:5","tags":["AWS"],"title":"Uploading multiple files to AWS S3 in parallel","uri":"/2018/uploading-multiple-files-to-aws-s3-in-parallel/"},{"categories":null,"content":"Haters gonna hate YAML, thats for sure. I am on the other hand in love with YAML; when one have to manually write/append config files I find YAML easier than JSON (and you have comments too). Ansible, various static-site-generators and quite a lot of opensource tools use YAML syntax for the configuration purposes. But still, YAML syntax highlighting is not a part of the Common languages shipped with highlight.js compiled package. Hugo also uses the hljs to colorize code snippets, but it uses the default pack of languages that lacks YAML support. Look at this greyish snippet, looks ugly. --- - name: Prepare linux virt host gather_facts: no hosts: localhost tasks: - name: Include packages/services to install/set include_vars: main.yml Luckily, we can add custom languages using Cloudflare CDN collection of pre-built packages. To do so, add this config portion to your Hugo' config.toml: # double check, that you have # syntaxHighlighter = \"highlight.js\" in your config.toml # note, [[params.customJS]] is nested under [params] section [[params.customJS]] src = \"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js\" integrity = \"sha256-tvm0lHsuUZcOfj/0C9xJTU4OQx5KpUgxUcAXLX5kvwA=\" crossorigin = \"anonymous\" async = true defer = true And now our YAML looks a bit better: ---- name:Prepare linux virt hostgather_facts:nohosts:localhosttasks:- name:Include packages/services to install/setinclude_vars:main.yml I changed the style-*-.min.css property to highlight string portions in green, instead of dark blue. A proper way would be to use a custom HLjs theme, but building it in Tranquilpeak theme is kinda tedious, so I picked up a shortcut changing the compiled css instead: # change the color code for this class # in my case I changed to #a5c261 .codeblock .string,figure.highlight .string{color:#a5c261} Thanks to Tranquilpeack for Hugo theme maintainer, who shared with me this option for custom highlighting PS. Since the method to load custom JS described in this article has a bug when it comes to Chrome browser, I changed the way Hugo loads custom JS as suggested in the referenced issue. ","date":"2017-11-24","objectID":"/2017/how-to-add-yaml-highlight-in-highlight.js/:0:0","tags":["highlightjs","hugo"],"title":"How to add YAML highlight in Highlight.js?","uri":"/2017/how-to-add-yaml-highlight-in-highlight.js/"},{"categories":null,"content":"Back in the days when I mostly did routing stuff I spent the whole day configuring SROS devices via SSH. And once in a while I saw that SSH session or its server part (or even underlying connection) glitched, resulting in a corrupted lines feeded to the device. What was also quite common is to make a mistake (i.e. syntax one) in a single line and watch like the rest of config got applied to the wrong context. These sad facts pushed me to create a rootifier CLI script, that was converting tree-like SROS config into flattented (aka rooted) fashion. Now I decided to make a web service of that script, that is publicly available at http://rootifier.netdevops.me/ ","date":"2017-11-22","objectID":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/:0:0","tags":["SROS","Nokia"],"title":"SROS Rootifier or how to flatten 7750 SR config","uri":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/"},{"categories":null,"content":"SROS config structureAs you well aware, SROS config is of indent-based tree-like structure: configure -------------------------------------------------- echo \"System Configuration\" -------------------------------------------------- system name \"ntdvps\" location \"netdevops.me\" chassis-mode d It is readable for a human, but it is much safer to apply batch config using the flattened structure, where each command is given in a full context fashion. Passed through a rootifier our example will transform as displayed: /configure system name \"ntdvps\" /configure system location \"netdevops.me\" /configure system chassis-mode d Now each command has a full path applied and even making an error in a single command will not affect the rest of them, making configuration safer. Yeah, probably applying rootifier to a short config snippets make a little sense, but pushing a solid 300+ lines config to a fresh box would definitely benefit from rootifying. Take a look at this diff made for a real-life config of SROS box before and after rootifying. Not only it downsized from 1600 lines to 600, it also became safer to push via console/SSH connection. ","date":"2017-11-22","objectID":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/:1:0","tags":["SROS","Nokia"],"title":"SROS Rootifier or how to flatten 7750 SR config","uri":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/"},{"categories":null,"content":"Usage scenarios and limitationsAs I explain in the Usage and Limitations section rootifier accepts either the whole config file content or any part of it, that starts under configure section For instance, valid config portions are: 1. Full config As you see it via admin display-config or in the config file you can copy it it as a whole, or from the beginning to the desired portion # TiMOS-B-14.0.R4 both/x86_64 Nokia 7750 SR Copyright (c) 2000-2016 Nokia. # All rights reserved. All use subject to applicable license agreements. # Built on Thu Jul 28 17:26:11 PDT 2016 by builder in /rel14.0/b1/R4/panos/main # Generated WED NOV 22 12:22:35 2017 UTC exit all configure #-------------------------------------------------- echo \"System Configuration\" #-------------------------------------------------- system name \"pe.pod62.cats\" chassis-mode d dns exit snmp exit time ntp server 10.167.55.2 no shutdown exit sntp shutdown exit dst-zone CEST start last sunday march 02:00 end last sunday october 03:00 exit zone UTC exit 2. Portion of the config that starts with 4 spaces exactly system name \"pe.pod62.cats\" chassis-mode d dns exit snmp exit time ntp server 10.167.55.2 no shutdown exit 3. Any part of the config with specified context Since rootifier does not know the config structure and makes decision only by indentations in the passed config, it can not say what context was this snippet from: #-------------------------------------------------- echo \"Policy Configuration\" #-------------------------------------------------- policy-options begin prefix-list \"loopback\" prefix 1.1.1.1/32 exact exit policy-statement \"export_loopback\" entry 10 from prefix-list \"loopback\" exit action accept exit exit exit commit exit Thus, rootifier will not render the rooted version of this snippet correctly. Now we, of course, know that policies are configured under the /configure router context, so we can help rootifier by setting the context: # put a missing context before your snippet router policy-options begin prefix-list \"loopback\" prefix 1.1.1.1/32 exact exit policy-statement \"export_loopback\" entry 10 from prefix-list \"loopback\" exit action accept exit exit exit commit exit You can even extract deeply nested config portions and rootify them, just specify the missing context: # missing context router policy-options # original snippet prefix-list \"Customer_1\" prefix 10.10.55.0/24 exact prefix 10.10.66.0/24 exact exit prefix-list \"Customer_2\" prefix 172.10.55.0/24 exact prefix 172.10.66.0/24 exact exit community \"East\" members \"65510:200\" community \"West\" members \"65510:100\" community \"Customer_2\" members \"65510:2\" ","date":"2017-11-22","objectID":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/:2:0","tags":["SROS","Nokia"],"title":"SROS Rootifier or how to flatten 7750 SR config","uri":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/"},{"categories":null,"content":"PSRootifier web service is a Flask application deployed in a container in ElasticBeanstalk on AWS. Probably I will write about this way of deploying the code in a later post. Rootifier source code is hosted on Github. A similar work (CLI version) was done by honorable David Roy - transcode-sros. ","date":"2017-11-22","objectID":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/:3:0","tags":["SROS","Nokia"],"title":"SROS Rootifier or how to flatten 7750 SR config","uri":"/2017/sros-rootifier-or-how-to-flatten-7750-sr-config/"},{"categories":null,"content":"Flask documentation is very clear on where is the place for its built-in WSGI application server: While lightweight and easy to use, Flask‚Äôs built-in server is not suitable for production as it doesn‚Äôt scale well and by default serves only one request at a time. So how about I share with you a Dockerfile that will enable your Flask application to run properly and ready for production-like deployments? As a bonus, I will share my findings discovered along the way of building this container image. But before we dive in and start throwing words like uwsgi, nginx and sockets lets set up our vocabulary. As DigitalOcean originally wrote: WSGI: A Python spec that defines a standard interface for communication between an application or framework and an application/web server. This was created in order to simplify and standardize communication between these components for consistency and interchangeability. This basically defines an API interface that can be used over other protocols. uWSGI: An application server container that aims to provide a full stack for developing and deploying web applications and services. The main component is an application server that can handle apps of different languages. It communicates with the application using the methods defined by the WSGI spec, and with other web servers over a variety of other protocols. This is the piece that translates requests from a conventional web server into a format that the application can process. uwsgi: A fast, binary protocol implemented by the uWSGI server to communicate with a more full-featured web server. This is a wire protocol, not a transport protocol. It is the preferred way to speak to web servers that are proxying requests to uWSGI. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:0:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Why do we even need nginx and uWSGI in front of Flask?That is the question everyone should ask. Main reason is performance, of course. The Flasks built-in web server is a development server by Werkzeug which was not designed to be particularly efficient, stable, or secure. And by all means Werkzeug was not optimized to serve static content, that is why production deployments of Flask apps rely on the following stack: Front-end web-server (nginx or Apache): load balancing, SSL termination, rate limiting, HTTP parsing and serving static content. WSGI application server (uWSGI, Gunicorn, CherryPy): runs WSGI compliant web applications and does it in a production-grade manner. Handling concurrent requests, process management, cluster membership, logging, configuration, shared memory, etc. Obviously, development server which comes with Flask simply does not bother about all these tasks that production deployments face. That is why it is so strongly advised against using Flask' server in any kind of production. Speaking about the performance I suggest to check out this presentation from Pycon IE ‚Äò13 called Maximum Throughput (baseline costs of web frameworks) that explains how number of queries per second depends on web stack you choose. While there are many alternatives to nginx+uWSGI pair, I will focus on these two in this post. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:1:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Do I need a production grade Flask app for a pet project?While you may go with built-in Flask server for the little projects of your own, this container is so simple that you would not need to use the Built-in server anymore. Why opting out for testing server, if it is easy to launch it in a production-ready way? ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:2:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Configuring nginxWe start with configuration of nginx server that will face incoming traffic and handle it for us. We also keep in mind that our nginx server will run in an Alpine Linux docker container. nginx config consists of two parts: global nginx config file (nginx.conf) site-specific config file (flask-site-nginx.conf) ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:3:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"nginx global configFor the global nginx config file I combined the recommendations outlined in the post How to Configure NGINX for a Flask Web Application with nginx configuration samples from uWSGI docs. A little caveat that you might encounter when deploying nginx in Alpine Linux renders itself like that: Error: nginx: [emerg] open() \"/run/nginx/nginx.pid\" failed (2: No such file or directory) All you need to do is to to change pid file location since /run/ path is not available in Alpine Linux. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:3:1","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"nginx site configSite config (flask-site-nginx.conf) is short and simple: server { location / { try_files $uri @yourapplication; } location @yourapplication { include uwsgi_params; uwsgi_pass unix:///tmp/uwsgi.sock; } # Configure NGINX to deliver static content from the specified folder location /static { alias /app/static; } } Basically, all you saying here is that your application will be served at / endpoint and use uwsgi wire protocol via unix socket at unix:///tmp/uwsgi.sock. Also we ask nginx to serve static content that is stored in /app/static. Communication path between nginx and WSGI app server can be configured with different sockets and protocols, but unix_socket + uwsgi protocol tends to be the most appropriate way. The uwsgi protocol is derived from SCGI but with binary string length representations and a 4-byte header that includes the size of the var block (16 bit length) and a couple of general-purpose bytes. Binary management is much easier and cheaper than string parsing. So far we dealt with the first bastion, which is nginx config. Our configuration path can be depicted as that: ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:3:2","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"uWSGI configurationuWSGI documentation is extensive, you may find all the tweaks and recommendations for the wide range of deployment scenarios. Since this container we build is of general purpose, a sensible uWSGI configuration file (uwsgi.ini) could look as follows: [uwsgi] module = main callable = app plugins = /usr/lib/uwsgi/python uid = nginx gid = nginx socket = /tmp/uwsgi.sock chown-socket = nginx:nginx chmod-socket = 664 cheaper = 1 processes = %(%k + 1) This configuration file consists of uWSGI options each of which is documented quite extensively. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:4:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Module and CallableWe start with defining where is an entry point for uWSGI server to call our app. The module directive corresponds to the name of the python module holding your app. In my case the demo Flask app I built is contained in the main.py file, hence the main module name. On the other hand, callable is the name of an object inside your module, which is a Flask application entry point. # coding: utf-8 from flask import Flask app = Flask(__name__) # rest output is omitted For me, its the app variable that should be populated to the callable parameter. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:4:1","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"PluginsuWSGI is modular and language-agnostic. In Apline Linux deployments it comes with core features built in, but python support is not one of them. uWSGI can include features in the core or as loadable plugins. uWSGI packages supplied with OS distributions tend to be modular. In such setups, be sure to load the plugins you require with the plugins option. That is why plugins parameter is needed where we specify where to find the python plugin. I installed uwsgi-python via apt package manager, this step will be covered as we move to Dockerfile explanation section. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:4:2","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"uid, gidCommon sense: do not run uWSGI instances as root. You can start your uWSGIs as root, but be sure to drop privileges with the uid and gid options. I dropped privileges to nginx user level. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:4:3","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Socket configurationAs you remember, we agreed that uwsgi protocol over unix socket will be used as a communication suite between nginx and uWSGI. We already told so to nginx, now its time for uWSGI. Same /tmp/uwsgi.sock is referenced in this uwsgi.ini file. Moreover, we change permissions to that socket file to be readable for nginx user. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:4:4","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Processes configurationuWSGI can spawn multiple processes to run your Flask app, being very productive. But, you need to thoroughly calculate how many processes and threads works for your particular situation. There is no magic rule for setting the number of processes or threads to use. It is very much application and system dependent. Simple math like processes = 2 * cpucores will not be enough. You need to experiment with various setups and be prepared to constantly monitor your apps. uwsgitop could be a great tool to find the best values. In our config file these two lines will do the trick: cheaper = 1 processes = %(%k + 1) With cheaper = 1 we activate the The uWSGI cheaper subsystem which allows to dynamically scale the number of running workers (processes). So under the minimum load uWSGI will spawn just one workers. The upper limit is dictated by processes = %(%k + 1) statement. The %k is a magic variable, which will be resolved by uWSGI to the number of available cores. So for a single core system, number of max workers will be 1 + 1 = 2. We finished another configuration block: ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:5:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Supervisord to rule them allA cherry on a pie is to use the supervisord service to manage nginx and uWSGI. For that we create supervisord.conf with a plain and simple config. Supervisord will watch for these process and restart/start them automatically if things go south for one of them. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:6:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Lets load it in a container?Our final part will be creating a lightweight Alpine Linux Docker container image that will have all these parts inside ready to consume. Refer to this comments-rich Dockerfile where we glue together all the things we discussed above in a docker image. One thing to mention here is that python2 and python3 uWSGI plugins are separate packages in Alpine packages system. ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:7:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Enjoying the resultI built two container images for python2 and python3 respectively along with a sample python application. Lets taste them out: # pull the image (tagged py2 or py3 respectively) [ec2~]$ sudo docker pull hellt/nginx-uwsgi-flask-alpine-docker:py3 py3: Pulling from hellt/nginx-uwsgi-flask-alpine-docker b56ae66c2937: Already exists # omitted Status: Downloaded newer image for hellt/nginx-uwsgi-flask-alpine-docker:py3 The image is very lightweight (62 MB): REPOSITORY TAG IMAGE ID CREATED SIZE hellt/nginx-uwsgi-flask-alpine-docker py3 7fb6af3baf0e 6 minutes ago 62.5 MB Since docker image contains a sample application we can run it to test that everything works as expected: sudo docker run -p 38080:80 hellt/nginx-uwsgi-flask-alpine-docker:py3 Voila ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:8:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"How do I use this one?First of all, there is no need to use the image from the docker hub, it was created for demonstration purposes. To create the same container but for your application, consider the following steps: Clone the repo with the Dockerfile and configuration files Tune the config files if necessary: Tune uwsgi.ini config: i.e. cheaper number and processes to match your hardware Enhance nginx config Copy your app to the /app subdirectory and you are good to build your image ","date":"2017-11-09","objectID":"/2017/flask-application-in-a-production-ready-container/:9:0","tags":["Flask","nginx","uWSGI","Docker"],"title":"Flask application in a production-ready container","uri":"/2017/flask-application-in-a-production-ready-container/"},{"categories":null,"content":"Today I faced a task which required first to establish an SSH tunnel in a background process and later use this tunnel for SSH connection. What seemed like a child‚Äôs play first actually had some fun inside. A problem were hidden right between the moment you spawned ssh process in the background and the next moment you tried to use this tunnel. In other words, it takes literally no time to spawn a process in the background, but without checking that tunnel is ready, you will quite likely receive an error, since your next instructions will be executed immediately after. Consequently, I needed a way to ensure that the SSH service is ready before I try to consume it. But how do you check if there is a server behind some host:port and that this server is of SSH nature? In Ansible we could leverage wait_for module that can poke a socket and see if OpenSSH banner is there. But in my case Python \u0026 Paramiko was all I had. It turned out that with Paramiko it is possible to achieve the goal with most straightforward and probably least elegant code: I found it sufficient to setup a timer-driven while loop where Paramiko tries to open a connection without credentials. In order to detect if socket is opened I catch different type of exceptions that Paramiko emits: if there is nothing listening on a particular socket, then Paramiko emits paramiko.ssh_exception.NoValidConnectionsError if the socket is open, but the responding service is not SSH, then Paramiko emits paramiko.ssh_exception.SSHException with a particular message Error reading SSH protocol banner if the socket is open and SSH service responding on the remote part - we are good to go! This time still paramiko.ssh_exception.SSHException is emitted, but the error message would be No authentication methods provided. And that quite does the trick: ","date":"2017-10-29","objectID":"/2017/waiting-for-ssh-service-to-be-ready-with-paramiko/:0:0","tags":["paramiko","python"],"title":"Waiting for SSH service to be ready with Paramiko","uri":"/2017/waiting-for-ssh-service-to-be-ready-with-paramiko/"},{"categories":null,"content":"At work I always prefer KVM hosts for reasons such as flexible, free and GUI-less. Yet I never bothered to go deeper into the networking features of Libvirt, so I only connect VMs to the host networks via Linux Bridges or OvS. Far far away from fancy virtual libvirt networks. Even with this simple networking approach I recently faced a tedious task of reconnecting VMs to different bridges on-the-fly. My use case came from a need to connect a single traffic generator VM to the different access ports of virtual CPEs. Essentially this meant that I need to reconnect my traffic generator interfaces to different bridges back and forth: Apparently there is no such virsh command that will allow you to change bridge attachments for networking devices, so a bit of bash-ing came just handy. You know network interface device definition grepped from Libvirt XML format holds bridge association: \u003c!-- OMITTED --\u003e \u003cdevices\u003e \u003c!-- OMITTED --\u003e \u003cinterface type='bridge'\u003e \u003cmac address='52:54:00üíø75:4f'/\u003e \u003csource bridge='br12'/\u003e \u003cmodel type='virtio'/\u003e \u003caddress type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/\u003e \u003c/interface\u003e \u003c!-- OMITTED --\u003e The brute force approach would be to change the bridge identified in the domain definition, restart the domain and be with it. Not sporty at all! Instead I decided that it would be good to have a script that for starters will take domain_name, interface_name and new_bridge_id and do the rest. So vifmove.sh (virsh interface move) was born. This is a tiny bash script which does the job for me just fine: Underneath its all simple, I leveraged virsh update-device command and just templated the interface definition XML file: If you find this one useful, feel free to add your ideas in the gist comments. ","date":"2017-10-26","objectID":"/2017/changing-libvirt-bridge-attachment-in-a-running-domain-aka-on-the-fly/:0:0","tags":["virsh"],"title":"Changing Libvirt bridge attachment in a running domain aka on-the-fly","uri":"/2017/changing-libvirt-bridge-attachment-in-a-running-domain-aka-on-the-fly/"},{"categories":null,"content":"xrdp is defacto the default RDP server for Linux systems sharing with VNC the remote access solution olympus. I personally found it more resource friendly and feature rich compared to VNC solutions I tried. The only problem I found with xrdp is that current Ubuntu LTS release Xenial 16.04 has a way outdated 0.6.1-2 version of xrdp in the packages repo. This version has no shared clipboard support, which makes remote support/remote access a tedious task. xrdp currently in its 0.9.3 version and it would be really nice to have a more recent package, rather than installing it from sources, like many solutions propose. Well, no need to compile xrdp from sources (unless you want to), because you can leverage a ppa from hermlnx that has xrdp 0.9.1-7 already built for amd64 and i386 systems # all you need is sudo add-apt-repository ppa:hermlnx/xrdp sudo apt-get update sudo apt-get install xrdp You can also try a deb package of xrdp 0.9.2 ‚Äì https://github.com/suminona/xrdp-ru-audio ","date":"2017-08-25","objectID":"/2017/installing-xrdp-0.9.1-on-ubuntu-16.04-xenial/:0:0","tags":["xrdp","ubuntu"],"title":"Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial","uri":"/2017/installing-xrdp-0.9.1-on-ubuntu-16.04-xenial/"},{"categories":null,"content":"While Amazon Linux AMI has yum as a package manager, it is not that all compatible with any RHEL or CentOS distributive. A lot of changes that AWS team brought into this image made it a separate distro, so no eyebrows should be given when battle-tested procedure to install python3 will fail on Amazon Linux. (Yeah, python3 does not come included yet in Amazon Linux) Fortunately it is very easy to fetch (while not the latest release) python3: # list available packages that have python3 in their name yum list | grep python3 # install python3+pip, plus optionally packages to your taste sudo yum install python35 python35-devel python35-pip python35-setuptools python35-virtualenv # update pip3. optionally set a symbolic link to pip3 sudo pip-3.5 install --upgrade pip And that is it! ","date":"2017-08-20","objectID":"/2017/how-to-install-python3-in-amazon-linux-ami/:0:0","tags":["AWS","python"],"title":"How to install python3 in Amazon Linux AMI","uri":"/2017/how-to-install-python3-in-amazon-linux-ami/"},{"categories":null,"content":"I wrote about the Yang Explorer in a docker quite some time ago, Yang Explorer was v0.6 at that time. Back then the motivation to create a docker image was pretty simple ‚Äì installation was a pain in v0.6, it is still a pain, but the official version bumped to 0.8(beta). So I decided to re-build an image, now using Alpine Linux as a base image to reduce the size. Just take a look how noob-ish I was to publish a Dockerfile like this: FROMubuntu:14.04MAINTAINERRoman Dodin \u003cdodin.roman@gmail.com\u003eRUN DEBIAN_FRONTEND=noninteractive apt-get update; apt-get install -y python2.7 python-pip python-virtualenv git graphviz libxml2-dev libxslt1-dev python-dev zlib1g-devRUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/CiscoDevNet/yang-explorer.gitWORKDIR/yang-explorerRUN bash setup.sh -yRUN sed -i -e 's/HOST=\\x27localhost\\x27/HOST=$HOSTNAME/g' start.shCMD [\"bash\", \"start.sh\"] Several unnecessary layers, using Ubuntu as a base ‚Äì these are the Docker-novice errors. Few things changed in the Yang Explorer regarding the setup process, now you do not need to install explicitly all the dependencies, they will be installed using the packaged requirements.txt file, so our Dockerfile could be as short as this: FROMalpineLABEL maintainer=\"dodin.roman@gmail.com, netdevops.me\"RUN apk add --no-cache bash git python \u0026\u0026 \\ python -m ensurepip \u0026\u0026 \\ rm -r /usr/lib/python*/ensurepip \u0026\u0026 \\ git clone https://github.com/CiscoDevNet/yang-explorer.gitWORKDIR/yang-explorerRUN apk add --no-cache gcc py-crypto python-dev libffi-dev musl-dev openssl-dev libxml2-dev libxslt-dev \u0026\u0026 \\ bash setup.sh -y \u0026\u0026 \\ sed -i -e 's/HOST=\\x27localhost\\x27/HOST=$HOSTNAME/g' start.sh \u0026\u0026 \\ apk del musl-dev gccCMD [\"bash\", \"start.sh\"] In the first RUN we write a layer with the tools that are needed to clone the official repo and in the second RUN we install build dependencies, go through setup process and uninstall unnecessary build dependencies to reduce the size. Compressed image size is 358Mb. Uncompressed size is 1.9Gb ","date":"2017-08-10","objectID":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/:0:0","tags":["YANG","Docker","Yang Explorer","Alpine"],"title":"Yang Explorer in a docker container based on Alpine","uri":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/"},{"categories":null,"content":"UsageTo use this image: Start the container docker run -p 8088:8088 -d hellt/yangexplorer-docker Navigate your flash-capable browser to http://\u003cip_of_your_docker_host\u003e:8088 ","date":"2017-08-10","objectID":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/:1:0","tags":["YANG","Docker","Yang Explorer","Alpine"],"title":"Yang Explorer in a docker container based on Alpine","uri":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/"},{"categories":null,"content":"Differences with Robert Csapo imageMain differences are in the size: Compressed = 358Mb vs 588Mb Uncompressed = 1.9Gb vs 2.51Gb ","date":"2017-08-10","objectID":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/:2:0","tags":["YANG","Docker","Yang Explorer","Alpine"],"title":"Yang Explorer in a docker container based on Alpine","uri":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/"},{"categories":null,"content":"Links My image on Docker Hub Robert' image on Docker hub Official Yang Explorer repo ","date":"2017-08-10","objectID":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/:3:0","tags":["YANG","Docker","Yang Explorer","Alpine"],"title":"Yang Explorer in a docker container based on Alpine","uri":"/2017/yang-explorer-in-a-docker-container-based-on-alpine/"},{"categories":null,"content":"Hugo gets a lot of attention these days, it is basically snapping at the Jekyll' heels which is still the king of the hill! I don‚Äôt know if Hugo' popularity coupled with the fastest static-site-generator statement, but for me ‚Äúspeed‚Äù is not the issue at all. A personal blog normally has few hundreds posts, not even close to thousands to be worried about slowness. Then if it is not for speed then why did I choose Hugo? Because it became a solid product with a crowded community and all the common features available. (To be honest I also got an illusion that one day I might start sharpen my Go skills through Hugo as well). As you already noticed, this blog is powered by Hugo, is running on GitLab pages, with SSL certificate from CloudFlare and costs me $0. And I would like to write down the key pieces that‚Äôll probably be of help on your path to a zero-cost personal blog/cv/landing/etc. The key ingredients of a modern zero-cost blog powered by a Static Site Generator are: Version Control System ‚Äì Git Web-based version control repository ‚Äì GitLab/Github Web server ‚Äì GitLab Pages/GitHub Pages Static Site Generator Engine ‚Äì Hugo/Jekyll/Hexo/Pelican/many-others SSL Certificate provider ‚Äì Cloudflare/LetsEncrypt (optional) Custom domain linked to a free one from GitLab/Github (optional) While Git and GitLab/GitHub are of obvious choice we better discuss the Hugo + GitLab CI + GitLab pages + Cloudflare mix that I chose to enable this blog. HugoHugo installation is ridiculously easy, thanks to Golang that powers it. Download a single hugo binary from the official repo and thats it. No need for virtualenvs, npms and alike, a single binary is all you need. Once the binary is in your $PATH create a site skeleton with hugo new site \u003cyourBlogName\u003e For details refer to the QuickStart guide to get a locally running site under 5 minutes. ","date":"2017-08-04","objectID":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/:0:0","tags":["Hugo","Gitlab","Cloudflare"],"title":"Setting up a Hugo blog with GitLab and CloudFlare","uri":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/"},{"categories":null,"content":"ThemeHugo community produced over 100+ themes for different needs. As to me, most of them are ugly, or as minimalistic as the blogspot. Probably the hardest thing in the whole process is to find a theme that suits you. This blog uses a Tranquilpeak theme. To onboard a chosen theme follow the quickstart guide' step 3. GitLabNow when you have an engine and a theme coupled together its GitLab' part to present your content to the world. GitLab has the GitLab Pages service created just for what we need and highligted by being: free SSG-agnostic SSL \u0026 custom domains ready It has more whistles than Github pages and is completely free without any limitations. There is a comprehensive guide about onboarding a static generated site within GitLab, I will boil it down to a few steps (you can always peer into my repo for a complete code and settings): Create a .gitlab-ci.yml file at the root of your repo with the pages job pages: # this image is slimmer than official one; based on Alpine image: fundor333/hugo script: - hugo # send all files from public directory to the CI server artifacts: paths: - public only: - master # this job will affect only the 'master' branch Note, that you can put the exact same content in your file, there are no custom parts here. Find your baseurl and put it into config.toml of your Hugo site. A base URL depends on how did you create a GitLab project. Is it a project placed under you personal account or a under a group? All the options are outlined in the official docs. Make a commit with the contents of your blog and push the changes to the master branch git push origin master. That will automatically trigger the pages job to build your site and start serving it from https://yournamespace.gitlab.io At this point you are good to go, you have TLS certificate provided by Gitlab for *.gitlab.io namespace, your posts will be automatically generated once you push to master branch and your texts are in VSC. WIN! You can stop here and start generate the content, but if you are up to custom domain or custom TLS certificate -\u003e continue to read. ","date":"2017-08-04","objectID":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/:0:1","tags":["Hugo","Gitlab","Cloudflare"],"title":"Setting up a Hugo blog with GitLab and CloudFlare","uri":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/"},{"categories":null,"content":"Custom domainHaving your site to render by myawesome.blog URL instead of gitlab.io is solid. For that you just need a A or CNAME DNS record provisioned as explained in the docs. I chose to delegate my netdevops.me domain to Cloudflare, since they provide a TLS certificate for free. ","date":"2017-08-04","objectID":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/:1:0","tags":["Hugo","Gitlab","Cloudflare"],"title":"Setting up a Hugo blog with GitLab and CloudFlare","uri":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/"},{"categories":null,"content":"TLS (SSL) certificatesTwo common free options when we talk about TLS certs are LetsEncrypt and Cloudflare certs. I am no security expert to claim that one is better than other, I chose a path that is easier, which is Cloudflare FlexSSL in my case. Flexible SSL encrypts traffic from Cloudflare to end users of your website, but not from Cloudflare to your origin server. This is the easiest way to enable HTTPS because it doesn‚Äôt require installing an SSL certificate on your origin. While not as secure as the other options, Flexible SSL does protect your visitors from a large class of threats including public WiFi snooping and ad injection over HTTP. Implications are clear, FlexSSL is free but does not make secure connection end-to-end, which is fine with me. FlexSSL configuration is as easy as going to Crypto pane in the Cloudflare admin panel and enabling Flexible SSL: In that case nothing is needed to be configured in GitLab, just enjoy your TLS-enabled site. In case you want to enable end-to-end encryption (Strict SSL) there is a thorough guide from Gitlab covering every step. Post comments are here. ","date":"2017-08-04","objectID":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/:2:0","tags":["Hugo","Gitlab","Cloudflare"],"title":"Setting up a Hugo blog with GitLab and CloudFlare","uri":"/2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/"},{"categories":null,"content":"I started to play with Go aka Golang. Yeah, leaving the comfort zone, all that buzz. And for quite some time I‚Äôve been engaged with VS Code whenever/wherever I did dev activities. VS Code has a solid Go support via its official extension: This extension adds rich language support for the Go language to VS Code, including: Completion Lists (using gocode) Signature Help (using gogetdoc or godef+godoc) Quick Info (using gogetdoc or godef+godoc) Goto Definition (using gogetdoc or godef+godoc) Find References (using guru) File outline (using go-outline) Workspace symbol search (using go-symbols) Rename (using gorename) Build-on-save (using go build and go test) Lint-on-save (using golint or gometalinter) Format (using goreturns or goimports or gofmt) Generate unit tests skeleton (using gotests) Add Imports (using gopkgs) Add/Remove Tags on struct fields (using gomodifytags) Semantic/Syntactic error reporting as you type (using gotype-live) Mark that gotools in the brackets, these ones are powering all that extra functionality and got installed into your GOPATH once you install them via VS Code. And here you might face an issue if you want to use Go + VS Code both on Mac and Linux using the Dropbox folder (or any other syncing service). The issue is that binaries for Mac and Linux will overwrite themselves once you decide to install the extension on your second platform. Indeed, by default VS Code will fetch the source code of the tools and build them, placing binaries in the $GOPATH/bin. Lucky we, the Go Extension developers have a special setting to put extension dependencies to a different $GOPATH: Tools this extension depends on This extension uses a host of Go tools to provide the various rich features. These tools are installed in your GOPATH by default. If you wish to have the extension use a separate GOPATH for its tools, provide the desired location in the setting go.toolsGopath. Read more about this and the tools at Go tools that the Go extension depends on And thats it, open your settings.json, put something like \"go.toolsGopath\": \"~/.gotools\" and thats it, next time you hit ‚Äúinstall‚Äù of Go Extension dependencies, they will be stored outside your Dropbox-powered $GOPATH and won‚Äôt interfere with each other. ","date":"2017-07-28","objectID":"/2017/how-to-make-vs-code-go-extension-to-work-in-your-cloud-folder-on-different-platforms/:0:0","tags":["Golang","VSCode"],"title":"How to make VS Code Go extension to work in your cloud folder on different platforms?","uri":"/2017/how-to-make-vs-code-go-extension-to-work-in-your-cloud-folder-on-different-platforms/"},{"categories":null,"content":"Nothing bad in knowing how many lines of code or text out there in your repo. You don‚Äôt even need your VCS to convey this analytics. All you need is git, grep and wc. # count lines in .py and .robot files in /nuage-cats dir of the repo $ git ls-files nuage-cats/ | grep -E \".*(py|robot)\" | xargs wc -l 0 nuage-cats/robot_lib/__init__.py 817 nuage-cats/robot_lib/lib/NuageQoS.py 409 nuage-cats/robot_lib/lib/NuageVCIN.py 1841 nuage-cats/robot_lib/lib/NuageVNS.py 2964 nuage-cats/robot_lib/lib/NuageVSD.py # OMITTED 26 nuage-cats/test_suites/0910_fail_bridges_kvm_vms/0910_fail_bridges_kvm_vms.robot 13636 total ","date":"2017-07-25","objectID":"/2017/how-to-count-lines-of-code-in-a-git-repo/:0:0","tags":["git"],"title":"How to count lines of code in a git repo?","uri":"/2017/how-to-count-lines-of-code-in-a-git-repo/"},{"categories":null,"content":"Cloud-native revolution pointed out the fact that the microservice is the new building block and your best friends now are Containers, AWS, GCE, Openshift, Kubernetes, you-name-it. But suddenly micro became not that granular enough and people started talking about serverless functions! When I decided to step in the serverless property I chose AWS Lambda as my instrument of choice. As for experimental subject, I picked up one of my existing projects - a script that tracks new documentation releases for Nokia IP/SDN products (which I aggregate at nokdoc.github.io). Given that not so many posts are going deeper than onboarding a simplest function, I decided to write down the key pieces I needed to uncover to push a real code to the Lambda. Buckle up, our agenda is fascinating: testing basic Lambda onboarding process powered by Serverless framework accessing files in AWS S3 from within our Lambda with boto3 package and custom AWS IAM role packaging non-standard python modules for our Lambda exploring ways to provision shared code for Lambdas and using path variables to branch out the code in Lambda InitWhat I am going to lambdsify is an existing python3 script called nokdoc-sentinel which has the following Lambda-related properties: uses non standard python package ‚Äì requests reads/writes a file. I specifically emphasized this non-std packages and relying on persistence since these aspects are not covered in 99% of Lambda-related posts, so, filling the spot. AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second. Multiple choices are exposed to you when choosing an instrument to configure \u0026 deploy an AWS Lambda: AWS Console (web) AWS CLI Multiple frameworks (Serverless, Chalice, Pywren) While it might be good to feel the taste of a manual Lambda configuration process through the AWS Console, I decided to go ‚Äúeverything as a code‚Äù way and use the Serverless framework to define, configure and deploy my first Lambda. The Serverless Framework helps you develop and deploy your AWS Lambda functions, along with the AWS infrastructure resources they require. It‚Äôs a CLI that offers structure, automation and best practices out-of-the-box, allowing you to focus on building sophisticated, event-driven, serverless architectures, comprised of Functions and Events. Serverless installation and configurationFirst things first, install the framework and configure AWS credentials. I already had credentials configured for AWS CLI thus skipped that part, if that is not the case for you, the docs are comprehensive and should have you perfectly covered. Creating a Service templateOnce serverless is installed, start with creating an aws-python3 service: A service is like a project. It‚Äôs where you define your AWS Lambda Functions, the events that trigger them and any AWS infrastructure resources they require, all in a file called serverless.yml. $ serverless create --template aws-python3 --name nokdoc-sentinel Two files will be created: handler.py ‚Äì a module with Lambda function boilerplate code serverless.yml ‚Äì a service definition file Making lambda instance out of a templateI renamed handler.py module to sentinel.py, also changed the enclosed function' name and deleted redundant code from the template. For starters I kept the portion of a sample code just to test that deploying to AWS via serverless actually works. # sentinel.py import json def check(event, context): body = { \"message\": \"Sentinel is on watch!\", } response = { \"statusCode\": 200, \"body\": json.dumps(body) } return response Thing to remember is that you also must to make appropriate changes in the serverless.yml, once you renamed the module and the function names: functions: # name of the func in the module check: # `handler: sentinel.check` reads as # \"`check` function in the `sentinel` module handler: sentinel.check ","date":"2017-07-24","objectID":"/2017/building-aws-lambda-with-python-s3-and-serverless/:0:0","tags":["AWS","AWS Lambda","Boto3","Python","Serverless"],"title":"Building AWS Lambda with Python, S3 and serverless","uri":"/2017/building-aws-lambda-with-python-s3-and-serverless/"},{"categories":null,"content":"Deploying and Testing AWS LambdaBefore adding some actual load to the Lambda function, lets test that the deployment works. To trigger Lambda execution I added HTTP GET event with the test path in the serverless.yml file. So a call to https://some-aws-hostname.com/test should trigger our lambda function to execute. functions: hello: handler: handler.hello # add http GET trigger event events: - http: path: test method: get Read all about supported by serverless framework events in the official docs. And we are coming to the first test deployment with the following assets: $ tree -L 1 . |-- sentinel.py `-- serverless.yml Lets go and deploy: $ serverless deploy Serverless: Packaging service... Serverless: Creating Stack... Serverless: Checking Stack create progress... ..... Serverless: Stack create finished... Serverless: Uploading CloudFormation file to S3... Serverless: Uploading artifacts... Serverless: Uploading service .zip file to S3 (0.33 MB)... Serverless: Validating template... Serverless: Updating Stack... Serverless: Checking Stack update progress... .............................. Serverless: Stack update finished... Service Information service: nokdoc-sentinel stage: dev region: us-east-1 api keys: None endpoints: GET - https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/test functions: check: nokdoc-sentinel-dev-check Note the endpoint URL at the bottom of the output, using this API endpoint we can check if our Lambda is working: curl https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/test {\"message\": \"Sentinel is on watch!\"} Exploring Serverless artifactsServerless deployed the Lambda using some defaults parameters (region: us-east-1, stage: dev, IAM role); plus serverless did some serious heavy-lifting in order to deploy our code to AWS. In particular: archived the project files as a zip archive and loaded it to AWS S3 created CloudFormation template that defines all the steps needed to onboard a Lambda and setup an API gateway to respond to GET requests Key artifacts that were created by serverless in AWS can be browsed with the AWS CLI: # exploring deployed Lambda $ aws --region us-east-1 lambda list-functions { \"Functions\": [ { \"FunctionName\": \"nokdoc-sentinel-dev-check\", \"FunctionArn\": \"arn:aws:lambda:us-east-1:446595173912:function:nokdoc-sentinel-dev-check\", \"Runtime\": \"python3.6\", \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-us-east-1-lambdaRole\", \"Handler\": \"sentinel.check\", \"CodeSize\": 1395199, \"Description\": \"\", \"Timeout\": 6, \"MemorySize\": 1024, \"LastModified\": \"2017-07-17T19:06:59.405+0000\", \"CodeSha256\": \"QrFOl8eBL8HipGRCkN/P7wsxkn8/LDIMCAQLxAVmFfI=\", \"Version\": \"$LATEST\", \"TracingConfig\": { \"Mode\": \"PassThrough\" } } ] } # exploring S3 artifacts $ aws s3 ls | grep sentinel 2017-07-17 22:05:13 nokdoc-sentinel-dev-serverlessdeploymentbucket-moviajl407hw $ aws s3 ls nokdoc-sentinel-dev-serverlessdeploymentbucket-moviajl407hw/serverless/nokdoc-sentinel/dev/1500318307598-2017-07-17T19:05:07.598Z/ 2017-07-17 22:05:40 3578 compiled-cloudformation-template.json 2017-07-17 22:05:41 395199 nokdoc-sentinel.zip # exploring CloudFormation stack $ aws --region us-east-1 CloudFormation list-stacks { \"StackSummaries\": [ { \"StackId\": \"arn:aws:cloudformation:us-east-1:446595173912:stack/nokdoc-sentinel-dev/da010710-6b22-11e7-aa95-500c20fef6d1\", \"StackName\": \"nokdoc-sentinel-dev\", \"TemplateDescription\": \"The AWS CloudFormation template for this Serverless application\", \"CreationTime\": \"2017-07-17T19:05:08.875Z\", \"LastUpdatedTime\": \"2017-07-17T19:05:45.283Z\", \"StackStatus\": \"UPDATE_COMPLETE\" } ] } Are you interested what is in this archive nokdoc-sentinel.zip? $ ls -la ~/Downloads/nokdoc-sentinel/ total 16 drwx------@ 6 romandodin staff 204 Jul 18 09:51 . drwx------+ 54 romandodin staff 1836 Jul 18 09:51 .. drwxr-xr-x@ 3 romandodin staff 102 Jul 18 09:51 .vscode -rw-r--r--@ 1 romandodin staff 208 Jan 1 1980 sentinel.py -rw-r--r--@ 1 romandodin staff 3720 Jan 1 1980 watcher.py There are two files w","date":"2017-07-24","objectID":"/2017/building-aws-lambda-with-python-s3-and-serverless/:1:0","tags":["AWS","AWS Lambda","Boto3","Python","Serverless"],"title":"Building AWS Lambda with Python, S3 and serverless","uri":"/2017/building-aws-lambda-with-python-s3-and-serverless/"},{"categories":null,"content":"Sorting out permissionsWhat you have to sort out before digging into S3 interaction is the permissions that your Lambda has. When serverless deployed our Lambda with a lot of defaults it also handed out a default IAM role to our Lambda: aws --region us-east-1 lambda list-functions | grep Role # role name is nokdoc-sentinel-dev-us-east-1-lambdaRole \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-us-east-1-lambdaRole\", To be able to interact with AWS S3 object model, this Role should have access to S3. Lets investigate: aws iam get-role-policy --role-name nokdoc-sentinel-dev-us-east-1-lambdaRole --policy-name dev-nokdoc-sentinel-lambda { \"RoleName\": \"nokdoc-sentinel-dev-us-east-1-lambdaRole\", \"PolicyName\": \"dev-nokdoc-sentinel-lambda\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"logs:CreateLogStream\" ], \"Resource\": [ \"arn:aws:logs:us-east-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*\" ], \"Effect\": \"Allow\" }, { \"Action\": [ \"logs:PutLogEvents\" ], \"Resource\": [ \"arn:aws:logs:us-east-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*:*\" ], \"Effect\": \"Allow\" } ] } } As you see S3 access is not a part of default permissions, so we must grant it to our Lambda. Instead of adding additional permissions to the existing role manually, we can re-deploy the Lambda with updated serverless.yml file. In this edition I specified availability zone, set existing S3 bucket as a deployment target and included IAM role configuration allowing full-access to S3 objects: # serverless.yml provider: name: aws runtime: python3.6 stage: dev region: eu-central-1 # deploy Lambda function files to the bucket `rdodin` deploymentBucket: rdodin # IAM Role configuration to allow all-access for S3 objects of bucket `rdodin` iamRoleStatements: - Effect: \"Allow\" Action: \"s3:*\" Resource: \"arn:aws:s3:::rdodin/*\" Now the re-deployment will create another Lambda (hence the availability zone has changed), deploy the code in the existing bucket rdodin and apply a policy that allows S3 interaction. # checking the inline policy of the IAM Role bound to Lambda aws lambda get-function --function-name nokdoc-sentinel-dev-check | grep Role \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-eu-central-1-lambdaRole\", aws iam list-role-policies --role-name nokdoc-sentinel-dev-eu-central-1-lambdaRole { \"PolicyNames\": [ \"dev-nokdoc-sentinel-lambda\" ] } aws iam get-role-policy --role-name nokdoc-sentinel-dev-eu-central-1-lambdaRole --policy-name dev-nokdoc-sentinel-lambda { \"RoleName\": \"nokdoc-sentinel-dev-eu-central-1-lambdaRole\", \"PolicyName\": \"dev-nokdoc-sentinel-lambda\", \"PolicyDocument\": { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Action\": [ \"logs:CreateLogStream\" ], \"Resource\": [ \"arn:aws:logs:eu-central-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*\" ], \"Effect\": \"Allow\" }, { \"Action\": [ \"logs:PutLogEvents\" ], \"Resource\": [ \"arn:aws:logs:eu-central-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*:*\" ], \"Effect\": \"Allow\" }, { \"Action\": \"s3:*\", \"Resource\": \"arn:aws:s3:::rdodin/*\", \"Effect\": \"Allow\" } ] } } Now as the S3 permissions are there, we are free to list bucket contents and modify the files in it. ","date":"2017-07-24","objectID":"/2017/building-aws-lambda-with-python-s3-and-serverless/:2:0","tags":["AWS","AWS Lambda","Boto3","Python","Serverless"],"title":"Building AWS Lambda with Python, S3 and serverless","uri":"/2017/building-aws-lambda-with-python-s3-and-serverless/"},{"categories":null,"content":"Using Boto3 to read/write files in AWS S3AWS provides us with the boto3 package as a Python API for AWS services. Moreover, this package comes pre-installed on the system that is used to run the Lambdas, so you do not need to provide a package. I put a file (releases_current.json) that my script expects to read to the directory created by the serverless deployment script: $ aws s3 ls rdodin/serverless/nokdoc-sentinel/ PRE dev/ 2017-07-22 15:57:07 3424 releases_current.json Lets see if we can access it from within the Lambda using boto3 and its documentation: # sentinel.py import json import boto3 def check(event, context): s3 = boto3.resource('s3') bucket = s3.Bucket('rdodin') # reading a file in S3 bucket original_f = bucket.Object( 'serverless/nokdoc-sentinel/releases_current.json').get()['Body'].read()[:50] # writing to a file new_f = bucket.put_object( Key='serverless/nokdoc-sentinel/newfile.txt', Body='Hello AWS').get()['Body'].read() body = { \"message\": \"Sentinel loaded a file {} and created a new file {}\" .format(original_f, new_f), } response = { \"statusCode\": 200, \"body\": json.dumps(body) } return response Re-deploy and check: $ curl https://xxxxx.execute-api.eu-central-1.amazonaws.com/dev/test {\"message\": \"Sentinel loaded a file b'{\\\"nuage-vsp\\\": [\\\"4.0.R8\\\", \\\"4.0.R7\\\", \\\"4.0.R6.2\\\", \\\"4.' and created a new file b'Hello AWS'\"} So far, so good. We are now capable of reading/writing to a file stored in AWS S3. Adding python packages to LambdaWe were lucky to use only the packages that either standard (json) or comes preinstalled in Lambda-system (boto3). But what if we need to use packages other from that, maybe your own packages or from PyPI? Well, in that case you need to push these packages along with your function' code as a singe deployment package. As official guide says you need to copy packages to the root directory of your function and zip everything as a single archive. What comes as a drawback of this recommendation is that your project dir will be dirty with all these packages sitting in the root you will have to .gitignore these packages directory to keep your packages out of a repository I like the solution proposed in the ‚ÄúBuilding Python 3 Apps On The Serverless Framework‚Äù post. Install your packages in a some directory in your projects dir and modify your PYTHONPATH to include this directory. # install `requests` package in a `vendored` dir at the projects root pip install -t vendored/ requests # `requests` and its dependencies are there $ ls vendored/ certifi chardet idna requests urllib3 certifi-2017.4.17.dist-info chardet-3.0.4.dist-info idna-2.5.dist-info requests-2.18.1.dist-info urllib3-1.21.1.dist-info Now modify your code to include vendored directory in your PYTHONPATH import boto3 here = os.path.dirname(os.path.realpath(__file__)) sys.path.append(os.path.join(here, \"vendored\")) # now it is allowed to add a non-std package import requests def check(event, context): # output omitted Note, that if a package has a native binary code, it must be compiled for the system that is used to run Lambdas. Shared code for LambdasEven though a Lambda often assumed as an independent function, a real application you might want to transfer to Lambda quite likely will have dependencies on some common code. Refer to the ‚ÄúWriting Shared Code‚Äù section of the above mentioned blog post to see how its done. Handling arguments in LambdasAnother common practice in a classic util function is to have some arguments (argparse) that allow to branch out the code and make an app' logic feature-rich. In Lambdas, of course, you have no CLI exposed, so to make a substitution for the arguments you can go two ways: create several functions for your project and bind different API endpoints to each of them use a single function and add a variable part to the API endpoint I will show how to handle the latter option. First, create a variable parameter for your API endpoint in the serverless.yml: events: - http: # `{command}` is a va","date":"2017-07-24","objectID":"/2017/building-aws-lambda-with-python-s3-and-serverless/:3:0","tags":["AWS","AWS Lambda","Boto3","Python","Serverless"],"title":"Building AWS Lambda with Python, S3 and serverless","uri":"/2017/building-aws-lambda-with-python-s3-and-serverless/"},{"categories":null,"content":"virsh is a goto console utility for managing Qemu/KVM virtual machines. But when it comes to deletion of the VMs you better keep calm - there is no single command to destroy the VM, its definition XML file and disk image. Probably not a big problem if you have a long-living VMs, but if you in a testing environment it is naturally to spawn and kill VMs quite often. Lets see how xargs can help us with that routine. Right now my KVM hypervisor is busy with virtualizing small Nuage VNS environment where 4010_DEMO* VMs are virtual SDN gateways: [root@srv601 ~]# virsh list Id Name State ---------------------------------------------------- 1 1-vsc1.pod60.cats running 2 1-vsc2.pod60.cats running 3 g1pe.play.cats running 4 jenkins running 5 dns running 6 1-es1.pod60.cats running 7 1-vsd1.pod60.cats running 8 1-util1.pod60.cats running 18 4010_DEMO_I2 running 19 4010_DEMO_I1 running 22 4010_DEMO2_NSGI1 running 23 4010_DEMO2_NSGI2 running 24 4010_DEMO_1upl_testNSGI1 running 25 4010_DEMO_1upl_testNSGI2 running And I really need to get these last six boys gone for good. With virsh one need to perform the following set of commands to achieve the goal: # removing the last VM with a virsh domain name 4010_DEMO_1upl_testNSGI2 # first destroy the domain (you even cant pass multiple names) $ virsh destroy 4010_DEMO_1upl_testNSGI2 Domain 4010_DEMO_1upl_testNSGI2 destroyed # now undefine the domain $ virsh undefine 4010_DEMO_1upl_testNSGI2 Domain 4010_DEMO_1upl_testNSGI2 has been undefined # deleting the disk images # assuming that all the VM-related data resides in that dir $ rm -rf /var/lib/libvirt/images/4010_DEMO_1upl_testNSGI2 Too much typing for a simple task‚Ä¶ Lets see how xargs comes into play! grep and xargs!What we need to do is to filter out the target domain names and pass these names to the virsh destroy \u0026\u0026 virsh undefine \u0026\u0026 rm -rf commands. First things first, lets get the names of the domains. grep is the tool of choice. # grep flags: # -o -- return only the matched group (not the whole line with match highlighted) # -E -- regular expression virsh list --all | grep -o -E \"(4010_DEMO\\w*)\" # OUTPUT: 4010_DEMO_I2 4010_DEMO_I1 4010_DEMO2_NSGI1 4010_DEMO2_NSGI2 4010_DEMO_1upl_testNSGI1 Bingo, now its xargs time: xargs reads items from the standard input, delimited by blanks or newlines, and executes the command (default is /bin/echo) one or more times with any initial-arguments followed by items read from standard input. Blank lines on the standard input are ignored. virsh list --all | grep -o -E \"(4010_DEMO\\w*)\" | \\ xargs -I % sh -c 'virsh destroy % \u0026\u0026 virsh undefine % \u0026\u0026 rm -rf /var/lib/libvirt/images/%;' The xargs flag -I % here allows us to substitute each % sign in the command with the xargs input argument. This effectively destroys the virsh domain along with its definition and disk image. Post comments are here. ","date":"2017-07-18","objectID":"/2017/destroy-and-undefine-kvm-vms-in-a-single-run/:0:0","tags":["virsh","qemu/kvm/libvirt"],"title":"Destroy and Undefine KVM VMs in a single run","uri":"/2017/destroy-and-undefine-kvm-vms-in-a-single-run/"},{"categories":null,"content":"It may very well be¬†that VPLS days are numbered¬†and EVPN is to blame. Nevertheless, it would be naive to expect VPLS extinction in the near future. With all its shortcomings VPLS is still¬†very well standardized, interop-proven and has a huge footprint in MPLS networks of various scale. In this post I will cover theory and configuration parts for one particular flavor of VPLS signalling - BGP VPLS (aka¬†Kompella VPLS) defined in RFC4761. I‚Äôll start with simple single home VPLS scenario while multi-homing techniques and some advanced¬†configurations might appear in a separate post later. In this topic the following SW releases were used: Nokia (Alcatel-Lucent) VSR¬†14.0.R4 Juniper vMX 14.1R1.10 ","date":"2016-11-14","objectID":"/2016/11/bgp-vpls-explained-nokia-juniper/:0:0","tags":["bgp","vpls","nokia","juniper"],"title":"BGP VPLS deep dive. Nokia SR OS \u0026 Juniper","uri":"/2016/11/bgp-vpls-explained-nokia-juniper/"},{"categories":null,"content":"BGP VPLS BasicsVirtual Private LAN Service (VPLS) is seen like an Ethernet LAN by the customers of a Service Provider. However, in a VPLS, not all of the customers are connected to a single LAN; they may be spread across a metro or wide area network. In essence, a VPLS glues together several individual LANs across a packet switched network to appear and function as a single LAN. This is accomplished by incorporating MAC address learning, flooding, and forwarding functions in the context of pseudowires that connect these individual LANs. The entire VPLS service behaves like a big switch with distributed MAC learning intelligence implemented on each PE, and as in a switch, MAC learning¬†happens in a dataplane. The following two types of interfaces are typical for VPLS: Attachment Circuits (AC) - circuits connecting¬†Customer Edge (CE) devices to¬†Provider Edge (PE) routers.¬†PE routers often called VPLS Edge (VE) devices in VPLS terminology. Pseudowires (PW) - circuits connecting PEs between each other In the context of a given VPLS instance, a PE can have one or more local ACs, and one or more PWs toward remote PEs. Full-mesh of transport tunnels between PEs is required. In Kompella¬†VPLS, BGP is a key enabler and is responsible for: Auto-discovery: process of finding all PE routers participating in a VPLS instance; Signalling:¬†the setup and tear-down of pseudowires (PW) that constitute the VPLS service. Auto-discoveryEach PE ‚Äúdiscovers‚Äù which other PEs are part of a given VPLS by means of BGP. This allows each PE‚Äôs configuration to consist only of the identity of the VPLS instance established on this PE, not the identity of every other PE in that VPLS instance. Moreover, when the topology of a VPLS changes, only the affected PE‚Äôs configuration changes; other PEs automatically find out about the change and adapt. The Route Target community is used to¬†identify members of a VPLS.¬†A PE announces that it belongs to VPLS V by annotating its NLRIs for VPLS V with Route Target RT, and acts on this by accepting NLRIs from other PEs that have Route Target RT. A PE announces that it no longer participates in VPLS V by withdrawing all NLRIs that it had advertised with Route Target RT. SignallingOnce discovery is done, each pair of PEs in a VPLS must be able to establish (and tear down) pseudowires to each other, i.e., exchange (and withdraw) demultiplexors. This process is known as signaling. Signaling is also used to transmit certain characteristics of the pseudowires that a PE sets up for a given VPLS. BGP Update message carrying BGP VPLS NLRI (AFI:25, SAFI:65) is used to signal VPLS membership and multiplexors for a VPLS service: Let‚Äôs expand some of the fields of the BGP VPLS NLRI: Route Distinguisher - used to differentiate between customer NLRIs thus should be unique for every VPLS service. VE ID - unique identifier (aka site-id), manually assigned to every VPLS Edge device. VE Block Offset, VE Block Size and Label Base are used for calculating the service label (multiplexor). Label BlocksUsing a distinct BGP Update message to send a demultiplexor to each remote PE would require the originating PE to send N such messages for N remote PEs. In order to minimize the control plane load original standard introduced Label Blocks which drastically reduce the amount of BGP Update messages.¬†A label block is a set of demultiplexor labels used to reach a given VE ID. A single BGP VPLS NLRI signals a label block which consists of: VE ID - manually assigned to VE device identifier Label Base (LB)¬†- first label assigned to a label block VE Block Size (VBS) - number of labels assigned to a label block. Vendor-dependant value, Nokia and Juniper both use Block Size of 8. VE Block Offset (VBO) - first VE ID assigned to a label block A contiguous label block defined by \u003cLB, VBO, VBS\u003e is the set {LB+VBO, LB+VBO+1, ..., LB+VBO+VBS-1}. Thus, instead of a single large label block to cover all VE IDs in a VPLS, one can have several label blocks, each with a dif","date":"2016-11-14","objectID":"/2016/11/bgp-vpls-explained-nokia-juniper/:0:1","tags":["bgp","vpls","nokia","juniper"],"title":"BGP VPLS deep dive. Nokia SR OS \u0026 Juniper","uri":"/2016/11/bgp-vpls-explained-nokia-juniper/"},{"categories":null,"content":"VPLS data planeThis topic is focusing on VPLS¬†data¬†plane encapsulation, as defined in RFC 4448 - Encapsulation Methods for Transport of Ethernet over MPLS Networks. MAC learningVPLS is a multipoint service with a MAC learning on a data plane. This means that the entire Service Provider network should appear as a single logical learning bridge (Ethernet switch) for each VPLS that the SP network supports. The logical ports for the SP ‚Äúbridge‚Äù are the AC as well as the PW¬†on a PE.¬†As a result of MAC learning, bridges populate a MAC table in which they keep track of the interface (or PW) where each unicast MAC is reachable. Aging, Flooding, BUM trafficVPLS PEs SHOULD have an aging mechanism to remove a MAC address associated with a logical port.¬†Aging reduces the size of a VPLS MAC table to just the active MAC addresses. When a bridge receives a packet to a destination that is not in its FIB, it floods the packet on all the other ports (process known as replication). Frames that should be flooded are Broadcast, Unknown unicast and Multicast Broadcast frames have destination MAC address ff:ff:ff:ff:ff:ff. Multicast frames have a destination MAC address whose first octet has its last bit set to one To avoid loops during replication process split-horizon rule should be honored:¬†A frame received on a PW is never sent back on the same or any other PW (default, but configurable behavior). ","date":"2016-11-14","objectID":"/2016/11/bgp-vpls-explained-nokia-juniper/:0:2","tags":["bgp","vpls","nokia","juniper"],"title":"BGP VPLS deep dive. Nokia SR OS \u0026 Juniper","uri":"/2016/11/bgp-vpls-explained-nokia-juniper/"},{"categories":null,"content":"Case study: Single-homed VPLSEnough with theory, time to practice¬†some VPLS! I will start with a simple case of two CE routers (CE1 and CE2) connected to a Service Provider‚Äôs PE routers (R1, R2) configured with a VPLS service. Refer to Fig. 60 outlining lab topology for this case. It is assumed that ISIS and LDP are configured and operational. Refer to these baseline configurations: R1 (Nokia) R2 (Juniper) R3 Route reflector (Nokia) BGP configurationThis one is really simple. All we need is to configure MP-iBGP peering between PEs and RR with L2 VPN family enabled: R1 (Nokia) *A:R1\u003econfig\u003erouter# info #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- ## \u003c Output omitted for brevity \u003e autonomous-system 65000 #-------------------------------------------------- *A:R1\u003econfig\u003erouter\u003ebgp# info ---------------------------------------------- connect-retry 1 min-route-advertisement 1 rapid-withdrawal rapid-update l2-vpn group \"RR\" family l2-vpn enable-peer-tracking neighbor 3.3.3.3 type internal exit exit no shutdown ---------------------------------------------- R3 Route reflector (Nokia) *A:R3\u003econfig\u003erouter# info #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- ## \u003c Output omitted for brevity \u003e autonomous-system 65000 #-------------------------------------------------- A:R3\u003econfig\u003erouter\u003ebgp# info ---------------------------------------------- family l2-vpn connect-retry 1 min-route-advertisement 1 enable-peer-tracking rapid-withdrawal rapid-update l2-vpn group \"RRC\" cluster 3.3.3.3 neighbor 1.1.1.1 type internal exit neighbor 2.2.2.2 type internal exit exit no shutdown ---------------------------------------------- R2 (Juniper) root@R2# show routing-options | grep auto autonomous-system 65000; root@R2# show protocols bgp group RR { type internal; family l2vpn { signaling; } neighbor 3.3.3.3; } Interface configurationTo demonstrate vlan-normalization methods I used two different vlans on attachment circuits connected to R1 and R2. Our CE1 and CE2 devices have simple¬†dot1q interfaces addressed in this way: CE1 has interface toCE2 with address 192.168.1.1/24, VLAN 10 CE1 has interface toCE1 with address 192.168.1.2/24, VLAN 600 Router R1 has vlan 10 on its AC, while R2 configured with vlan-id 600 (on Juniper vlan ids values for VPLS interfaces must be \u003e 512). Nokia routers do not differ if interface is going to be used in any particular service or in no service at all, therefore the configuration steps are obvious. The part which enables particular ethernet encapsulation (802.1q in this case) is done under port configuration: R1: A:R1# configure port 1/1/2 A:R1\u003econfig\u003eport# info ---------------------------------------------- ethernet mode hybrid ## hybrid means that port can act as an access \u0026 network port encap-type dot1q exit no shutdown ---------------------------------------------- Configuration of Vlan-id 10 attachment to a VPLS service will be done later in the VPLS configuration section. Note, that Ethernet MTU on Nokia routers includes¬†Ethernet header. This means that, for instance, interface with MTU 2000 will be able to put on wire exactly 2000 bytes, for example: TOTAL 2000B == Frame 54: 2000 bytes on wire (16000 bits), 2000 bytes captured (16000 bits) on interface 0 14B == Ethernet II, Src: 50:01:00:04:00:01 (50:01:00:04:00:01), Dst: 50:01:00:05:00:01 (50:01:00:05:00:01) 4B == 802.1Q Virtual LAN, PRI: 7, CFI: 0, ID: 10 20B == Internet Protocol Version 4, Src: 192.168.1.1, Dst: 192.168.1.2 1962B == Internet Control Message Protocol In contrast with Nokia, Juniper‚Äôs configuration of interface¬†is done in a different way. root@R2# show interfaces ge-0/0/1 flexible-vlan-tagging; encapsulation flexible-ethernet-services; unit 600 { encapsulation vlan-vpls; vlan-id 600; } In Juniper you have to specify¬†encapsulation vlan-vpls¬†under the interface‚Äôs configuration for a logical u","date":"2016-11-14","objectID":"/2016/11/bgp-vpls-explained-nokia-juniper/:0:3","tags":["bgp","vpls","nokia","juniper"],"title":"BGP VPLS deep dive. Nokia SR OS \u0026 Juniper","uri":"/2016/11/bgp-vpls-explained-nokia-juniper/"},{"categories":null,"content":"References \u0026 further reading RFC4761¬†Virtual Private LAN Service (VPLS) Using BGP for Auto-Discovery and Signaling RFC4448 Encapsulation Methods for Transport of Ethernet over MPLS Networks draft-vpls-multihoming BGP based Multi-homing in Virtual Private LAN Service Configuring VPLS on SR OS VPLS Services¬†in JunOS Configuring and troubleshooting BGP VPLS on Nokia SROS VPLS services explained (Nokia official doc) MPLS in the SDN Era ","date":"2016-11-14","objectID":"/2016/11/bgp-vpls-explained-nokia-juniper/:0:4","tags":["bgp","vpls","nokia","juniper"],"title":"BGP VPLS deep dive. Nokia SR OS \u0026 Juniper","uri":"/2016/11/bgp-vpls-explained-nokia-juniper/"},{"categories":null,"content":"Recently I revived my relationship with Python in an effort to tackle the routine tasks appearing here and there. So I started to write some pocket scripts and, luckily, was not the only one on this battlefield - my colleagues also have a bunch of useful scripts. With all those code snippets sent in the emails, cloned from the repos, grabbed on the network shares‚Ä¶ I started to wonder how much easier would it be if someone had them all aggregated and presented with a Web UI for a shared access? Thus, I started to build web front-end to the python scripts we used daily with these goals in mind: allow people with a zero knowledge of Python to use the scripts by interacting with them through a simple Web UI; make script‚Äôs output more readable by leveraging CSS and HTML formatting; aggregate all the scripts in one a single repo but in a separate sandboxed directories to maintain code manageability. This short demo should give you some taste of what it is: Disclaimer: I am nowhere near even a professional python or web developer. And what makes it even worse is that I used (a lot) a very dangerous coding paradigm - SDD - Stack Overflow Driven Development. So, hurt me plenty if you see some awful mistakes. Project source code PLAZA (this is the name I gave this project) implements a straightforward user experience flow: a user opens a web page, selects a script from the menu, fills in the necessary input data and run a script to get the results back. By hitting submit data goes to the back-end part, where the chosen python script does it‚Äôs noble job and produces some data. This data gets pushed back to the browser and as displayed to a user. Obviously, one will need some front-end technologies to build the web layer and some back-end to process the incoming data. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:0:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Tools \u0026 Technologies","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:1:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Front-endTo build a fairly fresh-looking (fresh as in 2016yr), dynamic web view we need a web framework to leverage. I used Bootstrap package (CSS and JS) as it is well documented and have tons of implementations and examples. What tastes good with Bootstrap - JQuery, of course. JQuery was used to handle AJAX response/request messages between the front-end and the back-end without reloading the whole page. Since I had no previous experience with both of these technologies, I heavily used everything google served me. Here is my list of useful resources I found noteworthy: Layoutit.com - there you can create Bootstrap grid and play with elements in a drag and drop fashion. Load the result in a zip file and your grid system is almost ready. Bootply.com - visual builder for Bootstrap layout. It has some good examples which cover basic Bootstrap elements behavior (navbar, grid rules, etc). Form validator by 1000hz - well, it‚Äôs a form validator. And since every script needs to get input data from a user, form validation is a must-have for a sleek user experience. Bootsnipp.com - crowdsource collection of snippets written with Bootstrap. I grabbed my side menu from it. Another useful section from this site is Form Builder. Formden - another form builder. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:1:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Back-endThe heavy lifting in the back is done by the gorgeous Flask, which is a micro framework for writing web applications. It includes a web-server, Jinja2 templating engine and lots of features to make back-end easy even for dummies like me. As to the Flask related resources I cherry-picked the following: Famous Flask Mega Tutorial by Miguel Grinberg Discover Flask - A full season of youtube videos from Michael Herman Official documentation of course! Another good post on AJAX+Flask interaction from giantflyingsaucer.com ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:1:2","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Project structure overviewHaving frameworks set and tools figured out I started to outline the project‚Äôs high-level structure. Flask maintains a simple yet flexible project structure. In my case, I didn‚Äôt deviate far away from a basic setup, since the overall simplicity is one of the project‚Äôs objectives. ‚îú‚îÄ‚îÄ app.py # Flask application ‚îú‚îÄ‚îÄ config.py # Flask configuration ‚îú‚îÄ‚îÄ .env # env variables for dev/prod environments ‚îú‚îÄ‚îÄ scripts_bank # directory to store all python scripts we're going to use via Web ‚îú‚îÄ‚îÄ static # static data for Bootstrap CSS, JS, custom fonts, etc ‚îÇ ‚îú‚îÄ‚îÄ css ‚îÇ ‚îú‚îÄ‚îÄ fonts ‚îÇ ‚îî‚îÄ‚îÄ js ‚îú‚îÄ‚îÄ templates # HTML templates used to render pages\u003c/pre\u003e Although the comments above give enough information about the structure, let‚Äôs go into details a bit Flask application - app.py - is an entry point for the whole project. It starts the web-server, loads the routes (aka links to the pages of your web project) and plugs in python scripts stored in the scripts_bank directory. As every other app, Flask app should be configured differently for development and production. This is done via the config.py and the environment variables .env file. In the static directory you normally store your CSS, JS, pictures, custom fonts. So did I. HTML pages are in the templates directory. And the pythonic scripts with all the relevant files (unique HTML templates for input forms, additional front-end Javascript code, etc) are living inside the scripts_bank directory. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:2:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Configuring FlaskOnce you have Flask installed and got familiar with its basics (either through official quick start guide or tons of tutorials) it is time to configure it. There are several ways to configure a Flask application. The basic one is to specify the configuration statements as the arguments to your app instance: app = Flask(__name__) ## pass secret_key and SQLAlchemy params app.secret_key = 'test' app.config[SQLALCHEMY_DATABASE_URI] = 'sqlite:///db/sql.db' if __name__ == '__main__': app.run(debug=True) ## pass DEBUG param\u003c/pre\u003e A bit more advanced way is to specify all the config parameters in uppercase in your `app.py` and tell the `app` instance to get config from this file: ```DEBUG = True SECRET_KEY = 'yekterces' SQLALCHEMY_DATABASE_URI = 'sqlite:///db/sql.db' app = Flask(__name__) app.config.from_object(__name__) # get config from this module\u003c/pre\u003e But the methods discussed so far can‚Äôt let you have different configurations for Dev and Prod environments (which you‚Äôd want to have eventually). When I was choosing the configuration method for this app I followed a path which consists of these three key points: creating configuration classes for different environments using inheritance (explained here) choosing the right configuration class based on the current value of the environment variable storing environment variables in a file (.env) and parsing its contents for parameters (more here) Detailed explanation of Flask app configuration Going from bottom to top, .env is a file, which stores application parameters in a way like classic environment variables do. # This file is used to store configuration settings for # Dev and Prod environments. PLAZA_SETTINGS value is used by app.py to # properly detect which configuration class to use # uncomment/modify desired section prior to use # dev PLAZA_SETTINGS = config.Development # prod # PLAZA_SETTINGS = config.Production # HOST = 0.0.0.0\u003c/pre\u003e Then Flask application initializes and gets configuration from a class, stored in PLAZA_SETTINGS variable: from flask import Flask, render_template import os import config root_folder_path = os.path.dirname(os.path.abspath(__file__)) # get env_settings list env_settings = config.EnvironmentSettings(root_folder_path) # initialize Flask app app = Flask(__name__) # configure Flask app from a class, stored in PLAZA_SETTINGS variable app.config.from_object(env_settings['PLAZA_SETTINGS']) if __name__ == '__main__': # if we are in Prod, use HOST and PORT specified try: app.run(host=str(env_settings['HOST']), port=80) except config.ConfigurationError: app.run()\u003c/pre\u003e Functions subject to configuration along with configuration classes are stored in the config.py file: import os # default config class class Base(object): DEBUG = False SECRET_KEY = 'your_secret' class Development(Base): DEBUG = True class Production(Base): DEBUG = False class EnvironmentSettings: \"\"\" Access to environment variables via system os or .env file for different environments (Prod vs Dev) \"\"\" def __init__(self, root_folder_path): self._root_folder_path = root_folder_path def __getitem__(self, key): return self._get_env_variable(key) def __setitem__(self, key, value): raise InvalidOperationException('Environment Settings are read-only') def __delitem__(self, key): raise InvalidOperationException('Environment Settings are read-only') def _get_env_variable(self, var_name, default=False): \"\"\" Get the environment variable or return exception :param var_name: Environment Variable to lookup \"\"\" try: return os.environ[var_name] except KeyError: from io import StringIO from configparser import ConfigParser env_file = os.environ.get('PROJECT_ENV_FILE', self._root_folder_path + \"/.env\") try: config = StringIO() config.write(\"[DATA]\\n\") config.write(open(env_file).read()) config.seek(0, os.SEEK_SET) cp = ConfigParser() cp.read_file(config) value = dict(cp.items('DATA'))[var_name.lower()] if value.startswith('\"') and value.endswith('\"'): value = value[1:-1] elif value.starts","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:3:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Setting up front-endGood, Flask app has been configured and is ready to render some pages, so let‚Äôs go and prepare out front-end to display projects' web pages. Download Bootstrap, JQuery, Fontawesome and store theirs minified min.css and min.js artifacts in the static directory of the project. This is how it should look like: ‚îú‚îÄ‚îÄ static ‚îÇ ‚îú‚îÄ‚îÄ css ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ bootstrap.min.css ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ font-awesome.min.css ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ style.css # custom styles css for every page ‚îÇ ‚îú‚îÄ‚îÄ fonts ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ FontAwesome.otf ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ NokiaPureHeadline_ExtraBold.ttf # custom fonts like this also live here ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fontawesome-webfont.eot ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fontawesome-webfont.svg ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fontawesome-webfont.ttf ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fontawesome-webfont.woff ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ fontawesome-webfont.woff2 ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ glyphicons-halflings-regular.eot ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ glyphicons-halflings-regular.svg ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ glyphicons-halflings-regular.ttf ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ glyphicons-halflings-regular.woff ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ glyphicons-halflings-regular.woff2 ‚îÇ ‚îî‚îÄ‚îÄ js ‚îÇ ‚îú‚îÄ‚îÄ bootstrap.min.js ‚îÇ ‚îú‚îÄ‚îÄ jquery-2.2.0.min.js ‚îÇ ‚îú‚îÄ‚îÄ loadingoverlay.min.js # CSS for overlay animation. ‚îÇ ‚îú‚îÄ‚îÄ scripts.js # custom JS scripts ‚îÇ ‚îî‚îÄ‚îÄ validator.min.js # form validation JS code ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:4:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"LayoutBefore diving into HTML it is advised to think about pages layout. I recommend you to get familiar with Bootstrap CSS rules and choose a layout that fits your project. I decided to go with a 3+9 scheme. Three parts are for side menu and nine parts are for a content area with a navigation bar at the top of the page. I composed a sketch of the page depicting how I would like to see my projects web view for an arbitrary script: Follow the link to see my script‚Äôs page template on codepen and see how things interact. Do not worry if you can‚Äôt pick rock solid layout right now, you will be able to modify it on-the-fly and decide what suits your needs better. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:4:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Flask routes \u0026 templates","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:5:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"RoutesFlask uses routes to create URL‚Äôs for the web pages. If we need to show the main page for example for the URL abc.com we need to define the root route - / - like this: @app.route('/') def index(): return 'Index Page' This will effectively bind the index() function to the route / , so when a user navigates to the application‚Äôs root it will trigger the index() function. @app.route('/') def index(): return render_template('index.html') My index() function does one simple thing, it asks Flask to render specific template - index.html. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:5:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"TemplatesYou might guess that a template has to do something with the HTML content rendered by a browser. Yes, it has, but it is far more powerful than a static HTML file. Generating HTML from within Python is not fun, and actually pretty cumbersome because you have to do the HTML escaping on your own to keep the application secure. Because of that Flask configures the Jinja2 template engine for you automatically. So Flask‚Äôs template is a Jinja2-based template which allows you to build dynamic web-pages instead of a static content. To render a template you can use the render_template() method. All you have to do is to provide the name of the template and the variables you want to pass to the template engine. You can name your templates as you like, but normally it will have an .html extension to reflect their purpose. This is my index.html template mentioned earlier bound to the route /. {% extends 'base.html' %} {% block content %} \u003ch2\u003e Welcome to \u003cspan class=\"text-primary\"\u003ePLAZA\u003c/span\u003e. \u003csmall\u003efront-end for python scripts we used to run from console\u003c/small\u003e\u003c/h2\u003e \u003ch3 class=\"text-primary\"\u003e What is PLAZA? \u003c/h3\u003e \u003cp \u003e PLAZA is a web front-end to python scripts built with these goals in mind: \u003cul\u003e \u003cli\u003eallow people with zero python knowledge to use the scripts by interaction through simple Web GUI;\u003c/li\u003e \u003cli\u003ebeautify scripts' output with modern CSS and HTML formatting;\u003c/li\u003e \u003cli\u003eaggregate all the scripts in one repo but in a separate sandboxed directories to increase code manageability.\u003c/li\u003e \u003c/ul\u003e \u003c/p\u003e \u003ch3 class=\"text-primary\"\u003e How to use? \u003c/h3\u003e \u003cp \u003e Navigate through the side menu to the desired script and follow the instructions. \u003c/p\u003e \u003ch3 class=\"text-primary\"\u003e Contacts \u003c/h3\u003e \u003cp \u003e Have any ideas, questions, problems? Visit \u003ca href=\"/contacts\"\u003econtacts\u003c/a\u003e page for all the details. \u003c/p\u003e {% endblock %} And this is how it gets rendered: Dynamic version of the index page can be found on the codepen as well. The trick behind that magic template-\u003erendered page transformation is in the first two lines. This is template inheritance magic - {% extends 'base.html' %} - and that is what makes templating so powerful. Template inheritanceInheritance drill described briefly in the official documentation and the main part of it sounds like this: Template inheritance allows you to build a base ‚Äúskeleton‚Äù template that contains all the common elements of your site and defines blocks that child templates can override. Apart from the official docs, you can watch this video from the ‚ÄúDiscover Flask‚Äù series to better understand how does template inheritance work. Main templateOne of the best practices regarding template inheritance is to compose a base template or a layout for the whole site so every other template will inherit from it. My ‚Äúmain‚Äù template is called base.html and it describes the logical parts for each page in this project. The main template consists of the static parts like Navbar, side menu, it also connects core CSS, JS and fonts. And finally, it specifies where would child template‚Äôs content be placed. \u003c!DOCTYPE HTML\u003e \u003cHTML lang=\"en\"\u003e \u003chead\u003e \u003cmeta charset=\"utf-8\"\u003e \u003cmeta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"\u003e \u003cmeta name=\"viewport\" content=\"width=device-width, initial-scale=1\"\u003e \u003ctitle\u003ePLAZA Project\u003c/title\u003e \u003cmeta name=\"description\" content=\"NOKIA Launchpad for scripts\"\u003e \u003cmeta name=\"author\" content=\"Roman Dodin\"\u003e \u003clink href=\"/static/css/bootstrap.min.css\" rel=\"stylesheet\"\u003e \u003clink href=\"/static/css/style.css\" rel=\"stylesheet\"\u003e \u003c!-- custom CSS --\u003e \u003clink href=\"/static/css/font-awesome.min.css\" rel=\"stylesheet\"\u003e \u003c/head\u003e \u003cbody\u003e \u003c!-- NAV BAR --\u003e \u003cdiv class=\"navbar navbar-nokia navbar-fixed-top\"\u003e \u003cdiv class=\"container\"\u003e \u003cdiv class=\"navbar-header\"\u003e \u003cbutton type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\"\u003e \u003cspan class=\"icon-bar\"\u003e\u003c/span\u003e \u003c/button\u003e \u003ca class=\"navbar-brand\" href=\"/\"\u003ePLAZA\u003c/a\u003e \u003c/div\u003e \u003cdiv class=\"collapse navbar-collapse\"\u003e \u003cul class=\"nav navbar-nav\"\u003e \u003cli\u003e \u003ca href=\"#contact\"\u003eContact\u003c/a\u003e \u003c/","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:5:2","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Flask BlueprintsAnother project‚Äôs major building block is Blueprint. Blueprints are important and actually making it possible to isolate various scripts in their appropriate sandboxes. And by sandbox I mean separate directory which hosts all the files linked to the script. Take a look inside scripts_bank directory which will host all the scripts-related files: ‚îú‚îÄ‚îÄ scripts_bank ‚îÇ ‚îî‚îÄ‚îÄ vmware ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îî‚îÄ‚îÄ get_vmrc_links ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ get_vmrc_links.py ‚îÇ ‚îú‚îÄ‚îÄ static ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ get_vmrc_links_scripts.js ‚îÇ ‚îî‚îÄ‚îÄ templates ‚îÇ ‚îî‚îÄ‚îÄ get_vmrc_links.html It‚Äôs the blueprints which allow us to modularize the app by storing some of it‚Äôs components in the different directories and still be able to link them up to the main Flask app. See how elegantly JS code along with CSS styles needed only by this particular application get_vmrc_links found their‚Äôs place in a separate directory - /scripts_bank/vmware/get_vmrc_links/! ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:6:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Blueprint creationTo create a Blueprint I placed this code in the get_vmrc_links.py: get_vmrc_links_bp = Blueprint('get_vmrc_links', __name__, template_folder='templates', static_folder='static', static_url_path='/get_vmrc_links/static') @get_vmrc_links_bp.route('/get_vmrc_links', methods=['GET','POST']) def get_vmrc_links(): # some code When I created a blueprint I defined it‚Äôs static_url_path to /get_vmrc_links/static . But don‚Äôt get confused if you don‚Äôt see this path, I don‚Äôt have it. That is because blueprints can be registered from a specific point and not directly from the project‚Äôs root. Once we have Blueprint created we need to bind it to the route (line 4 in the snippet above). And again the route /get_vmrc_links will have it‚Äôs root at the directory where Blueprint will be registered later. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:6:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Register a blueprintTo register the blueprint navigate to the main app.py and add the following lines: from scripts_bank.vmware.get_vmrc_links.get_vmrc_links import get_vmrc_links_bp app.register_blueprint(get_vmrc_links_bp, url_prefix='/vmware') Registration is easy! Have you spotted the url_prefix='/vmware' part? This is the Blueprints root directory I was talking about! So now you can glue the parts in a whole picture. Blueprint‚Äôs root directory is /vmware It‚Äôs static directory path is /get_vmrc_links/static which turns to /vmware + /get_vmrc_links/static == /vmware/get_vmrc_links/static The Flask route /get_vmrc_links transforms to /vmware/get_vmrc_links and by following this URL the script‚Äôs page will be rendered ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:6:2","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Front-end\u003c-\u003eback-end data exchangeTo pass data back and forth between front-end and back-end we need to: (@front-end) serialize data from the input elements (@front-end) pass this data to the back-end (@back-end) receive data, make calculations, construct a response, send it (@front-end) receive a response and render it in the output form, handle errors ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:7:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Serializing input dataSerializing is not hard at all. Since it is a front-end‚Äôs task it is done by the JS code which is also stored in a separate file unique to this particular script /scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links.js. This example shows you how you separate one script from another by maintaining all related files in a script‚Äôs folder, in this example I‚Äôm working on get_vmrc_links script, so all the JS and specific HTML templates are stored under /scripts_bank/vmware/get_vmrc_links/ directory. Take a look at get_vmrc_links.js and pay attention to $('#submit_form').click(function(). This function handles things occurring on on-click event to the Submit button. /scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js: // filling data to the input elements based on selection of predefined hosts $('#known_hosts_select').change(function () { $(\"#vmware_ip_addr\").val($('#known_hosts_select option:selected').attr('ip')); $(\"#vmware_login\").val($('#known_hosts_select option:selected').attr('login')); $(\"#vmware_pass\").val($('#known_hosts_select option:selected').attr('pass')); }); $(function() { $('#submit_form').click(function() { // start showing loading animation $.LoadingOverlay(\"show\", { image : \"\", fontawesome : \"fa fa-cog fa-spin\" }) $.ajax({ url: window.location.pathname, // url: /vmware/get_vmrc_links data: $('form').serialize(), type: 'POST', success: function(response) { $.LoadingOverlay(\"hide\"); if (response.error != \"\") { $('#output_div').HTML(response.error) } else { $('#output_div').HTML(response.collected_vm_info) } } }); }); }); String data: $('form').serialize() produces a string of serialized data with all \u003cform\u003e‚Äôs input elements IDs and their values. Along with serialization task, this JS file contains additional things like showing ‚ÄúLoading‚Äù overlay and filling the inputs with the predefined data from select object. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:7:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Sending serialized data to the back-endSerialized data goes via POST method to the back-end via an url you specify. /scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js: $.ajax({ url: newPathname + '/get_vmrc_links', // url: /vmware/get_vmrc_links data: $('form').serialize(), type: 'POST', ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:7:2","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Receiving data by the back-end and processingTo receive serialized data you need to create a POST requests handler: # /scripts_bank/vmware/get_vmrc_links/get_vmrc_links.py @get_vmrc_links_bp.route('/get_vmrc_links', methods=['GET','POST']) def get_vmrc_links(): if request.method == 'GET': return render_template('get_vmrc_links.html') # handle POST method from JQuery elif request.method == 'POST': getvmrc_args = {'host': request.form['vmware_ip_addr'], 'user': request.form['vmware_login'], 'pass': request.form['vmware_pass']} To get the contents arrived in POST I queried form data structure of the request object with appropriate keys. form[] object is an ImmutableDict data structure which contains all the data received in the POST method: ImmutableMultiDict([('vmware_pass', 'mypass'), ('vmware_ip_addr', '172.17.255.253'), ('vmware_login', 'root')]) Once you received your inputs you pass it along to the main function of the chosen script to process. Here I should mention that you have two ways of generating output data: you could leave it in plain text and wrap it in the appropriate HTML tags with Flask or you could enclose scripts' output data in HTML tags during scripts execution process In this example with the get_vmrc_links.py script I chose the latter option and wrapped the whole output of the script (which normally would have found it‚Äôs peace in stdout) with HTML tags: # /scripts_bank/vmware/get_vmrc_links/get_vmrc_links.py \u003c... omitted ...\u003e vmrc_links['collected_vm_info'] += \"\u003cp\u003e\u003cpre\u003e\" # opening paragraph and preformatted section vmrc_links['collected_vm_info'] += \"\u003cstrong\u003eName : \" + vm_summary.config.name + \"\u003c/strong\u003e\u003c/br\u003e\" vmrc_links['collected_vm_info'] += \"Path : \" + vm_summary.config.vmPathName + \"\u003c/strong\u003e\u003c/br\u003e\" \u003c... omitted ...\u003e See these \u003cpre\u003e, \u003cp\u003e and \u003cstrong\u003e tags I used? It‚Äôd done exactly to get rich formatting. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:7:3","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Passing the results back to the front-endOne of the goals of this project was to make script‚Äôs output look more readable. Thanks to modern front-end techniques and frameworks you could render whatever/however you like, your skills are the limit. At this time, my scripts produce just some text which I can render in various ways with HTML. But how do I actually pass this data to the front-end engine and in a what form? I pass it as JSON-formatted structure composed in a two-step process: Firstly, I collected scripts output data as a dict with the keys representing output data and errors (if any): vmrc_links = {'collected_vm_info': '', ## collected results 'error': ''} ## errors Once I have a dict with results and errors to show I use Flask‚Äôs jsonify function to represent my dict as JSON and compose a Response object to pass it further to the front-end: \u003c... omitted ...\u003e elif request.method == 'POST': getvmrc_args = {'host': request.form['vmware_ip_addr'], 'user': request.form['vmware_login'], 'pass': request.form['vmware_pass']} global vmrc_links vmrc_links = {'collected_vm_info': '', 'error': ''} vm_info = main(getvmrc_args) return jsonify(vm_info) And that‚Äôs it. Now fast forward to the front-end and see how it processes received data: /scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js: $(function() { $('#submit_form').click(function() { // start showing loading animation $.LoadingOverlay(\"show\", { image : \"\", fontawesome : \"fa fa-cog fa-spin\" }) $.ajax({ url: window.location.pathname, // url: /vmware/get_vmrc_links data: $('form').serialize(), type: 'POST', success: function(response) { $.LoadingOverlay(\"hide\"); if (response.error != \"\") { $('#output_div').HTML(response.error) } else { $('#output_div').HTML(response.collected_vm_info) } } }); }); }); On a successful return, I check if output has any errors and if it has - put an error message in the #output_div block. If things went smooth I put collected results in this block instead. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:7:4","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Adding new script is easy","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:8:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Create the file structureIt‚Äôs very easy to add a new script. Walk with me and see how easily I add completely separate script called SAM-O XML API Tester. At first, I created directories which represent sandbox for the script in a folder dedicated to storing scripts (scripts_bank). As I said, my directory structure follows my side-menu bar, that‚Äôs why for the new script called SAM-O_XML_API_Tester I first off created root directory 5620sam and then subdirectory SAM-O_XML_API_Tester. The latter dir will carry all files related to this particular script. Do not forget to create empty __init.py__ files inside directories of the script to treat folders as python packages. . ‚îú‚îÄ‚îÄ scripts_bank ‚îÇ ‚îú‚îÄ‚îÄ _5620sam ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ sam_xml_api_tester ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ static ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ templates ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:8:1","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Create HTML filesNow it‚Äôs user-facing HTML template‚Äôs turn. I created sam-o_xml_api_tester.html file in SAM-O_XML_API_Tester/templates dir leveraging sandbox environment. See, this makes individual script management very convenient, one directory stores em all. Following inheritance model this template inherits markup from the content-template.html. As I explained earlier it makes easier to fill in general text information (such as a name of the script, usage guide, author info, etc). Consider this as static or temporary layout for almost every new script. /scripts_bank/5620sam/SAM-O_XML_API_Tester/templates/sam-o_xml_api_tester.html: {% extends 'content_template.html' %} {% block title %} New script {% endblock %} {% block main_purpose %} This is just demo for the blog. {% endblock %} {% block descr %} Description TBD... {% endblock %} {% block usage %} It does nothing, \u003cmark\u003efor now\u003c/mark\u003e {% endblock %} {% block limitations %} My code does not have limitation! {% endblock %} {% block author %} Roman Dodin {% endblock %} {% block version %} 0.1 {% endblock %} {% block tags %} noshut.ru {% endblock %} {% block script_content %} \u003cdiv class=\"container-fluid\"\u003e \u003cdiv class=\"row\"\u003e \u003cdiv class=\"col-md-12\"\u003e \u003cdiv class=\"panel panel-default\"\u003e \u003cdiv class=\"panel-heading\"\u003e \u003ch3 class=\"panel-title\"\u003eInputs\u003c/h3\u003e \u003c/div\u003e \u003cdiv class=\"panel-body\"\u003e \u003c/div\u003e \u003c/div\u003e \u003cdiv class=\"panel panel-default\"\u003e \u003cdiv class=\"panel-heading\"\u003e \u003ch3 class=\"panel-title\"\u003eOutput\u003c/h3\u003e \u003c/div\u003e \u003cdiv class=\"panel-body\" id=\"output_div\"\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e \u003c/div\u003e {% endblock %} {% block added_js %} \u003c!-- nothing here for the moment --\u003e {% endblock %} ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:8:2","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"Create \u0026 register a BlueprintNow it‚Äôs time to write few lines for back-end part. Create a python file which will hold blueprint for this script we‚Äôve been adding and back-end activities: #/scripts_bank/_5620sam/sam_xml_api_tester/sam_xml_api_tester.py from flask import render_template, request, Blueprint, jsonify ############### #### FLASK #### ############### sam_api_tester_bp = Blueprint('sam_api_tester', __name__, template_folder='templates', static_folder='static', static_url_path='/sam_xml_api_tester/static') @sam_api_tester_bp.route('/sam_xml_api_tester', methods=['GET','POST']) def sam_api_tester(): if request.method == 'GET': return render_template('sam_xml_api_tester.html') # handle POST method from JQuery (will be filled later) elif request.method == 'POST': return 0 Register it in the main app.py: from scripts_bank._5620sam.sam_xml_api_tester.sam_xml_api_tester import sam_api_tester_bp app.register_blueprint(sam_api_tester_bp, url_prefix='/5620sam') And you are good to go! ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:8:3","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"How to test PLAZA?Apart from traditional way of cloning a repo and building a virtual environment, you can use a docker container. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:9:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"What‚Äôs next?Tons of useful things are missing at the moment - no search, no active tags, no login-based system, no tests, etc. I will probably add some of this features later, but you are welcome to suggest, blame, and pull-request. Yeah, the code as is can be grabbed from GitHub. ","date":"2016-04-01","objectID":"/2016/04/building-web-front-end-for-python-scripts-with-flask/:10:0","tags":["Bootstrap","Flask","Python"],"title":"Building Web front end for Python scripts with Flask","uri":"/2016/04/building-web-front-end-for-python-scripts-with-flask/"},{"categories":null,"content":"The topic of this post is Layer 3 VPN (L3VPN or VPRN as we call it in SROS) configuration, and I decided to kill two birds with one stone by inviting Juniper vMX to our cozy SROS environment. The BGP/MPLS VPN (RFC 4364) configuration will undergo the following milestones: PE-PE relationship configuration with VPN IPv4 address family introduction PE-CE routing configuration with both BGP and OSPF as routing protocols Export policy configuration for advertising VPN routes on PE routers AS override configuration and many more We‚Äôll wrap it up with the Control Plane/Data Plane evaluation diagrams which help a lot with understanding the whole BGP VPN mechanics. Take your seats, and buckle up! The topology I use throughout this tutorial consists of two customers (namely Alcatel and Juniper) which have two remote sites and want to get connectivity between them by means of an L3VPN service: We start off with ISIS (any IGP would suffice) running smoothly in our provider core so every router can reach every other‚Äôs loopback address. Another prerequisite is to have an MPLS enabled core, since L3VPN uses MPLS encapsulation for dataplane communication. I configured RSVP-TE tunnels between PE routers for this tutorial in the way that PE1_ALU can resolve PE2_JUN (and vice versa) loopback address via RSVP-TE tunnel. Lets have a look at the relevant configuration blocks on the three routers PE1_ALU, P1_ALU and PE2_JUN PE1_ALU: A:PE1_ALU\u003econfig\u003erouter# info ---------------------------------------------- #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 10.10.10.1/32 no shutdown exit interface \"toP1\" address 10.99.99.0/31 port 1/1/2 no shutdown exit autonomous-system 100 #-------------------------------------------------- echo \"ISIS Configuration\" #-------------------------------------------------- isis level-capability level-1 area-id 49.10 traffic-engineering reference-bandwidth 100000000 level 1 wide-metrics-only exit interface \"system\" no shutdown exit interface \"toP1\" interface-type point-to-point no shutdown exit no shutdown exit #-------------------------------------------------- echo \"MPLS Configuration\" #-------------------------------------------------- mpls interface \"system\" no shutdown exit interface \"toP1\" no shutdown exit exit #-------------------------------------------------- echo \"RSVP Configuration\" #-------------------------------------------------- rsvp interface \"system\" no shutdown exit interface \"toP1\" no shutdown exit no shutdown exit #-------------------------------------------------- echo \"MPLS LSP Configuration\" #-------------------------------------------------- mpls path \"loose\" no shutdown exit lsp \"toPE2\" to 10.10.10.3 cspf primary \"loose\" exit no shutdown exit no shutdown exit ## Verification commands A:PE1_ALU# show router route-table 10.10.10.3 =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 10.10.10.3/32 Remote ISIS 01d00h57m 15 10.99.99.1 200 ------------------------------------------------------------------------------- A:PE1_ALU# show router tunnel-table 10.10.10.3 =============================================================================== Tunnel Table (Router: Base) =============================================================================== Destination Owner Encap TunnelId Pref Nexthop Metric ------------------------------------------------------------------------------- 10.10.10.3/32 rsvp MPLS 1 7 10.99.99.1 200 ------------------------------------------------------------------------------- P1_ALU: A:P1_ALU\u003econfig\u003erouter# info ---------------------------------------------- #------------------------------------","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:0:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"BGP L3VPN terminologyBefore we dive deep into the BGP L3VPN configuration it is necessary to refresh on some basic theory. To get a deeper and broader knowledge on the following topic please consider Juniper‚Äôs JUNOS MPLS and VPNs student guide and Alcatel-Lucent‚Äôs Service Routing Architect guide. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:1:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"VRFsIn order to maintain different customer‚Äôs routes independently PE routers use separate logical routing tables called Virtual Routing and Forwarding (VRF). RFC 4364. VRFs: Multiple Forwarding Tables in PEs Each PE router maintains a number of separate forwarding tables. One of the forwarding tables is the ‚Äúdefault forwarding table‚Äù. The others are ‚ÄúVPN Routing and Forwarding tables‚Äù, or ‚ÄúVRFs‚Äù. 3.1. VRFs and Attachment Circuits Every PE/CE attachment circuit is associated, by configuration, with one or more VRFs. An attachment circuit that is associated with a VRF is known as a ‚ÄúVRF attachment circuit‚Äù. In the simplest case and most typical case, a PE/CE attachment circuit is associated with exactly one VRF. When an IP packet is received over a particular attachment circuit, its destination IP address is looked up in the associated VRF. The result of that lookup determines how to route the packet. Provider Edge routers must have a VRF configured for each connected site. VRFs are totally separated in routers control plane by default, so we can depict VRFs as the routers on their own caged in a single hardware unit: VRFs also remain local to the corresponding hosting PE routers and their number representation or names are never propagated to the other PEs. In our example we have four VRFs in total, two VRFs (VRF Alcatel and VRF Juniper) per PE. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:1:1","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Route DistinguisherSince one router can have many routing instances (VRFs) inside, it is necessary to help a router to distinct between the different routes in the different VRFs. It is highly likely that customers connected to a single PE will have overlapping IP addresses and this will potentially lead to troubles as the router won‚Äôt know which customer a route belongs to. I emulated this situation to help you better understand the problem; see, Juniper‚Äôs loopback address for the emulated customers CE1/CE2 overlaps with Alcatel‚Äôs customer loopback addresses. How will a PE router PE1_ALU distinct between these routes? Route Distinguisher (RD) comes to the rescue. RFC 4364. Route Distinguisher definition An RD is simply a number, and it does not contain any inherent information; it does not identify the origin of the route or the set of VPNs to which the route is to be distributed. The purpose of the RD is solely to allow one to create distinct routes to a common IPv4 address prefix. RD can be written in several forms, but it is handy to use the IP address in the Administrator subfield and VPN number in the Assigned number subfield: A:PE1_ALU\u003econfig\u003eservice\u003evprn# route-distinguisher - no route-distinguisher - route-distinguisher \u003crd\u003e \u003crd\u003e : \u003cip-addr:comm-val\u003e|\u003c2byte-asnumber:ext-comm-val\u003e| \u003c4byte-asnumber:comm-val\u003e ip-addr - a.b.c.d comm-val - [0..65535] 2byte-asnumber - [1..65535] ext-comm-val - [0..4294967295] 4byte-asnumber - [1..4294967295] ### EXAMPLE ## A:PE1_ALU\u003econfig\u003eservice\u003evprn# route-distinguisher 10.10.10.1:20 ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:1:2","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"VPN-IPv4 routesA combination of a Route Distinguisher and an IPv4 route effectively produces what is called the VPN-IPv4 route. VPN-IPv4 routes are 12 byte length (8b RD + 4b IPv4) addresses exchanged by MP-BGP speakers. PE routers compose VPN-IPv4 addresses and allocate MPLS labels for the routes before sending them to the MP-BGP neighbors. Consider the picture above to get a visual representation of an VPN-IPv4 route. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:1:3","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Route TargetsSo Route Distinguishers make every VPN-IPv4 route unique in a providers core, but we still need a mechanism to tell what VRF a single VPN-IPv4 route belongs to? We need a way to extend the VPN-IPv4 route with the information about which routing instance this route should be put into. BGP community is a good way to solve this problem. For L3VPNs a specific extended community was defined in RFC 4364 Section 4.3.1 called Route Target. RFC 4364. Route Target definition Every VRF is associated with one or more Route Target (RT) attributes. When a VPN-IPv4 route is created (from an IPv4 route that the PE has learned from a CE) by a PE router, it is associated with one or more Route Target attributes. These are carried in BGP as attributes of the route. Any route associated with Route Target T must be distributed to every PE router that has a VRF associated with Route Target T. When such route is received by a PE router, it is eligible to be installed in those of the PE‚Äôs VRFs that are associated with Route Target T. (Whether it actually gets installed depends upon the outcome of the BGP decision process, and upon the outcome of the decision process of the IGP (i.e., the intra-domain routing protocol) running on the PE/CE interface.) A Route Target attribute can be thought of as identifying a set of sites. (Though it would be more precise to think of it as identifying a set of VRFs.) Associating a particular Route Target attribute with a route allows that route to be placed in the VRFs that are used for routing traffic that is received from the corresponding sites. There is a set of Route Targets that a PE router attaches to a route received from site S; these may be called the ‚ÄúExport Targets‚Äù. And there is a set of Route Targets that a PE router uses to determine whether a route received from another PE router could be placed in the VRF associated with site S; these may be called the ‚ÄúImport Targets‚Äù. The two sets are distinct, and need not be the same. Note that a particular VPN-IPv4 route is only eligible for installation in a particular VRF if there is some Route Target that is both one of the route‚Äôs Route Targets and one of the VRF‚Äôs Import Targets. Usually the RTs are represented as \u003cAS Number of a client network\u003e:\u003cVRF ID\u003e: *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ vrf-target - vrf-target {\u003cext-community\u003e|export \u003cext-community\u003e|import \u003cext-community\u003e} - no vrf-target \u003cext-community\u003e : target:{\u003cip-addr:comm-val\u003e| \u003c2byte-asnumber:ext-comm-val\u003e| \u003c4byte-asnumber:comm-val\u003e} ip-addr - a.b.c.d comm-val - [0..65535] 2byte-asnumber - [0..65535] ext-comm-val - [0..4294967295] 4byte-asnumber - [0..4294967295] ### EXAMPLE ## *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ vrf-target target:200:20 ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:1:4","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"PE\u003c-\u003ePE MP-BGP configurationFirst thing to accomplish in the L3VPN configuration is the BGP peering inside the provider‚Äôs core network. We have two Provider Edge routers (PE) and one core provider (P) router in our simple network. Our business goal is to provide the L3VPN service to our beloved JUN and ALU customers. To do so, we need to configure BGP peering between all the PE routers involved in the L3VPN service setup, these two routers are PE1_ALU and PE2_JUN. The BGP configuration part for PE1_ALU and PE2_JUN routers follows a simple iBGP configuration routine (check BGP configuration tutorial to grab the basics), the only part which is different is a need of a new BGP address family. We need to enable this address family to deal with the VPN routes, which are different from the IPv4 routes. In Juniper this family is called inet-vpn, in SROS it is vpn-ipv4, but nonetheless it is just an address family which enables communication of VPN routes between the peers. We will see later how this family differs from a classic IPv4, but for now just look at the BGP configuration part for both PE routers: PE1_ALU: *A:PE1_ALU\u003econfig\u003erouter\u003ebgp# info ---------------------------------------------- group \"iBGP\" family ipv4 vpn-ipv4 peer-as 100 local-address 10.10.10.1 neighbor 10.10.10.3 exit exit no shutdown ---------------------------------------------- PE2_JUN: bgp { group iBGP { local-address 10.10.10.3; family inet { unicast; } family inet-vpn { unicast; } peer-as 100; neighbor 10.10.10.1; } } As you see, the only part which is related to L3VPN is this new VPN address family. Support for the additional address families transforms a classical BGP to a fancy Multi-Protocol BGP (RFC 4760). Lets see how this family is communicated in the BGP messages: Both routers announces the capability to exchange VPN Unicast IPv4 routes in the BGP OPEN messages. If a BGP peer sees this capability in an incoming OPEN message, it assumes that the neighbor speaks VPN IPv4 routes. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:2:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Configuring VRFs on PE1_ALU (SROS)So far we have configured PE-PE relationship which is a foundation for a working L3VPN service. Our next step is a VRF configuration which can be seen as a customers facing dedicated routers inside a singel PE router hardware unit. We will start with PE1_ALU and configure VRFs 20 and 30. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:3:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"1. Ports configurationAt first we should ensure that customer facing ports operate in access mode. ### customer router `CE1_ALU` connects to a PE via port 1/1/1 *A:PE1_ALU# configure port 1/1/1 shutdown *A:PE1_ALU# configure port 1/1/1 ethernet mode access *A:PE1_ALU# configure port 1/1/1 no shutdown ### customer router `CE1_JUN` connects to a PE via port 1/1/3 *A:PE1_ALU# configure port 1/1/3 shutdown *A:PE1_ALU# configure port 1/1/3 ethernet mode access *A:PE1_ALU# configure port 1/1/3 no shutdown``` ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:3:1","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"2. Customers creationSROS uses the concept of the customers which is similar to the tenants in a virtualization world. I will create two new customers (Customer 1 is a default one) to map them to the customers we have in our network: *A:PE1_ALU\u003econfig\u003eservice# info ---------------------------------------------- customer 1 create description \"Default customer\" exit customer 20 create description \"Juniper\" exit customer 30 create description \"Alcatel\" exit ---------------------------------------------- ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:3:2","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"3. VRF configurationAfter that I create a VPRN service (which is a fancy SROS name for a L3VPN) for each customer: ### create vprn service *A:PE1_ALU# configure service vprn 20 customer 20 create ### give it a name *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ description \"Juniper Site A\" ### create route-distinguisher for VRF 20 *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ route-distinguisher 10.10.10.1:20 ### set route target for this VRF ### here I configure the use of a single target ### for both import and export operations following this form \u003cAS_Num\u003e:\u003cService_Num\u003e *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ vrf-target target:200:20 ### Create an interface in this VRF *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ interface toCE1 create *A:PE1_ALU\u003econfig\u003eservice\u003evprn\u003eif$ address 10.20.99.0/31 ### map a port to this interface. SAP here goes for \"Service Access Point\" *A:PE1_ALU\u003econfig\u003eservice\u003evprn\u003eif$ sap 1/1/3 create *A:PE1_ALU\u003econfig\u003eservice\u003evprn\u003eif\u003esap$ back *A:PE1_ALU\u003econfig\u003eservice\u003evprn\u003eif$ back ### tell a router to resolve Next-Hop address in this VRF with MPLS tunnels *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ auto-bind mpls ### enable VPRN service *A:PE1_ALU\u003econfig\u003eservice\u003evprn$ no shutdown VRF 30 configuration repeats the same steps: A:PE1_ALU\u003econfig\u003eservice# info ---------------------------------------------- vprn 30 customer 30 create route-distinguisher 10.10.10.1:30 auto-bind mpls vrf-target target:300:30 interface \"toCE1\" create address 10.30.99.0/31 sap 1/1/1 create exit exit no shutdown exit ---------------------------------------------- ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:3:3","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Configuring PE -\u003e CE routing protocolsOk, our VRFs 20 and 30 are configured on PE1_ALU router and we have customers interfaces attached. What we need to do next is to configure a routing protocol which will propagate customers routes to the PE router. On PE1_ALU router we will use BGP as a routing protocol towards the CE routers, consequently CE routers will use BGP as well. Lets configure BGP instances for VRFs 20 and 30: BGP configuration for VRF 20: /configure service vprn 20 customer 20 ### specify AS number for BGP speaker in VRF 20 autonomous-system 100 ### configure BGP peer and use \"as-override\" technique bgp group \"toCE\" as-override peer-as 200 local-address 10.20.99.0 split-horizon neighbor 10.20.99.1 exit exit no shutdown exit no shutdown ---------------------------------------------- AS overrideThe as-override command under the BGP section is used to resolve the issue with AS-PATH loop prevention mechanism. When a BGP UPDATE message goes from CE1_JUN over eBGP to PE1_ALU it has AS-PATH value of 200. Then this UPDATE message traverses Service Provider‚Äôs network and as it goes over eBGP session to CE2_JUN its AS-PATH value becomes \"100 200\". But CE2_JUN is a part of AS 200 itself, so it will silently discard a route update with AS-PATH value containing its AS number (AS PATH loop prevention mechanism makes it so). as-override command placed under the BGP context of the receiving VRF on a PE router replaces the customers AS number with Service Providers own AS number, so AS-PATH string of \"100 200\" will become \"100 100\" and will be accepted by the CE router residing in AS 200 since no loop will be detected. Export policiesNote, that it is mandatory to create an export policy on SROS PE routes for incoming BGP-VPN routes to leave the VRF over the PE -\u003e CE routing protocol to the CE router: *A:PE1_ALU# configure router policy-options *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options# begin *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options# policy-statement \"MP-BGP_to_CE\" *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options\u003epolicy-statement\u003eentry$ from protocol bgp-vpn *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options\u003epolicy-statement\u003eentry$ action accept *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options\u003epolicy-statement\u003eentry\u003eaction$ back *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options\u003epolicy-statement\u003eentry$ back *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options\u003epolicy-statement$ back *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options# commit *A:PE1_ALU\u003econfig\u003erouter\u003epolicy-options# info ---------------------------------------------- policy-statement \"MP-BGP_to_CE\" entry 10 from protocol bgp-vpn exit action accept exit exit exit ---------------------------------------------- Now add this policy under the BGP context of the VRF: *A:PE1_ALU# configure service vprn 20 bgp group \"toCE\" export \"MP-BGP_to_CE\" Super, now repeat the steps for VRF 30. The complete service configuration part on PE1_ALU should look as follows: A:PE1_ALU\u003econfig\u003eservice# info ---------------------------------------------- customer 1 create description \"Default customer\" exit customer 20 create description \"Juniper\" exit customer 30 create description \"Alcatel\" exit vprn 20 customer 20 create description \"Juniper Site A\" autonomous-system 100 route-distinguisher 10.10.10.1:20 auto-bind mpls vrf-target target:200:20 interface \"toCE1\" create address 10.20.99.0/31 sap 1/1/3 create exit exit bgp group \"toCE\" as-override export \"MP-BGP_to_CE\" peer-as 200 local-address 10.20.99.0 split-horizon neighbor 10.20.99.1 exit exit no shutdown exit no shutdown exit vprn 30 customer 30 create description \"Alcatel Site A\" autonomous-system 100 route-distinguisher 10.10.10.1:30 auto-bind mpls vrf-target target:300:30 interface \"toCE1\" create address 10.30.99.0/31 sap 1/1/1 create exit exit bgp group \"toCE\" as-override export \"MP-BGP_to_CE\" peer-as 300 local-address 10.30.99.0 split-horizon neighbor 10.30.99.1 exit exit no shutdown exit no shutdown exit ---------------------------------------------- ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:3:4","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Configuring VRFs on PE2_JUN (Juniper)Juniper JUNOS does not use concept of network/access ports, thats why you deal with CE-facing interfaces just like you do with the normal ones: set interfaces ge-0/0/1 unit 0 family inet address 10.20.99.2/31 set interfaces ge-0/0/2 unit 0 family inet address 10.30.99.2/31 Now the VRF part; VRF is called a routing-instance in JUNOS. [edit routing-instances] root@PE2_JUN# show | display set ### create a routing instance and set its type to VRF ### in JUNOS its possible to set VRF name like ### set routing-instance \"Juniper_Site_B\" but we will use numerical id for consistency set routing-instances 20 instance-type vrf ### give VRF a description set routing-instances 20 description \"Juniper Site B\" ### provision interface to CE router, RT and RD set routing-instances 20 interface ge-0/0/1.0 set routing-instances 20 route-distinguisher 10.10.10.3:20 set routing-instances 20 vrf-target target:200:20 ### and for VRF 30 set routing-instances 30 description \"Alcatel Site B\" set routing-instances 30 instance-type vrf set routing-instances 30 interface ge-0/0/2.0 set routing-instances 30 route-distinguisher 10.10.10.3:30 set routing-instances 30 vrf-target target:300:30 VRF configuration on Juniper looks almost identical to Nokia. The major difference here is that you don‚Äôt have to tell JUNOS to resolve VRF‚Äôs next-hop address via MPLS tunnel. And you don‚Äôt have to configure an export policy in case you are using eBGP as a PE-CE protocol. Juniper defaults to that behavior. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:4:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"PE -\u003e CE configuration on Juniper.Note, that with Juniper we omit the explicit AS number configuration under the BGP configuration. In that case the globally configured AS number will be used. Configuration portion for VRF 20 will look as follows: root@PE2_JUN# show | display set \u003c... omitted ...\u003e set routing-instances 20 protocols bgp group toCE local-address 10.20.99.2 set routing-instances 20 protocols bgp group toCE peer-as 200 set routing-instances 20 protocols bgp group toCE as-override set routing-instances 20 protocols bgp group toCE neighbor 10.20.99.3 So far we‚Äôve played with BGP as a PE-CE routing protocol, but frankly speaking OSPF is not a stranger for this task as well. Lets see how to configure the OSPF adjacency between the Juniper PE and a CE2_ALU router. A key piece in this configuration block is the export policy which lets vpn-ipv4 routes imported into VRF 30 to be exported to CE2_ALU over OSPF. Configure an export policy first: [edit] root@PE2_JUN# show \u003c... omitted ...\u003e policy-options { policy-statement MP-BGP_to_CE_via_OSPF { term export { from protocol bgp; then accept; } } } Then configure PE-CE OSPF protocol with the export policy applied: ### optionally set router-id for OSPF process to use routing-options { router-id 100.100.100.100; } protocols { ospf { export MP-BGP_to_CE_via_OSPF; ## configure OSPF area and interfaces area 0.0.0.0 { interface ge-0/0/2.0 { interface-type p2p; } } } } ### DISPLAY SET set routing-instances 30 routing-options router-id 100.100.100.100 set routing-instances 30 protocols ospf export MP-BGP_to_CE_via_OSPF set routing-instances 30 protocols ospf area 0.0.0.0 interface ge-0/0/2.0 interface-type p2p``` Complete VRF configuration for PE2_JUN goes like this: [edit routing-instances] root@PE2_JUN# show 20 { description \"Juniper Site B\"; instance-type vrf; interface ge-0/0/1.0; route-distinguisher 10.10.10.3:20; vrf-target target:200:20; protocols { bgp { group toCE { local-address 10.20.99.2; peer-as 200; as-override; neighbor 10.20.99.3; } } } } 30 { description \"Alcatel Site B\"; instance-type vrf; interface ge-0/0/2.0; route-distinguisher 10.10.10.3:30; vrf-target target:300:30; routing-options { router-id 100.100.100.100; } protocols { ospf { export MP-BGP_to_CE_via_OSPF; area 0.0.0.0 { interface ge-0/0/2.0 { interface-type p2p; } } } } } ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:4:1","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"CE -\u003e PE routing protocol configurationNow it is time to connect our customers to the service provider‚Äôs network via VRFs created earlier and finally add some VPN routes. CE routers completely unaware of a complex L3VPN configuration on the PE routers, what they need to do is just setup a routing protocol over which customers routes could be delivered to (and received from) the Service Provider. Starting with Juniper CE1_JUN and CE2_JUN that run eBGP with PE routers: CE1_JUN: root@CE1_JUN# show ### Last changed: 2015-10-28 17:40:05 UTC version 14.1R1.10; \u003c... omitted ...\u003e ### 1. configure PE-facing interface interfaces { ge-0/0/0 { mac 50:01:00:06:00:02; unit 0 { family inet { address 10.20.99.1/31; } } } ### 2. dont forget about loopback, this address will be exported to remote site lo0 { unit 0 { family inet { address 1.1.1.1/32; } } } } ### 3. AS number is a mandatory for eBGP session to run routing-options { autonomous-system 200; } ### 4. and a simple eBGP configuration protocols { bgp { group toPE { local-address 10.20.99.1; ## 5. we need to export loopback address to eBGP export export_loopback; peer-as 100; neighbor 10.20.99.0; } } } ### 6. policy to export loopback address policy-options { prefix-list loopback { 1.1.1.1/32; } policy-statement export_loopback { term Loopback { from { prefix-list loopback; } then accept; } } } CE2_JUN: root@CE2_JUN# show ### Last changed: 2015-10-28 16:50:23 UTC version 14.1R1.10; ### 1. configure PE-facing interface interfaces { ge-0/0/0 { unit 0 { family inet { address 10.20.99.3/31; } } } ### 2. dont forget about loopback, this address will be exported to remote site lo0 { unit 0 { family inet { address 2.2.2.2/32; } } } } ### 3. AS number is a mandatory for eBGP session to run routing-options { autonomous-system 200; } ### 4. and a simple eBGP configuration protocols { bgp { group toPE { local-address 10.20.99.3; family inet { unicast; } ## 5. we need to export loopback address to eBGP export export_loopback; peer-as 100; neighbor 10.20.99.2; } } } ### 6. policy to export loopback address policy-options { prefix-list loopback { 2.2.2.2/32; } policy-statement export_loopback { term Loopback { from { prefix-list loopback; } then accept; } } } Now its Nokia time. Pay attention to the CE2_ALU router, since we are using OSPF on CE2-PE2 link configuration it is a little bit different from other CE‚Äôs configs. CE1_ALU: A:CE1_ALU\u003econfig\u003erouter# info ---------------------------------------------- ### 1. Interfaces and AS Num config #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 1.1.1.1/32 no shutdown exit interface \"toPE\" address 10.30.99.1/31 port 1/1/1 no shutdown exit autonomous-system 300 ### 2. Policy for exporting loopback address #-------------------------------------------------- echo \"Policy Configuration\" #-------------------------------------------------- policy-options begin prefix-list \"loopback\" prefix 1.1.1.1/32 exact exit policy-statement \"export_loopback\" entry 10 from prefix-list \"loopback\" exit action accept exit exit exit commit exit #-------------------------------------------------- echo \"BGP Configuration\" #-------------------------------------------------- bgp group \"toPE\" export \"export_loopback\" ## tell CE to export its system address to eBGP peer peer-as 100 local-address 10.30.99.1 split-horizon neighbor 10.30.99.0 exit exit no shutdown exit ---------------------------------------------- CE2_ALU: A:CE2_ALU\u003econfig\u003erouter# info ---------------------------------------------- #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 2.2.2.2/32 no shutdown exit interface \"toPE\" address 10.30.99.3/31 port 1/1/1 no shutdown exit autonomous-system 300 ### we running OSPF as PE-CE protocol #-------------------------------------------------- echo \"OSPFv2 Configuration\" #---","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:5:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Control plane walkthroughWe are done with the configuration, all in all it was not a complex task, what is more important is to understand what‚Äôs going on with control and data planes. I believe you will like this step-by-step walkthrough via every node in the network. I will start with dissection of a control plane operation from the point where MP-BGP session between PE routers has been already established and we are enabling VRFs on customers routers. Refer to this overview chart and see how an information about CE1_JUN‚Äôs loopback interface propagates through the entire network to CE2_JUN counterpart: All the pictures are clickable, to see the full sized pics choose ‚Äúopen an image in a separate tab‚Äù option in your browser. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 1CE1_JUN router has an export policy export_loopback configured which is used by BGP to construct the BGP UPDATE message with lo0 prefix as an NLRI. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:1","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 2CE1_JUN sends a regular BGP UPDATE message to its eBGP peer PE1_ALU. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:2","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 3PE1_ALU router receives this update via its interface toCE1 configured in vprn 20 context. PE1_ALU populates its VRF 20 with a route to 1.1.1.1/32 via 10.20.99.1. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:3","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 4PE1_ALU router has an established MP-iBGP session with PE2_JUN so it takes a BGP route from VRF 20 and automatically sends an MP-BGP UPDATE message to its peer. Note, that ALU routers will send MP-BGP update automatically only for the connected to VRF routes and the routes received via BGP. If we had OSPF between CE1 and PE1, we would need to configure an export policy to propagate this update over MP-BGP session. Since PE1_ALU router wants to send an update for a route in the VRF it should construct an MP-BGP Update message which has a specific Path attribute - MP_REACH_NLRI - to communicate this routing information. And PE1_ALU will transform the 1.1.1.1/32 IPv4 prefix to an VPN-IPv4 one. Take a closer look at this BGP message. See how PE1_ALU router added some valuable additional information to correctly pass CE1_ALU‚Äôs loopback address via MP-BGP. First of all examine how NLRI has been transformed in MP-BGP: it now has a Route Distinguisher which we configured for VRF 20 earlier, it has the IPv4 prefix itself and it has the MPLS label 131068. PE1_ALU router allocated a VPN label which it associated with the VRF 20. This label tells PE1_ALU router that if it ever receives a data packet with this label it should associate the data encapsulated within it with VRF 20! This way ingress PE routers tell others PEs what label should be used as a VPN label for the routes residing in a particular VRF. There are two methods of allocating the VPN labels (they are also called Service labels): per VRF: all routes originated from a single VRF will have the same VPN label. SROS routers default to this. per VRF per next-hop: If a VRF has \u003e1 CE interfaces, PE router will allocate different labels for different CE interfaces inside one VRF. Juniper routers default to this. If we zoom over the Extended Community attribute of the BGP UPDATE message, we can spot the Route Target 200:20 value there. Important things happened to the Next-Hop, not only it looks now like a VPN-IPv4 route with a Route Distinguisher value of 0:0 and without MPLS label, but Next-Hop IPv4 address has been changed to PE1_ALU‚Äôs system (loopback) interface 10.10.10.1. This is how PE1 router tells PE2 that it can reach VRF 20 routes via PE1. In the end of the day, PE1_ALU‚Äôs update reaches PE2_JUN since it has the IP destination address of 10.10.10.3. Notice, that BGP updates traverse Service Provider‚Äôs network in a form of the simple IP packets, MPLS is out of the picture at this moment. Service Provider‚Äôs core router - P1_ALU - simply routes IP packets and has no take in BGP at all. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:4","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 5PE2_JUN receives the BGP UPDATE with VPN-IPv4 route. Once this route passes validation checks (Nexhop resolvable, no AS Path loop) PE2 submits this route to a specific table named bgp.l3vpn.0. This table stores all BGP VPN routes, refer to this figure to examine some of its content: PE2 extracts the routing information from this update an based on the Route Target value installs the IPv4 route 1.1.1.1/32 into the VRF 20 table - 20.inet.0. PE2 resolves the next-hop address of the fellow PE1_ALU (10.10.10.1) via MPLS Label Switched Path (LSP) and stores this information in the 20.inet.0 table: 20.inet.0: 5 destinations, 5 routes (5 active, 0 holddown, 0 hidden) + = Active Route, - = Last Active, * = Both 1.1.1.1/32 *[BGP/170] 2d 08:38:15, localpref 100, from 10.10.10.1 AS path: 200 I, validation-state: unverified \u003e to 10.99.99.2 via ge-0/0/0.0, label-switched-path toPE1 Remember that it is mandatory to have an active LSP to the remote PE, since we have to have an MPLS transport to the remote end to carry the data packets. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:5","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 6Since we installed the route for the 1.1.1.1/32 IPv4 prefix into VRF 20 and we have an active eBGP peer in VRF 20, we should send an update for this IPv4 prefix to the CE2_JUN router to let the CE2 site to be aware of the remote prefix. This update goes as an ordinary eBGP update. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:6","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 7CE2_JUN receives the BGP UPDATE and installs a route into the only table it has for IPv4 routes - inet.0. This completes Control Plane operation regarding the prefix 1.1.1.1/32, same process goes for the other loopbacks and connected to VRFs link addresses for both Alcatel and Juniper customers. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:6:7","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Data plane walkthroughTo complete this post we should examine the data plane operations. We will see how data packets destined to 1.1.1.1 propagate through the network using the labels allocated during the control plane operations. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:0","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 1CE2_JUN wants to send a data packet to CE1_JUN via L3VPN service provided by our network. CE2 has an active route in its route table inet.0 that says that it can reach 1.1.1.1/32 via 10.20.99.2 address via the ge-0/0/0 interface. CE2 has a MAC address for 10.20.99.2 so it constructs the whole frame and puts it on the wire. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:1","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 2PE2_JUN receives the Ethernet frame on its interface ge-0/0/1 which belongs to VRF 20, that is how PE2 decides to associate this packet with VRF 20. PE2 consults with the VRF 20 routing table and sees that it has to use the LSP toPE1 to send the incoming data packet further. Then PE2 gets MPLS label which it received earlier from its RSVP neighbor P1_ALU during the LSP signalization process. But this was just a transport MPLS label, it helps PE2_JUN to reach PE1_ALU, but PE2 needs one label more - the VPN Label - to tell PE1_ALU to which VRF this data belongs. This label was signalled earlier (see Control Plane operation section) via MP-BGP. Now PE2 has everything it needs: MPLS VPN label to encapsulate the data packets from its VRF 20 destined to the VRF 20 on PE1_ALU Transport MPLS Label to get to PE1_ALU via MPLS core and thus it constructs a packet with two labels stacked and fires it off. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:2","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 3P1_ALU is totally unaware of the whole services and customers mess, it just switches MPLS packets by replacing the incoming transport label with the outgoing one. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:3","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 4PE1_ALU receives an MPLS packet from P1_ALU. It pops out the transport label (fig. 4.1) and examines the enclosed MPLS label. This label value 131068 was signalled by PE1_ALU via MP-BGP during the Control Plane operation. So PE1 knows that it has to pop this label and associate the enclosed packet with the VPRN 20 (VRF 20) (fig. 4.2) VRF‚Äôs 20 routing table says that packets destined to 1.1.1.1 should be forwarded to 10.20.99.3 address (fig. 4.3), which is a connected network leading to CE1_JUN (fig. 4.4). PE1_ALU constructs the packet and moves it via Ethernet out of the 1/1/2 port (fig. 4.5). ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:4","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"Step 5CE2_JUN receives an ordinary IP packet with a destination address matching its interface. It decapsulates ICMP echo request and sends back the echo reply. This concludes the control and data plane operations walk through. If you followed along the explanations and practiced the configuration steps, you should be in a good shape to implement the basic L3VPN services and also should have a pretty solid understanding of the service establishment mechanics. ","date":"2015-11-03","objectID":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/:7:5","tags":["Nokia","Juniper","L3VPN","VPRN","BGP"],"title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS \u0026 Juniper MX","uri":"/2015/11/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-alcatel-lucent-juniper/"},{"categories":null,"content":"In the first part of this BGP tutorial we prepared the ground by configuring eBGP/iBGP peering. We did a good job overall, yet the plain BGP peering is not something you would not normally see in production. The power of BGP is in its ability for granular management of multiple routes from multiple sources. And the tools that help BGP to handle this complex task are BGP policies at their full glory. In this part we will discuss and practice: BGP export/import policies for route advertisement/filtering BGP communities operations BGP routes aggregation: route summarization and the corresponding aggregate and atomic-aggregate path attributes What are BGP policies for?The BGP peering configuration process is simple, you saw it in Part 1, but, frankly, no network engineer leaves a BGP router in a default ‚Äúreceive all, advertise all‚Äù state. We use BGP policies to tell the router which routes to accept and which to advertise. This is just an example on where the BGP policies play a part, but they are used for many other tasks as well. In BGP you can define two types of policies: Import and Export. To demonstrate where and when does a particular policy take place I paste here the BGP route processing diagram: Export polices are used for: export routes from different protocols to BGP (like IGP routes being exported to BGP in Part 1) granular control of the advertised routes prohibit unwanted prefixes advertising set the path attributes to a desired NLRI reducing control plane traffic by advertising aggregate routes Import policies are used for: filtering unwanted NLRI by prefix, prefix-length, community value manipulation with the outbound traffic applying Local-Pref attribute to desired prefixes modifying/setting MED value or any other transitive attribute In SROS BGP policy configuration takes place in a router‚Äôs policy-options context - configure router policy-options. To practice with BGP policies configuration we will go through a set of tasks that an ISP engineer can be expected to do in theirs day-to-day operations. We will simulate a simple ISP scenario using the following network topology: Network interfaces, IGP and basic BGP configuration are done exactly the same as in the Part 1 of this series. If you are interested in the final configuration output, please refer to the Wrapping up section. Community attributeLets statr with BGP communities introduction. A BGP community (not extended) is an optional transitive BGP Path attribute that is a group of destinations which share some common property as per the RFC 1997. I like to think of a community as a label (or a tag) which BGP speaker puts on a NLRI to give it a context. These labels could serve different purposes, for example: to mark/identify the prefixes originated from a specific geographic region, customer or service, or to indicate that a specific treatment is desired like for the prefixes with the use of no-export or no-advertise community values or could mean any other property which BGP speaker wants to communicate within an NLRI. BGP Communities are just the means to augment the specific prefixes with the metadata, they are useless until you bind some actions to them. For example, you tag some prefixes with a community A value and others with a community B; then you could tell your BGP peers to, say, set the Local Preference 200 attribute for the prefixes that have community A value and leave Local Preference intact for the ones marked with community B value. In this examples communities allowed us to set a specific action based on the community value associated with the prefixes. Communities are represented by a community string which is a 32 bit value. First two bytes of a community attribute have to be encoded with an AS number where community was born, and the other two bytes are set by an AS network engineer as he pleases (or in other words, the community string follows this template \u003c2byte-asnumber:community-value\u003e). The community attribute values range from ","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:0:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Adding communityCommunity strings are configured under the policy-options context. Before we will be able to add communities to the prefixes, we need to: Specify the prefix-lists for our the routes we would like to mark with a community Declare the community strings we want to refer later to. R5 policy config: *A:R5\u003econfig\u003erouter\u003epolicy-options# info ---------------------------------------------- prefix-list \"Customer_1\" prefix 10.10.55.0/24 exact prefix 10.10.66.0/24 exact exit prefix-list \"Customer_2\" prefix 172.10.55.0/24 exact prefix 172.10.66.0/24 exact exit community \"East\" members \"65510:200\" community \"West\" members \"65510:100\" community \"Customer_2\" members \"65510:2\" ---------------------------------------------- The same policy-options configuration block is applied to R6. To tag the routes with a community value we have to additionally configure a policy-statement which will associate a community string with the selected prefixes. This policy will be later referenced in the BGP configuration as the export policy. Consider the following policy-statement \"Adv_Customers_nets\" that is configured on R5: *A:R5\u003econfig\u003erouter\u003epolicy-options# info ---------------------------------------------- \u003cprefix-lists and community strings are omitted\u003e policy-statement \"Adv_Customers_nets\" ## ======================================================================= entry 10 from protocol direct ## matches every connected network exit action next-entry ## next-entry means that upon successful completion of this action ## we DON'T stop the policy evaluation process and proceed to the next entry community add \"West\" ## adding community to every direct connected route ## matched by the \"from\" statement exit exit ## ======================================================================= entry 20 from prefix-list \"Customer_2\" ## matches Customer_2 routes exit action accept ## action \"accept\" stops policy evaluation effective immediately ## and its okay in this situation, since we have only two customers ## so if we matched \"Customer_2\" and marked it with its community, ## then we have nothing to do else and can stop policy evaluation community add \"Customer_2\" exit exit ## ======================================================================= entry 30 from prefix-list \"Customer_1\" exit action accept ## we do not need to add communities for the Customer_1 routes, but we need to create ## an \"action accept\" for its prefixes, since the default action is deny-all exit exit exit ---------------------------------------------- The same policy statement configuration should be created on R6 router. In the example above we added the community string using the community add operation; SROS also has additional operations provided for the community strings: - community add \u003cname\u003e [\u003cname\u003e...(upto 28 max)] - community remove \u003cname\u003e [\u003cname\u003e...(upto 28 max)] - community replace \u003cname\u003e [\u003cname\u003e...(upto 28 max)] - no community As was explained before, the policy statement should then be applied as an export policy in the respective BGP configuration on both R5 and R6: *A:R5# configure router bgp group \"iBGP\" *A:R5\u003econfig\u003erouter\u003ebgp\u003egroup# export \"Adv_Customers_nets\" *A:R6# configure router bgp group \"iBGP\" *A:R6\u003econfig\u003erouter\u003ebgp\u003egroup# export \"Adv_Customers_nets\" Now take a look at the show router bgp summary command on R5. There are 2 routes advertised to every iBGP neighbor and 2 rotes received from its fellow - R6: =============================================================================== BGP Summary =============================================================================== Neighbor AS PktRcvd InQ Up/Down State|Rcv/Act/Sent (Addr Family) PktSent OutQ ------------------------------------------------------------------------------- 10.10.10.1 65510 593 0 04h55m11s 0/0/2 (IPv4) 595 0 10.10.10.2 65510 593 0 04h55m09s 0/0/2 (IPv4) 595 0 10.10.10.6 65510 592 0 04h53m51s 2/2/2 (IPv4) 592 0 ------------------------------------------------------------------","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:1:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Show communitiesTo ensure that the correct communities were passed along with the NLRI we can leverage the show router bgp routes \u003cprefix\u003e detail command and check the Community column in its output: A:R1# show router bgp routes 172.10.55.0/24 detail =============================================================================== BGP Router ID:10.10.10.1 AS:65510 Local AS:65510 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Original Attributes Network : 172.10.55.0/24 Nexthop : 10.10.10.5 Path Id : None From : 10.10.10.5 Res. Nexthop : 10.10.99.5 Local Pref. : 100 Interface Name : toR5 Aggregator AS : None Aggregator : None Atomic Aggr. : Not Atomic MED : None AIGP Metric : None Connector : None Community : 65510:2 65510:100 Cluster : No Cluster Members Originator Id : None Peer Router Id : 10.10.10.5 Fwd Class : None Priority : None Flags : Used Valid Best IGP Route Source : Internal AS-Path : No As-Path Route Tag : 0 Neighbor-AS : N/A Orig Validation: NotFound Source Class : 0 Dest Class : 0 A:R1# show router bgp routes 10.10.55.0/24 detail =============================================================================== BGP Router ID:10.10.10.1 AS:65510 Local AS:65510 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Original Attributes Network : 10.10.55.0/24 Nexthop : 10.10.10.5 Path Id : None From : 10.10.10.5 Res. Nexthop : 10.10.99.5 Local Pref. : 100 Interface Name : toR5 Aggregator AS : None Aggregator : None Atomic Aggr. : Not Atomic MED : None AIGP Metric : None Connector : None Community : 65510:100 Cluster : No Cluster Members Originator Id : None Peer Router Id : 10.10.10.5 Fwd Class : None Priority : None Flags : Used Valid Best IGP Route Source : Internal AS-Path : No As-Path Route Tag : 0 Neighbor-AS : N/A Orig Validation: NotFound Source Class : 0 Dest Class : 0 Everything works as expected. Customer_1 routes are tagged with just the ‚ÄúWest‚Äù community, and the Customer_2 routes are being tagged with both ‚ÄúWest‚Äù and ‚ÄúCustomer_2‚Äù communities. Lets go a bit deeper and see how the community strings are carried in the BGP Updates messages: Community path attribute propagates across eBGP links as well. To check this we can filter the routes with a specfic community that R3 has in its BGP routes tables. Note, that R3 resides in the AS 65520 and thus is a eBGP peer: ## show BGP routes labelled with a specific community string A:R3# show router bgp routes community 65510:2 =============================================================================== BGP Router ID:10.20.20.3 AS:65520 Local AS:65520 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Flag Network LocalPref MED Nexthop (Router) Path-Id Label As-Path ------------------------------------------------------------------------------- u*\u003ei 172.10.55.0/24 None None 10.0.99.0 None - 65510 *i 172.10.55.0/24 100 None 10.20.20.4 None - 65510 u*\u003ei 172.10.66.0/24 None No","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:2:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Operation replace \u0026 Well-known communitiesRFC 1997 specifies the following well-known communities as the well-known communities: NO_EXPORT (0xFFFFFF01) All routes received carrying a communities attribute containing this value MUST NOT be advertised outside a BGP confederation boundary (a stand-alone autonomous system that is not part of a confederation should be considered a confederation itself). NO_ADVERTISE (0xFFFFFF02) All routes received carrying a communities attribute containing this value MUST NOT be advertised to other BGP peers. NO_EXPORT_SUBCONFED (0xFFFFFF03) All routes received carrying a communities attribute containing this value MUST NOT be advertised to external BGP peers (this includes peers in other members autonomous systems inside a BGP confederation). You will encounter no-export and no-advertise communities quite often, as they are naturally used for route advertisement manipulations. To demonstrate the power of the default communities I would like to introduce you to another AS 65530 which has the single-homed peering with AS 65520: Router R7 residing in AS 65530 has the default BGP configuration: A:R7\u003econfig\u003erouter\u003ebgp# info ---------------------------------------------- group \"no_export_example\" peer-as 65520 split-horizon neighbor 10.0.99.4 local-address 10.0.99.5 exit exit no shutdown ---------------------------------------------- As a result of this configuration it receives all BGP routes that R3 has in its BGP Rib-Out database: R3 BGP routes: A:R3# show router bgp routes =============================================================================== BGP Router ID:10.20.20.3 AS:65520 Local AS:65520 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Flag Network LocalPref MED Nexthop (Router) Path-Id Label As-Path ------------------------------------------------------------------------------- u*\u003ei 10.10.55.0/24 None None 10.0.99.0 None - 65510 *i 10.10.55.0/24 100 None 10.20.20.4 None - 65510 u*\u003ei 10.10.66.0/24 None None 10.0.99.0 None - 65510 *i 10.10.66.0/24 100 None 10.20.20.4 None - 65510 u*\u003ei 172.10.55.0/24 None None 10.0.99.0 None - 65510 *i 172.10.55.0/24 100 None 10.20.20.4 None - 65510 u*\u003ei 172.10.66.0/24 None None 10.0.99.0 None - 65510 *i 172.10.66.0/24 100 None 10.20.20.4 None - 65510 ------------------------------------------------------------------------------- Routes : 8 =============================================================================== R7 BGP routes: A:R7# show router bgp routes =============================================================================== BGP Router ID:10.30.30.7 AS:65530 Local AS:65530 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Flag Network LocalPref MED Nexthop (Router) Path-Id Label As-Path ------------------------------------------------------------------------------- u*\u003ei 10.10.55.0/24 None None 10.0.99.4 None - 65520 65510 u*\u003ei 10.10.66.0/24 None None 10.0.99.4 None - 65520 65510 u*\u003ei 172.10.55.0/24 None None 10.0.99.4 None - 65520 65510 u*\u003ei 172.10.66.0/24 None None 10.0.99.4 None - 65520 65510 ------------------------------------------------------------------------------- Routes : 4 =============================================================================== Moreover, R7 receives all the community valu","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:3:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Removing communityCurrently R2 does not modify any routes flying off to its eBGP peer R4. Lets imagine that we now have to remove the community Customer_2 from all the Customer_2 routes on R2 before advertising them to the AS 65520. To do so we have to perform a community remove operation. To filter all the routes with a community string 65510:2 we could use prefix lists or filter by the community string value: *A:R2\u003econfig\u003erouter\u003epolicy-options# info ---------------------------------------------- community \"Customer2\" members \"65510:2\" policy-statement \"remove_Cust2_community\" entry 10 from community \"Customer2\" ## filter routes with specific community value exit action accept community remove \"Customer2\" exit exit exit ---------------------------------------------- *A:R2\u003econfig\u003erouter\u003ebgp\u003egroup# export \"remove_Cust2_community\" *A:R2\u003econfig\u003erouter\u003ebgp\u003egroup# info ---------------------------------------------- export \"remove_Cust2_community\" peer-as 65520 split-horizon neighbor 10.0.99.3 local-address 10.0.99.2 exit ---------------------------------------------- Now R2 selects the routes with the community value of 65510:2 and removes it before sending it to its eBGP peer R4: The following output verifies this behavior: *A:R2# show router bgp routes 172.10.55.0/24 hunt =============================================================================== BGP Router ID:10.10.10.2 AS:65510 Local AS:65510 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== ------------------------------------------------------------------------------- RIB In Entries ------------------------------------------------------------------------------- Network : 172.10.55.0/24 Nexthop : 10.10.10.5 Path Id : None From : 10.10.10.5 Res. Nexthop : 10.10.99.0 Local Pref. : 100 Interface Name : toR1 Aggregator AS : None Aggregator : None Atomic Aggr. : Not Atomic MED : None AIGP Metric : None Connector : None Community : 65510:2 65510:100 ## R2 receives this prefix with 2 communities Cluster : No Cluster Members Originator Id : None Peer Router Id : 10.10.10.5 Fwd Class : None Priority : None Flags : Used Valid Best IGP Route Source : Internal AS-Path : No As-Path Route Tag : 0 Neighbor-AS : N/A Orig Validation: NotFound Source Class : 0 Dest Class : 0 ------------------------------------------------------------------------------- RIB Out Entries ------------------------------------------------------------------------------- Network : 172.10.55.0/24 Nexthop : 10.0.99.2 Path Id : None To : 10.0.99.3 Res. Nexthop : n/a Local Pref. : n/a Interface Name : NotAvailable Aggregator AS : None Aggregator : None Atomic Aggr. : Not Atomic MED : None AIGP Metric : None Connector : None Community : 65510:100 ## on export operation R2 removes Customer_2 community Cluster : No Cluster Members Originator Id : None Peer Router Id : 10.20.20.4 Origin : IGP AS-Path : 65510 Route Tag : 0 Neighbor-AS : 65510 Orig Validation: NotFound Source Class : 0 Dest Class : 0 ------------------------------------------------------------------------------- Routes : 2 =============================================================================== ## same thing happened to another Customer_2 route A:R2# show router bgp routes 172.10.66.0/24 hunt =============================================================================== BGP Router ID:10.10.10.2 AS:65510 Local AS:65510 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =======================","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:4:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Matching communityThe whole thing with the communities is about performing the actions against the prefixed matched with it. As you saw previously, the action based on a community should first match the community by its value and there are several ways to do the matching. How can you pick the routes with the different communities and modify them altogether? One way would be to write multiple policy-statements with the same action and different from community \u003cname\u003e statements. This is a bruteforce. There are far more elegant techniques we are going to explore. ","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:5:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"‚ÄúAND‚Äù and ‚ÄúOR‚Äù operatorsTo create a community that matches some of the community values you can use the | operator like that: community West_or_Customer_2 members 65510:2|65510:100. This community statement will match prefixes with the community strings like 65510:2, 65510:2 65510:100 or 65510:2 63300:2 54487:200. To create a community statement that will match on a string that has multiple community values (aka AND operator) you can compose the following community statement: community \"A_and_B\" members \"65510:2\" \"65510:100\". This community will match 65510:2 65510:100 or 100:100 200:200 65510:2 65510:100 but not 65510:2 or 100:100 65510:2. ","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:5:1","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"ExpressionsNokia SR-OS has the built-in expressions engine equipped with a set of most commonly used operators. The Expressions syntax is described in the Routing Protocols guide: community \u003cname\u003e expression \u003cexpression\u003e [exact] \u003cexpression\u003e : [900 chars max] - \u003cexpression\u003e is one of the following: \u003cexpression\u003e {AND|OR} \u003cexpression\u003e [NOT] ( \u003cexpression\u003e ) [NOT] \u003ccomm-id\u003e \u003cexact\u003e : keyword A good example on the expressions syntax is demonstrated by the statement below which matches exactly the prefixes with agiven list of communities attached to them: community \"West_and_Cust_2_ONLY\" expression \"65510:2 AND 65510:100\" exact This community, once used in the from statement of a policy will match prefixes with the community string 65510:2 65510:100 only (enforced by the exact statement). ","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:5:2","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"Regular ExpressionsAnother filtering technique is based on the regular expressions. Let me show you how easy it is to filter the routes with a community string containing 65510:100 or 65510:2 values: *A:R1\u003econfig\u003erouter\u003epolicy-options# info ---------------------------------------------- prefix-list \"Customer_1_55_network\" prefix 10.10.55.0/24 exact exit community \"West\" members \"65510:100\" community \"no-export\" members \"no-export\" community \"no-advertise\" members \"no-advertise\" ## this community string equals to the form: ## members list contains 65510:2 OR 65510:100 community \"Customer_2_from_West\" members \"65510:(2|100)\" \u003c...omitted...\u003e policy-statement \"test_regexp_community\" entry 10 from community \"Customer_2_from_West\" exit action accept community replace \"no-advertise\" exit exit exit ---------------------------------------------- Here I defined the Customer_2_from_West community and used a simple regular expression for to match on the communities 65510:100 or 65510:2. This regexp community string will match strings like 65510:2 , 65510:2 65510:100 , 65510:2 63300:2 54487:200 Of course you can create a far more complex regexps, check the table of the supported operators of the Routing Protocols Guide to build a regexp that meets your needs. Wrapping upAs usual, check out the full config that you will have at the end of this tutorial: R1: A:R1\u003econfig\u003erouter# info ---------------------------------------------- #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 10.10.10.1/32 no shutdown exit interface \"toR2\" address 10.10.99.0/31 port 1/1/1 no shutdown exit interface \"toR3\" address 10.0.99.0/31 port 1/1/3 no shutdown exit interface \"toR5\" address 10.10.99.4/31 port 1/1/4 no shutdown exit autonomous-system 65510 #-------------------------------------------------- echo \"ISIS Configuration\" #-------------------------------------------------- isis level-capability level-1 area-id 10.10 reference-bandwidth 100000000 level 1 wide-metrics-only exit level 2 wide-metrics-only exit interface \"system\" no shutdown exit interface \"toR2\" interface-type point-to-point no shutdown exit interface \"toR5\" interface-type point-to-point no shutdown exit no shutdown exit #-------------------------------------------------- echo \"Policy Configuration\" #-------------------------------------------------- policy-options begin prefix-list \"Customer_1_55_network\" prefix 10.10.55.0/24 exact exit community \"West\" members \"65510:100\" community \"no-export\" members \"no-export\" community \"no-advertise\" members \"no-advertise\" community \"Customer_2_from_West\" members \"65510:(2|100)\" policy-statement \"replace_with_no_exp\" entry 10 from prefix-list \"Customer_1_55_network\" exit action accept community replace \"no-export\" exit exit exit policy-statement \"test_regexp_community\" entry 10 from community \"Customer_2_from_West\" exit action accept community replace \"no-advertise\" exit exit exit commit exit #-------------------------------------------------- echo \"BGP Configuration\" #-------------------------------------------------- bgp group \"eBGP\" export \"replace_with_no_exp\" \"test_regexp_community\" peer-as 65520 split-horizon neighbor 10.0.99.1 local-address 10.0.99.0 exit exit group \"iBGP\" next-hop-self peer-as 65510 neighbor 10.10.10.2 exit neighbor 10.10.10.5 exit neighbor 10.10.10.6 exit exit no shutdown exit R2: A:R2\u003econfig\u003erouter# info ---------------------------------------------- #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 10.10.10.2/32 no shutdown exit interface \"toR1\" address 10.10.99.1/31 port 1/1/1 no shutdown exit interface \"toR4\" address 10.0.99.2/31 port 1/1/3 no shutdown exit interface \"toR6\" address 10.10.99.2/31 port 1/1/4 no shutdown exit autonomous-system 65510 #-------------------------------------------------- echo \"ISIS Configuration","date":"2015-09-23","objectID":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/:5:3","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","uri":"/2015/09/alcatel-lucent-bgp-configuration-tutorial-part-2-bgp-policies-community/"},{"categories":null,"content":"There is no way I would leave you without covering configuration steps for one of the most versatile, scalable and robust internet protocols also known as BGP. And here it is - BGP configuration guide for Nokia (Alcatel-Lucent) Service Routers. As with the OSPF configuration tutorial I will cover the configuration process for various BGP scenarios along with the verification and troubleshooting steps bundled with colorful figures, detailed code snippets and useful remarks. BGP is so huge that I had no other option but to write about it in several parts: Part 1 - basic eBGP and iBGP configuration Part 2 - BGP policies. Community Part 1 is dedicated to basic eBGP/iBGP configuration. We will practice with common BGP configuration procedures at first, then learn how to export routes into BGP process and prevent unnecessary route reflection by means of split-horizon over eBGP links. Next we go over iBGP configuration to spread the eBGP learned routes across the Autonomous Systems. I will explain the necessity of having a full-mesh iBGP topology and the use of the next-hop-self command for iBGP peers. It‚Äôs a perfect time to configure some BGP, right? Common BGP configuration stepsDespite what type of BGP (Internal or External) you are going to configure there are some basic steps we are about to discuss. Address planning, IGP configuration, router-id selection, autonomous-system number setting, peer groups and neighbor configuration - all of these task are common to each and every BGP configuration routine. ","date":"2015-08-24","objectID":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/:0:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP","uri":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/"},{"categories":null,"content":"IGP and addressingBGP completely relies on IGP (or static routes) when resolving nexthop address received in BGP updates from its peers. This means that prior to BGP configuration you should have IGP up and running. During this session I will refer to this base topology: A few words about the address plan and key pieces of this diagram: a BGP peering will take place between the two Autonomous Systems (hereinafter AS) 65510 and 65520. AS 65510 utilizes 10.10.0.0/16 network for local link addresses, system interfaces of its routers and customers-assigned networks, whereas AS 65520 uses 10.20.0.0/16 for the same purposes. Address plan details could be found at the Legend section of the ‚Äúbase topology‚Äù figure. We will be working with the two customers networks: R5_Customer - 10.10.55.0/24 in AS 65510 R3_Ext_Customer - 172.16.33.0/24 in AS 65520 As to Interior Gateway Protocol - I chose IS-IS, though you can choose an IGP protocol of your choice - it wont be any different. IS-IS configuration for this tutorial is super straightforward, system and network interfaces are participating in IS-IS process within the relevant ASes (except interfaces between R1-R3, R2-R4 as they are connecting different AS‚Äôs and we will run BGP there). Inter-router links are all point-to-point type. IS-IS configuration section for reference: R1 (AS 65510): *A:R1\u003econfig\u003erouter\u003eisis# info ---------------------------------------------- level-capability level-1 area-id 10.10 reference-bandwidth 100000000 level 1 wide-metrics-only exit level 2 wide-metrics-only exit interface \"system\" no shutdown exit interface \"toR2\" interface-type point-to-point no shutdown exit interface \"toR5\" interface-type point-to-point no shutdown exit no shutdown ---------------------------------------------- ## IS-IS database for 65510 consists of LSP from every router in this AS *A:R1\u003econfig\u003erouter\u003eisis# show router isis database =============================================================================== Router Base ISIS Instance 0 Database =============================================================================== LSP ID Sequence Checksum Lifetime Attributes ------------------------------------------------------------------------------- Displaying Level 1 database ------------------------------------------------------------------------------- R1.00-00 0xd 0x7740 1182 L1 R2.00-00 0xb 0x7ad1 812 L1 R5.00-00 0xc 0x8dfd 817 L1 R6.00-00 0xb 0xe8a5 842 L1 Level (1) LSP Count : 4 Displaying Level 2 database ------------------------------------------------------------------------------- Level (2) LSP Count : 0 =============================================================================== ## Check if we have a route to every router's system address within AS *A:R1# show router route-table 10.10.10.0/24 longer =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 10.10.10.1/32 Local Local 02h12m52s 0 system 0 10.10.10.2/32 Remote ISIS 02h12m24s 15 10.10.99.1 100 10.10.10.5/32 Remote ISIS 02h12m25s 15 10.10.99.5 100 10.10.10.6/32 Remote ISIS 02h12m22s 15 10.10.99.1 200 ------------------------------------------------------------------------------- No. of Routes: 4 R3 (AS 65520): A:R3\u003econfig\u003erouter\u003eisis# info ---------------------------------------------- level-capability level-1 area-id 20.20 reference-bandwidth 100000000 level 1 wide-metrics-only exit level 2 wide-metrics-only exit interface \"system\" no shutdown exit interface \"toR4\" interface-type point-to-point no shutdown exit no shutdown ---------------------------------------------- ## AS 65520 consists of two routers R3 and R4, that is why we see only two LSP here A:R3\u003econfig\u003erouter\u003eisis# show router isis database ===================================","date":"2015-08-24","objectID":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/:1:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP","uri":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/"},{"categories":null,"content":"Configuring Router ID and Autonomous System numberOnce IGP is configured its time to configure a common entity for almost every routing protocol - Router ID. For BGP there is more than one place to configure the Router ID. Here is the Router ID selection process sorted by a priority: Router ID is configured in BGP global context with the command configure router bgp router-id \u003cip-address\u003e Router ID is configured globally for a router with the command configure router router-id \u003cip-address\u003e Router ID is inherited from system IP-address. Important thing to remember is that if no router-id nor system interface is configured - BGP will not start. Since we have the system interface configured for every router we don‚Äôt need to specify the router-id explicitly. AS number can be configured either globally for a router configure router autonomous-system \u003cautonomous-system\u003e or for a specified peer group with the local-as command. We will stick to the first option and configure our AS numbers globally for a router: A:R3# configure router autonomous-system 65520 *A:R3\u003econfig\u003erouter# info #-------------------------------------------------- echo \"IP Configuration\" #-------------------------------------------------- interface \"system\" address 10.20.20.3/32 no shutdown exit interface \"toR1\" address 10.0.99.1/31 port 1/1/3 no shutdown exit interface \"toR4\" address 10.20.99.0/31 port 1/1/2 no shutdown exit autonomous-system 65520 #-------------------------------------------------- Starting eBGPCommon parameters are now configured and we can jump to eBGP peers configuration. Recall that we have two routers within AS 65510 (R1 and R2) which will have eBGP peering sessions with R3 and R3 within AS 65520 accordingly. Thus we should configure eBGP peering between the pairs R1-R3, R2-R4. Nokia BGP configuration policy requires you to configure at least one peer group to make BGP peering happen. Peer groups are logical containers for BGP peers that share common parameters. Every BGP neighbor you add should find its place in any of the BGP peer groups, in other words - peer groups are mandatory in SROS. I will guide you through basic eBGP configuration between R1 and R3. R2 and R4 configuration will be just the same. R1: ## entering BGP configuration context *A:R1# configure router bgp ## creating group eBGP *A:R1\u003econfig\u003erouter\u003ebgp$ group \"eBGP\" ## specifying AS Number for AS we would want to peer to (which is 65520) ## for eBGP peer-as should differ from local AS ## for iBGP peer-as should match local AS Number *A:R1\u003econfig\u003erouter\u003ebgp\u003egroup$ peer-as 65520 ## setting IP address of the remote router in AS 65520 *A:R1\u003econfig\u003erouter\u003ebgp\u003egroup$ neighbor 10.0.99.1 ## specify local-address for eBGP peer *A:R1\u003econfig\u003erouter\u003ebgp\u003egroup\u003eneighbor# local-address 10.0.99.0 ## Viewing resulting configuration *A:R1\u003econfig\u003erouter\u003ebgp\u003egroup\u003eneighbor$ back *A:R1\u003econfig\u003erouter\u003ebgp\u003egroup$ back *A:R1\u003econfig\u003erouter\u003ebgp$ info ---------------------------------------------- group \"eBGP\" peer-as 65520 neighbor 10.0.99.1 local-address 10.0.99.0 exit exit no shutdown ----------------------------------------------``` R3: ## all the comments are the same as for R1 *A:R3# configure router bgp *A:R3\u003econfig\u003erouter\u003ebgp$ group \"eBGP\" *A:R3\u003econfig\u003erouter\u003ebgp\u003egroup$ peer-as 65510 *A:R3\u003econfig\u003erouter\u003ebgp\u003egroup$ neighbor 10.0.99.0 A:R3\u003econfig\u003erouter\u003ebgp\u003egroup\u003eneighbor$ local-address 10.0.99.1 *A:R3\u003econfig\u003erouter\u003ebgp\u003egroup\u003eneighbor$ back *A:R3\u003econfig\u003erouter\u003ebgp\u003egroup$ back *A:R3\u003econfig\u003erouter\u003ebgp$ info ---------------------------------------------- group \"eBGP\" peer-as 65510 neighbor 10.0.99.0 local-address 10.0.99.1 exit exit no shutdown ---------------------------------------------- As simple as that, eBGP in its simplest form has been configured in 5 lines. Pay additional attention to local-address command. It is a common practice to specify a link IP address for an eBGP peer, otherwise SROS router will try to establish TCP session from its system IP address and will fail. To verify the e","date":"2015-08-24","objectID":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/:2:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP","uri":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/"},{"categories":null,"content":"iBGP next-hop-selfThe next-hop-self command forces iBGP speaker, who received an eBGP update message to substitute next-hop information with its system IP address. A:R1# configure router bgp group \"iBGP\" A:R1\u003econfig\u003erouter\u003ebgp\u003egroup# next-hop-self Get back to R5 and check whats changed: A:R5# show router bgp routes =============================================================================== BGP Router ID:10.10.10.5 AS:65510 Local AS:65510 =============================================================================== Legend - Status codes : u - used, s - suppressed, h - history, d - decayed, * - valid l - leaked Origin codes : i - IGP, e - EGP, ? - incomplete, \u003e - best, b - backup =============================================================================== BGP IPv4 Routes =============================================================================== Flag Network LocalPref MED Nexthop (Router) Path-Id Label As-Path ------------------------------------------------------------------------------- u*\u003ei 172.16.33.0/24 100 None 10.10.10.1 None - 65520 ------------------------------------------------------------------------------- Routes : 1 =============================================================================== Now it is a totally different story! R5 successfully validates the received NLRI and can use it thanks to resolvable next-hop which is R1‚Äôs system IP address 10.10.10.1. The next step is to pass this route to the RTM which is responsible for routing-table provisioning. If we take a look at R5 routing table for the recently received 172.16.33.0/24 prefix we will see that next-hop isn‚Äôt 10.10.10.1: A:R5# show router route-table 172.16.33.0/24 =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 172.16.33.0/24 Remote BGP 00h41m45s 170 10.10.99.4 0 ------------------------------------------------------------------------------- No. of Routes: 1 The reason behind this discrepancy is that the routing table should have connected networks as a next-hop and since 10.10.10.1 is far from being connected to R5 it performs an operation called recursive lookup. R5 takes next-hop value received from the iBGP update 10.10.10.1 and performs the route-table lookup: A:R5# show router route-table 10.10.10.1 =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 10.10.10.1/32 Remote ISIS 00h44m28s 15 10.10.99.4 100 ------------------------------------------------------------------------------- No. of Routes: 1 R5 knows how to reach 10.10.10.1 by means of IS-IS protocol and the next-hop for this prefix is indeed 10.10.99.4 which is a connected network: A:R5# show router route-table 10.10.99.4 =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 10.10.99.4/31 Local Local 00h44m45s 0 toR1 0 ------------------------------------------------------------------------------- No. of Routes: 1 That is why we see a different next-hop in the routing and BGP Local-RIB tables. Wrapping upTo this moment we have done a good job - we have configured the peering between two autonomous systems AS 65510 and AS 65520 and successfully exchanged the prefixes. Now, a client residing in the R3_Ext_Customer network can reac","date":"2015-08-24","objectID":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/:3:0","tags":["Nokia","SROS","BGP"],"title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP","uri":"/2015/08/alcatel-lucent-bgp-configuration-tutorial-part-1-basic-ebgp-ibgp/"},{"categories":null,"content":"Packet¬†filters (or in Cisco terminology Access Control Lists, aka ACL) are one of the most used tools in a network engineer‚Äôs tool set. Blocking telnet/ssh access, restricting specific traffic flows, implementing policy-based routing or NATing - all of these tasks use IP filter‚Äôs capabilities. In this example I‚Äôll show you how to configure a basic SSH-blocking IP filter on a Nokia (Alcatel-Lucent) SROS running¬†TiMOS-B-12.0.R8. According to the topology provided we will block SSH access to R1‚Äôs system IP. This particular task could be done in various ways, but we will configure IP filter on R2 (applied to R2‚Äôs interface¬†to_R4¬†in the incoming direction). And the rule we will configure on R2 will be as follows: If R2¬†receives a packet with a TCP destination port == 22 on interface¬†to_R4¬†it¬†must¬†drop it. Lets begin with testing ssh access before any configuration is done: A:R4# ssh 1.1.1.1 The authenticity of host '1.1.1.1 (1.1.1.1)' can't be established. RSA key fingerprint is 9c:97:50:00:b0:f7:45:6f:9e:14:9a:06:11:ba:c6:e8. Are you sure you want to continue connecting (yes/no)? yes TiMOS-B-12.0.R8 both/i386 ALCATEL SR 7750 Copyright (c) 2000-2015 Alcatel-Lucent. All rights reserved. All use subject to applicable license agreements. Built on Fri Jan 9 09:55:30 PST 2015 by builder in /rel12.0/b1/R8/panos/main admin@1.1.1.1's password: A:R1# logout Connection to 1.1.1.1 closed. Working, as expected, good. Now lets block SSH access via IP filter configuration on R2: ## Creating ip-filter *A:R2# configure filter ip-filter 100 create ## Adding description (optional) *A:R2\u003econfig\u003efilter\u003eip-filter$ description \"block ssh to 1.1.1.1/32\" ## Adding name to a filter (optional) *A:R2\u003econfig\u003efilter\u003eip-filter$ filter-name \"block_ssh_to_R1\" ## Creating filter entry *A:R2\u003econfig\u003efilter\u003eip-filter$ entry 10 create ## Specifying match statement for TCP packets, since SSH uses TCP *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry$ match protocol \"tcp\" ## In match context specifying the SSH port number *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry\u003ematch$ dst-port eq 22 ## optionally adding another match rule - Destination IP for R1 *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry\u003ematch$ dst-ip 1.1.1.1/32 ## Leaving \"match\" context and adding DROP action to this filter's entry *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry\u003ematch$ back *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry$ action drop ## Moving one step back to filter's context and adding default action FORWARD, since implicitly it is DROP. *A:R2\u003econfig\u003efilter\u003eip-filter\u003eentry$ back *A:R2\u003econfig\u003efilter\u003eip-filter$ default-action forward ## Lets see the whole filter config at once *A:R2# configure filter ip-filter 100 *A:R2\u003econfig\u003efilter\u003eip-filter# info ---------------------------------------------- filter-name \"block_ssh_to_R1\" default-action forward description \"block ssh to 1.1.1.1/32\" entry 10 create match protocol tcp dst-ip 1.1.1.1/32 dst-port eq 22 exit action drop exit ---------------------------------------------- We created a simple IP filter, but it was not applied to any interface. Lets do this: *A:R2# configure router interface \"toR4\" *A:R2\u003econfig\u003erouter\u003eif# ingress filter ip ip \"block_ssh_to_R1\" 100 ## you can refer to ip filter by its name or id *A:R2\u003econfig\u003erouter\u003eif# ingress filter ip \"block_ssh_to_R1\" ## make sure that ip filter applied correctly *A:R2\u003econfig\u003erouter\u003eif# info ---------------------------------------------- address 10.2.4.2/24 port 1/1/3 ingress filter ip 100 exit no shutdown ---------------------------------------------- Done, the filter has been applied to the appropriate interface and now should be running properly. Lets verify it by making SSH login attempt once again: A:R4# ssh 1.1.1.1 Connect to address 1.1.1.1 failed ## Our filter is working as expected You¬†use show filter¬†command to see the details of the newly created filter along with a number of packets matched by this filter: *A:R2# show filter ip 100 =============================================================================== IP Filter ====================","date":"2015-06-24","objectID":"/2015/06/alcatel-lucent-configuring-packet-ip-filters/:0:0","tags":["SROS","Nokia","IP Filter"],"title":"Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters","uri":"/2015/06/alcatel-lucent-configuring-packet-ip-filters/"},{"categories":null,"content":"The purpose of this post is to cover basic OSPFv2 configuration steps and commands for Nokia SROS routers. Intended readers are engineers with basic OSPF knowledge who want to know how to configure OSPF on Alcatel-Lucent Service Routers (7750-SR, 7705-SR, 7210-SR). All examples are valid for TiMOS-B-12.0.R8 software. Single-area OSPFBasic OSPF protocol configuration in a single area consists of the following steps: Enable OSPF globally Configure router with OSPF Router ID Configure backbone OSPF Area 0 Include interfaces in Area 0 The following network topology will be used throughout this tutorial: ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:0:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Enabling OSPFTo enable OSPF on a router simply issue configure router ospf command. This will start OSPF process #0 on a router. If you would like to run another separate OSPF process on the same router, use configure router ospf \u003cN\u003e, where N is a decimal number of the desired OSPF process. ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:1:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Router IDEach router running OSPF should have an unique 32-bit identifier, namely Router ID. This identifier will be equal to the first configured value in the following prioritized: router-id value configured globally for a router system interface IPv4 address value the last 32 bits of the chassis MAC address Configuring router-id explicitly: *A:R1# configure router router-id - no router-id - router-id \u003cip-address\u003e \u003cip-address\u003e : a.b.c.d *A:R1# configure router router-id 2.2.2.2 Use show router ospf status command to check Router ID current value and OSPF status: *A:R1# show router ospf status =============================================================================== OSPFv2 (0) Status =============================================================================== OSPF Cfg Router Id : 0.0.0.0 OSPF Oper Router Id : 1.1.1.1 # Router ID inherited from System IPv4 address OSPF Version : 2 OSPF Admin Status : Enabled # OSPF administratively enabled OSPF Oper Status : Enabled # OSPF is operating \u003coutput omitted\u003e ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:2:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Configuring Backbone AreaAre configuration is done in the OSPF configuration context with area command: A:R1\u003econfig\u003erouter\u003eospf# area - area \u003carea-id\u003e - no area \u003carea-id\u003e \u003carea-id\u003e : \u003cip-address\u003e | [0..4294967295] Area ID can be set either as decimal (area 0) or in dotted-decimal format (area 0.0.0.0). It is easier to enable OSPF and set the Area ID in a single sweep: # This command will enable OSPF process (it is disabled by default) and configure Area 0 on this router. A:R1# configure router ospf area 0 To check all the configured areas on a router use show router ospf area command: A:R1# show router ospf area ================================================================== OSPFv2 (0) all areas ================================================================== Area Id Type SPF Runs LSA Count LSA Cksum Sum ------------------------------------------------------------------ 0.0.0.0 Standard 2 1 0xcaf7 ------------------------------------------------------------------ No. of OSPF Areas: 1 ================================================================== ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:3:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Configuring OSPF interfacesOnce the backbone area is configured its time to add some interfaces to it with interface \u003cinterface_name\u003e command. # Entering to OSPF Area 0 configuration context*A:R1# configure router ospf area 0# Adding system interface to OSPF process*A:R1\u003econfig\u003erouter\u003eospf\u003earea# interface \"system\"*A:R1\u003econfig\u003erouter\u003eospf\u003earea\u003eif$ back# Adding interface toR2 and configuring it with point-to-point type# By default, ethernet interfaces use Broadcast interface type *A:R1\u003econfig\u003erouter\u003eospf\u003earea# interface \"toR2\"*A:R1\u003econfig\u003erouter\u003eospf\u003earea\u003eif$ interface-type point-to-point*A:R1\u003econfig\u003erouter\u003eospf\u003earea\u003eif$ back*A:R1\u003econfig\u003erouter\u003eospf\u003earea# back# This configuration steps effectively lead us to this OSPF configuration for router R1*A:R1\u003econfig\u003erouter\u003eospf# info---------------------------------------------- area 0.0.0.0 interface \"system\" no shutdown exit interface \"toR2\" interface-type point-to-point no shutdown exit exit no shutdown---------------------------------------------- To check that OSPF interfaces were configured properly by evaluating their status use: *A:R1# show router ospf interface =============================================================================== OSPFv2 (0) all interfaces =============================================================================== If Name Area Id Designated Rtr Bkup Desig Rtr Adm Oper ------------------------------------------------------------------------------- system 0.0.0.0 1.1.1.1 0.0.0.0 Up DR toR2 0.0.0.0 0.0.0.0 0.0.0.0 Up PToP ------------------------------------------------------------------------------- No. of OSPF Interfaces: 2 =============================================================================== Repeat the same configuration steps to include all interfaces to OSPF Area 0 for the other backbone routers R2, R3, R4 and you will end up with a fully configured OSPF Backbone Area. ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:4:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"VerificationFinally its time to check that our routers have established the neighboring relationships: A:R1# show router ospf neighbor =============================================================================== OSPFv2 (0) all neighbors =============================================================================== Interface-Name Rtr Id State Pri RetxQ TTL Area-Id ------------------------------------------------------------------------------- toR2 2.2.2.2 Full 1 0 33 0.0.0.0 toR3 3.3.3.3 Full 1 0 39 0.0.0.0 ------------------------------------------------------------------------------- No. of Neighbors: 2 =============================================================================== One of the most useful OSPF verification commands is show router ospf database. This command shows all the Links State Advertisements (LSA) and helps the engineer to troubleshoot OSPF-related issues. A:R1# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum ------------------------------------------------------------------------------- Router 0.0.0.0 1.1.1.1 1.1.1.1 873 0x8000000b 0x4dd5 Router 0.0.0.0 2.2.2.2 2.2.2.2 561 0x80000006 0x1c2b Router 0.0.0.0 3.3.3.3 3.3.3.3 879 0x80000008 0x6d94 Router 0.0.0.0 4.4.4.4 4.4.4.4 1588 0x80000005 0x94ec ------------------------------------------------------------------------------- No. of LSAs: 4 =============================================================================== You can query OSPF database for a specific LSAs by augmenting the above mentioned command: A:R1# show router ospf database - database [type {router|network|summary|asbr-summary|external|nssa|all}] [area \u003carea-id\u003e] [adv-router \u003crouter-id\u003e] [ \u003clink-state-id\u003e] [detail] \u003crouter|network|su*\u003e : keywords - specify database type \u003carea-id\u003e : ip-address - a.b.c.d area - [0..4294967295] \u003crouter-id\u003e : a.b.c.d \u003clink-state-id\u003e : a.b.c.d \u003cdetail\u003e : keyword - displays detailed information Multi-area OSPFBasic Multi-area OSPF configuration is straightforward as well. I added two more routers to the topology and introduced two areas: Area 1 and Area 2. I mistyped the port numbers for R5-R1 and R6-R2 pairs, it should be 1/1/4. Though this wont affect the course of this tutorial in anyway. We will start by configuring Area 1 on routers R5 and R1 using the same commands we used for single-area OSPF configuration. R5 configuration: # Creating Area 1 on R5 and adding \"system\" and \"toR1\" interfacesA:R5# configure router ospf area 1*A:R5\u003econfig\u003erouter\u003eospf\u003earea$ interface \"toR1\" interface-type point-to-point*A:R5\u003econfig\u003erouter\u003eospf\u003earea$ interface \"system\"# Verifying created Area 1 and its interfaces*A:R5# show router ospf interface===============================================================================OSPFv2 (0) all interfaces===============================================================================If Name Area Id Designated Rtr Bkup Desig Rtr Adm Oper-------------------------------------------------------------------------------system 0.0.0.1 5.5.5.5 0.0.0.0 Up DRtoR1 0.0.0.1 0.0.0.0 0.0.0.0 Up PToP-------------------------------------------------------------------------------No. of OSPF Interfaces: 2=============================================================================== R1 configuration # Creating Area 1 on R1 (ABR) and adding \"toR5\" interface to it.*A:R1# configure router ospf area 1*A:R1\u003econfig\u003erouter\u003eospf\u003earea$ interface \"toR5\" interface-type point-to-point# Verifying created Area 1 and its interfaces*A:R1# show router ospf interface===============================================================================OSPFv2 (0) all interfaces===============================================================================If Name Area Id Designated Rtr Bkup Desig Rtr Adm Oper------------------------------------","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:5:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Examining Multi-area LSDBSince we configured multi-area OSPF we should expect to see some new LSA in our Link State Database: On R1: A:R1# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum ------------------------------------------------------------------------------- Router 0.0.0.0 1.1.1.1 1.1.1.1 550 0x80000007 0x58cd Router 0.0.0.0 2.2.2.2 2.2.2.2 668 0x80000006 0xaf65 Router 0.0.0.0 3.3.3.3 3.3.3.3 193 0x80000006 0x7192 Router 0.0.0.0 4.4.4.4 4.4.4.4 732 0x80000007 0xa34d Summary 0.0.0.0 5.5.5.5 1.1.1.1 773 0x80000002 0x508f Summary 0.0.0.0 10.1.5.0 1.1.1.1 1091 0x80000002 0x7172 Summary 0.0.0.0 6.6.6.6 2.2.2.2 519 0x80000002 0x4d3 Summary 0.0.0.0 10.2.6.0 2.2.2.2 724 0x80000002 0x3ca1 Router 0.0.0.1 1.1.1.1 1.1.1.1 830 0x80000004 0x50ea Router 0.0.0.1 5.5.5.5 5.5.5.5 171 0x80000005 0x47bb Summary 0.0.0.1 1.1.1.1 1.1.1.1 660 0x80000002 0x1d37 Summary 0.0.0.1 2.2.2.2 1.1.1.1 183 0x80000002 0xda11 Summary 0.0.0.1 3.3.3.3 1.1.1.1 122 0x80000002 0xac3b Summary 0.0.0.1 4.4.4.4 1.1.1.1 318 0x80000002 0x6a15 Summary 0.0.0.1 6.6.6.6 1.1.1.1 207 0x80000002 0xe69 Summary 0.0.0.1 10.1.2.0 1.1.1.1 377 0x80000003 0x9055 Summary 0.0.0.1 10.1.3.0 1.1.1.1 262 0x80000003 0x855f Summary 0.0.0.1 10.2.4.0 1.1.1.1 1039 0x80000003 0x5a24 Summary 0.0.0.1 10.2.6.0 1.1.1.1 864 0x80000002 0x4637 Summary 0.0.0.1 10.3.4.0 1.1.1.1 1056 0x80000003 0x4e2f ------------------------------------------------------------------------------- No. of LSAs: 20 =============================================================================== On R5: A:R5# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum ------------------------------------------------------------------------------- Router 0.0.0.1 1.1.1.1 1.1.1.1 900 0x80000004 0x50ea Router 0.0.0.1 5.5.5.5 5.5.5.5 238 0x80000005 0x47bb Summary 0.0.0.1 1.1.1.1 1.1.1.1 728 0x80000002 0x1d37 Summary 0.0.0.1 2.2.2.2 1.1.1.1 252 0x80000002 0xda11 Summary 0.0.0.1 3.3.3.3 1.1.1.1 190 0x80000002 0xac3b Summary 0.0.0.1 4.4.4.4 1.1.1.1 387 0x80000002 0x6a15 Summary 0.0.0.1 6.6.6.6 1.1.1.1 276 0x80000002 0xe69 Summary 0.0.0.1 10.1.2.0 1.1.1.1 446 0x80000003 0x9055 Summary 0.0.0.1 10.1.3.0 1.1.1.1 329 0x80000003 0x855f Summary 0.0.0.1 10.2.4.0 1.1.1.1 1106 0x80000003 0x5a24 Summary 0.0.0.1 10.2.6.0 1.1.1.1 932 0x80000002 0x4637 Summary 0.0.0.1 10.3.4.0 1.1.1.1 1124 0x80000003 0x4e2f ------------------------------------------------------------------------------- No. of LSAs: 12 =============================================================================== Aha, R1 being an ABR lists all LSA‚Äôs for both Area 0 and Area 1. Moreover, R1 lists Type 3 Summary LSA from Area 0 to Area 1 and vice versa, from Area 1 to Area 0. R5 has only Area 1 LSAs, since R5 ‚Äúlives‚Äù exactly in a single area - Area 1. OSPF routes propagationTo ensure that OSPF routers exchanged OSPF routes lets check R5 and R1 routing tables: On R1 (ABR): # Checking routes that have been received via OSPF*A:R1# show router route-table protocol ospf===============================================================================Route Table (Router: Base)===============================================================================Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric-------------------------------------------------------------------------------2.2.2.2/32 Remote OSPF 03h01m17s 10 10.1.2.2 1003.3.3.3/32 Remote OSPF 03h01m13s 10 10.1.3.3 1004.4.4.4/32 Remote OSPF 03h01m10s 10 10.1.2.2 2005.5.5.5/32 Remote OSPF 00h39m07s 10 10.1.5.5 1006.6.6.6/32 Remote OSPF 00h19m38s 10 10.1.2.2 ","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:6:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"Verifying redistributed routes propagationNow, our R5 router is now configured as an ASBR and the local routes should get exported into OSPF process. These changes allow other routers in OSPF domain to receive these routes by means of Type 4 ASBR Summary LSA and Type 5 AS External LSA: R5: *A:R5# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum ------------------------------------------------------------------------------- Router 0.0.0.1 1.1.1.1 1.1.1.1 1033 0x80000004 0x50ea Router 0.0.0.1 5.5.5.5 5.5.5.5 1072 0x80000006 0x4bb4 Summary 0.0.0.1 1.1.1.1 1.1.1.1 706 0x80000004 0x1939 Summary 0.0.0.1 2.2.2.2 1.1.1.1 2226 0x80000002 0xda11 Summary 0.0.0.1 3.3.3.3 1.1.1.1 107 0x80000003 0xaa3c Summary 0.0.0.1 4.4.4.4 1.1.1.1 929 0x80000003 0x6816 Summary 0.0.0.1 6.6.6.6 1.1.1.1 1879 0x80000002 0xe69 Summary 0.0.0.1 10.1.2.0 1.1.1.1 366 0x80000003 0x9055 Summary 0.0.0.1 10.1.3.0 1.1.1.1 472 0x80000003 0x855f Summary 0.0.0.1 10.2.4.0 1.1.1.1 737 0x80000003 0x5a24 Summary 0.0.0.1 10.2.6.0 1.1.1.1 939 0x80000003 0x4438 Summary 0.0.0.1 10.3.4.0 1.1.1.1 442 0x80000003 0x4e2f Summary 0.0.0.1 192.168.3.0 1.1.1.1 355 0x80000003 0x5039 AS Ext n/a 192.168.5.1 5.5.5.5 506 0x80000001 0x63ea AS Ext n/a 192.168.5.2 5.5.5.5 506 0x80000001 0x59f3 AS Ext n/a 192.168.5.3 5.5.5.5 506 0x80000001 0x4ffc ------------------------------------------------------------------------------- No. of LSAs: 16 =============================================================================== R1: A:R1# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum ------------------------------------------------------------------------------- Router 0.0.0.0 1.1.1.1 1.1.1.1 655 0x80000006 0x5acc Router 0.0.0.0 2.2.2.2 2.2.2.2 988 0x80000006 0xaf65 Router 0.0.0.0 3.3.3.3 3.3.3.3 1122 0x80000009 0x1a6b Router 0.0.0.0 4.4.4.4 4.4.4.4 1133 0x80000006 0xa54c Summary 0.0.0.0 5.5.5.5 1.1.1.1 562 0x80000003 0x4e90 Summary 0.0.0.0 10.1.5.0 1.1.1.1 472 0x80000003 0x6f73 Summary 0.0.0.0 6.6.6.6 2.2.2.2 189 0x80000003 0x2d4 Summary 0.0.0.0 10.2.6.0 2.2.2.2 166 0x80000003 0x3aa2 AS Summ 0.0.0.0 5.5.5.5 1.1.1.1 1407 0x80000001 0x449b Router 0.0.0.1 1.1.1.1 1.1.1.1 1368 0x80000004 0x50ea Router 0.0.0.1 5.5.5.5 5.5.5.5 1409 0x80000006 0x4bb4 Summary 0.0.0.1 1.1.1.1 1.1.1.1 1042 0x80000004 0x1939 Summary 0.0.0.1 2.2.2.2 1.1.1.1 321 0x80000003 0xd812 Summary 0.0.0.1 3.3.3.3 1.1.1.1 443 0x80000003 0xaa3c Summary 0.0.0.1 4.4.4.4 1.1.1.1 1265 0x80000003 0x6816 Summary 0.0.0.1 6.6.6.6 1.1.1.1 266 0x80000003 0xc6a Summary 0.0.0.1 10.1.2.0 1.1.1.1 703 0x80000003 0x9055 Summary 0.0.0.1 10.1.3.0 1.1.1.1 809 0x80000003 0x855f Summary 0.0.0.1 10.2.4.0 1.1.1.1 1074 0x80000003 0x5a24 Summary 0.0.0.1 10.2.6.0 1.1.1.1 1275 0x80000003 0x4438 Summary 0.0.0.1 10.3.4.0 1.1.1.1 779 0x80000003 0x4e2f Summary 0.0.0.1 192.168.3.0 1.1.1.1 691 0x80000003 0x5039 AS Ext n/a 192.168.5.1 5.5.5.5 843 0x80000001 0x63ea AS Ext n/a 192.168.5.2 5.5.5.5 843 0x80000001 0x59f3 AS Ext n/a 192.168.5.3 5.5.5.5 843 0x80000001 0x4ffc ------------------------------------------------------------------------------- No. of LSAs: 25 =============================================================================== R3: A:R3# show router ospf database =============================================================================== OSPFv2 (0) Link State Database (Type : All) =============================================================================== Type Area Id Link State Id Adv Rtr Id Age Sequence Cksum -------------------------------------------------------------------------------","date":"2015-06-22","objectID":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/:7:0","tags":["SROS","Nokia","OSPF"],"title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","uri":"/2015/06/alcatel-lucent-ospf-configuration-tutorial/"},{"categories":null,"content":"When it comes to basic OSPF troubleshooting the first-aid kit is Neighbor states and things, that should match to form an adjacency. And on one early morning while refreshing my memory on OSPF neighbor states I accidentally ran into quite interesting problem. But before we start, answer the short question: Will adjacency be formed between directly connected via Gig. Ethernet interfaces routers R1 and R2 if\rR1‚Äôs OSPF interface type configured as point-to-point\r\rR2‚Äôs OSPF interface type configured as broadcast\r\rTime‚Äôs up. The answer is ‚Äì yes and no. Wanna know why? Jump in, I have to show you something.\rTake a look at this topology which consists of two directly connected Cisco routers. At first, topology seems as simple as a first figure in a CCNA book. But take a closer look at this OSPF interfaces, their types are different. Hm, definitely not a case you would find covered in an everyday OSPF configuration guide. I will not go into details about what are all the differences between various OSPF interface types, you already know it, or can read about 1-click away. Though I should remind you the key difference between point-to-point and broadcast interfaces behavior which is the necessity to elect DR/BDR routers for the latter. Also it is worth mention that it is best-practice nowadays to set ‚Äúbroadcast-by-nature‚Äù Ethernet interfaces to operate in a point-to-point fashion. Why would one do it? To reduce convergence time. Ethernet interface once configured as point-to-point won‚Äôt go into DR/BDR election which effectively means that neighbor relationships will reach FULL state 40 seconds faster (40s is the default wait interval for interface to elect DR/BDR routers_)_. Therefore this particular case with different interface types isn‚Äôt some artificial Lab exercise, you can easily meet this interface mix in the real world. Let‚Äôs say you simply forgot to configure another interface with p2p type, or your neighbor is under administration of the 3rd party who knows nothing about best practices and leave you with default broadcast behavior. Ok, different link types mean different networks with different behavior, rules and so forth, there is every indication that adjacency should not form. But it is not that simple. Let‚Äôs Lab! Lab timeI booted up two Cisco routers running IOS 15.2(4)S simultaneously and first thing checked R1‚Äôs OSPF neighbors, expecting to see zero active neighbors: R1#show ip ospf neighbor Neighbor ID Pri State Dead Time Address Interface 2.2.2.2 0 FULL/ - 00:00:36 10.1.2.2 GigabitEthernet1/0 Look at this, R1 has a neighbor in FULL state, which means that it has recognized R2 as a valid neighbor and successfully accomplished exchange of Link State Updates! As a precaution lets check OSPF interfaces parameters on both routers to confirm that they indeed operate in different types: On R1: R1# show ip ospf interface gi1/0 GigabitEthernet1/0 is up, line protocol is up Internet Address 10.1.2.1/24, Area 0, Attached via Interface Enable Process ID 1, Router ID 1.1.1.1, Network Type POINT_TO_POINT, Cost: 1 Topology-MTID Cost Disabled Shutdown Topology Name 0 1 no no Base Enabled by interface config, including secondary ip addresses Transmit Delay is 1 sec, State POINT_TO_POINT Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5 oob-resync timeout 40 Hello due in 00:00:00 Supports Link-local Signaling (LLS) Cisco NSF helper support enabled IETF NSF helper support enabled Index 1/1, flood queue length 0 Next 0x0(0)/0x0(0) Last flood scan length is 1, maximum is 1 Last flood scan time is 0 msec, maximum is 0 msec Neighbor Count is 1, Adjacent neighbor count is 1 Adjacent with neighbor 2.2.2.2 Suppress hello for 0 neighbor(s) On R2: R2#show ip ospf interface gi1/0 GigabitEthernet1/0 is up, line protocol is up Internet Address 10.1.2.2/24, Area 0, Attached via Interface Enable Process ID 1, Router ID 2.2.2.2, Network Type BROADCAST, Cost: 1 Topology-MTID Cost Disabled Shutdown Topology Name 0 1 no no Base Enable","date":"2015-06-14","objectID":"/2015/06/ospf-neighbors-on-a-point-to-broadcast-network/:0:0","tags":["OSPF"],"title":"OSPF. Neighbors on a \"point-to-broadcast\" network","uri":"/2015/06/ospf-neighbors-on-a-point-to-broadcast-network/"},{"categories":null,"content":"Major network vendors (except Cisco) default to the following modes of Label Distribution Protocol (LDP) operation (as per RFC 5036 LDP Specification): Label Distribution (Advertisement): Downstream Unsolicited (section 2.6.3) Label Control: Ordered (section 2.6.1) Label Retention: Liberal (section 2.6.2) This topic focuses on Ordered Label Distribution Control procedure to help you better understand when LSR actually assigns labels and initiates transmission of a label mapping. Both RFC 3031 Multiprotocol Label Switching Architecture and RFC 5036 LDP Specification give definition for Ordered Label Distribution Control mode: RFC 3031: In Ordered LSP Control, an LSR only binds a label to a particular FEC if it is the egress LSR for that FEC, or if it has already received a label binding for that FEC from its next hop for that FEC. RFC 5036: When using LSP Ordered Control, an LSR may initiate the transmission of a label mapping only for a FEC for which it has a label mapping for the FEC next hop, or for which the LSR is the egress. For each FEC for which the LSR is not the egress and no mapping exists, the LSR MUST wait until a label from a downstream LSR is received before mapping the FEC and passing corresponding labels to upstream LSRs. Lets break this definition into distinct sentences: In case LSR is the egress router for a FEC X then LSR maps label to FEC X and transmits this label mapping to its LDP peers; In case LSR is not the egress router for a FEC X then in order to map a label for this FEC and to propagate this label mapping to its peers, it has to wait until it receives a label mapping for this particular FEC X from its downstream LDP peer. In this manner the entire LSP is established before MPLS begins to map data onto the LSP, preventing early data mapping from occurring on the first LSR in the path. Let‚Äôs take a look at this simple topology to illustrate these steps. Routers R1-R2-R3 have OSPF enabled on their interfaces and announce their loopback (or system, as long as we‚Äôre using with Alcatel-Lucent routers in this example) IP addresses. So every router has a route to other router‚Äôs loopback address. A:R1# show router route-table =============================================================================== Route Table (Router: Base) =============================================================================== Dest Prefix[Flags] Type Proto Age Pref Next Hop[Interface Name] Metric ------------------------------------------------------------------------------- 10.1.2.0/24 Local Local 00h08m36s 0 toR2 0 10.2.3.0/24 Remote OSPF 00h04m20s 10 10.1.2.2 200 10.10.10.1/32 Local Local 00h08m36s 0 system 0 10.10.10.2/32 Remote OSPF 00h08m05s 10 10.1.2.2 100 10.10.10.3/32 Remote OSPF 00h01m22s 10 10.1.2.2 200 ------------------------------------------------------------------------------- No. of Routes: 5 Flags: n = Number of times nexthop is repeated B = BGP backup route available L = LFA nexthop available S = Sticky ECMP requested =============================================================================== To investigate label mappings creation and propagation processes lets dive into step-by-step LDP operation for the particular Forward Equivalence Class (FEC) 10.10.10.1/32 ‚Äì which is the loopback address of the router R1. ¬†Everything starts with R1 which is an Egress router for the FEC 10.10.10.1/32. According to abovementioned Ordered Distribution Control mode definition R1 (again, as an Egress router) has a right to create a binding for the FEC 10.10.10.1/32. Such bindings, created for the FECs which are local to a router often called ‚ÄúLocal bindings‚Äù. Let‚Äôs check that R1 actually has this local label mapping in its Label Information Base: A:R1# show router ldp bindings =============================================================================== LDP Bindings (IPv4 LSR ID 10.10.10.1:0) (IPv6 LSR ID ::[0]) =============================================================================== LDP IPv4 Prefix Bindings","date":"2015-06-02","objectID":"/2015/06/ldp-ordered-label-distribution-control-explained/:0:0","tags":["LDP","MPLS"],"title":"LDP. Ordered Label Distribution Control explained","uri":"/2015/06/ldp-ordered-label-distribution-control-explained/"},{"categories":null,"content":"Downstream or not?You may have already noticed that the key in making a decision to create a label mapping for a FEC is if the label mapping came from a downstream router? But how does a router decide if its peer is downstream router or upstream? Let‚Äôs think about it‚Ä¶ Rewind to Timestamp B. R2 receives a label mapping message from R1 and needs to decide if R1 is a downstream router regarding FEC 10.10.10.1/32? It looks into Forwarding Information Base for this prefix and sees the next-hop address for it is 10.1.2.1: *A:R2# show router fib 1 10.10.10.1/32 =============================================================================== FIB Display =============================================================================== Prefix [Flags] Protocol NextHop ------------------------------------------------------------------------------- 10.10.10.1/32 OSPF 10.1.2.1 (toR1) ------------------------------------------------------------------------------- Total Entries : 1 ------------------------------------------------------------------------------- So what? We have a next-hop, but how do we know if IP address 10.1.2.1 belongs to R1? To asnswer this question we need to open RFC 5036 LDP Specification once again. Navigate to section 2.7 LDP Identifiers and Next Hop Addresses: To enable LSRs to map between a peer LDP Identifier and the peer‚Äôs addresses, LSRs advertise their addresses using LDP Address and Withdraw Address messages. Well, it seems LDP peers share LDP Address messages where they communicate all of their configured IP addresses, let‚Äôs see under the hood: That is the answer. LDP speaker should tell its peers about the addresses it has configured. This info communicated via Address Messages and helps remote peers to map LDP indentifier to an IP address. With this information provided R2 now can tell for sure that next-hop address it has in its Forwarding Information Base belongs to R1. And that is how R2 can tell that R1 is a downstream router ‚Äì by matching its next-hop address from FIB with an IP addresses provided in an Address Message from R1. And this is all for this time. If you have any questions regarding this topic ‚Äì do not hesitate, I will gladly address them. ","date":"2015-06-02","objectID":"/2015/06/ldp-ordered-label-distribution-control-explained/:0:1","tags":["LDP","MPLS"],"title":"LDP. Ordered Label Distribution Control explained","uri":"/2015/06/ldp-ordered-label-distribution-control-explained/"}]