{"config":{"lang":["en"],"separator":"[\\s\\-\\_]","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"about/","title":"About me","text":"<p>Hey, let's meet? I am Roman, an engineer with a passion for network automation and this is my technical blog.</p>"},{"location":"about/#projects","title":"Projects","text":"<p>Hey, this will be filled out soon.</p>"},{"location":"about/#talks-demos","title":"Talks &amp; Demos","text":""},{"location":"about/#containerlab-nlnog-2021","title":"Containerlab @ NLNOG 2021","text":"<p>The very first in-person networking after a long COVID dry-out. In this talk I introduce containerlab to the audience where 30 minutes is all it takes to get the gist of the project and how it can fit in your environment.</p> <p>{{&lt; youtube n81Tc1g4W5U &gt;}}</p>"},{"location":"about/#contact","title":"Contact","text":"<p>You can reach me on twitter - @ntdvps</p> <p>Or connect on LinkedIn - rdodin.</p>"},{"location":"subscribe/","title":"Subscribe","text":"<p>If you like the topics I touch upon in this blog and you want to get notified when a new post comes out - consider subscribing via one of the following mediums.</p>"},{"location":"subscribe/#social-media","title":"Social media","text":"<p>I am an active Twitter user and besides tweeting about network automation, announce new blog posts as they get out. You can find me there as @ntdvps.</p> <p>If twitter is not particularly your game, we may get connected on  where my handle is in/rdodin. I am announcing my posts there too.</p>"},{"location":"subscribe/#rss","title":"RSS","text":"<p> What is dead may never die. But, hey, if you still ride the RSS wave, I have you covered. This blog publishes two feeds:</p> <ul> <li><code>https://netdevops.me/feed_rss_created.xml</code> - feed is updated whenever a new post is created.</li> <li><code>https://netdevops.me/feed_rss_updated.xml</code> - feed is updated whenever a new post is created or updated.</li> </ul> <p>Depending how thoroughly you want to monitor my blog you can choose between those two.</p>"},{"location":"subscribe/#email","title":"Email","text":"<p>What is more reliable than email? Probably, nothing. You can get notifications in your mailbox by piggy-backing on the RSS feed using one of those RSS-to-Email services.</p> <p>I personally use a free blogtrottr service, but if it doesn't suit you, there are other options like feedrabbit and IFTT.</p> <p>In blogtrottr all you need to do is to enter <code>https://netdevops.me</code> in the site input and type in your email. Then select which feed you want to receive (see rss section for an explanation).</p>"},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#alpine-linux","title":"alpine linux","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> </ul>"},{"location":"tags/#ansible","title":"ansible","text":"<ul> <li>SR Linux, JSON-RPC and Ansible</li> </ul>"},{"location":"tags/#apt","title":"apt","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#arista","title":"arista","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#aws","title":"aws","text":"<ul> <li>How to install python3 in Amazon Linux AMI</li> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Prepping up for and passing the AWS Certified Solution Architect - Associate</li> <li>Uploading multiple files to AWS S3 in parallel</li> </ul>"},{"location":"tags/#aws-lambda","title":"aws lambda","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> </ul>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Formatting bash</li> </ul>"},{"location":"tags/#bgp","title":"bgp","text":"<ul> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#blogging","title":"blogging","text":"<ul> <li>Adding border to the logos</li> <li>Upscaling images</li> </ul>"},{"location":"tags/#bootstrap","title":"bootstrap","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#boto3","title":"boto3","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> </ul>"},{"location":"tags/#ceos","title":"ceos","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#certification","title":"certification","text":"<ul> <li>Prepping up for and passing the AWS Certified Solution Architect - Associate</li> <li>Prepping up for and passing the Certified Openstack Administrator Exam</li> </ul>"},{"location":"tags/#cisco","title":"cisco","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#cloudflare","title":"cloudflare","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> </ul>"},{"location":"tags/#containerlab","title":"containerlab","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>Test coverage for Go integration tests</li> </ul>"},{"location":"tags/#crpd","title":"crpd","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#dknog","title":"dknog","text":"<ul> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Flask application in a production-ready container</li> <li>Yang Explorer in a docker container based on Alpine</li> <li>NETCONF console in a docker container</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Using scrapligo with kubectl exec</li> <li>OpenStack Client Container Image</li> </ul>"},{"location":"tags/#documentation","title":"documentation","text":"<ul> <li>Projectdocs</li> </ul>"},{"location":"tags/#elk","title":"elk","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"tags/#eve-ng","title":"eve-ng","text":"<ul> <li>Using Wireshark remote capture with EVE-NG</li> </ul>"},{"location":"tags/#flask","title":"flask","text":"<ul> <li>Flask application in a production-ready container</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#frontend","title":"frontend","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> </ul>"},{"location":"tags/#frr","title":"frr","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#ftp","title":"ftp","text":"<ul> <li>Saturating the network with FTP</li> </ul>"},{"location":"tags/#fwd","title":"fwd","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#gcp","title":"gcp","text":"<ul> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#gcp-function","title":"gcp function","text":"<ul> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#gemfury","title":"gemfury","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>How to count lines of code in a Git repo?</li> <li>Remove binaries and big files from Git repo</li> </ul>"},{"location":"tags/#gitlab","title":"gitlab","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> </ul>"},{"location":"tags/#gnmi","title":"gnmi","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMI Map</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>Easily exposing your local resources with ngrok and fwd</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> <li>Decoding gNMI with Wireshark</li> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#gnmic","title":"gnmic","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> <li>Decoding gNMI with Wireshark</li> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#go","title":"go","text":"<ul> <li>How to make VS Code Go extension to work in your cloud folder on different platforms?</li> <li>gNMIc - gNMI CLI client and collector</li> <li>Using scrapligo with kubectl exec</li> <li>Test coverage for Go integration tests</li> <li>Refreshing Go package index for your package</li> </ul>"},{"location":"tags/#goreleaser","title":"goreleaser","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#grpc","title":"grpc","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#guestfish","title":"guestfish","text":"<ul> <li>Using guestfish container image</li> </ul>"},{"location":"tags/#highlightjs","title":"highlightjs","text":"<ul> <li>How to add YAML highlight in Highlight.js?</li> </ul>"},{"location":"tags/#hugo","title":"hugo","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> <li>How to add YAML highlight in Highlight.js?</li> </ul>"},{"location":"tags/#javascript","title":"javascript","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> </ul>"},{"location":"tags/#json-rpc","title":"json-rpc","text":"<ul> <li>SR Linux, JSON-RPC and Ansible</li> </ul>"},{"location":"tags/#juniper","title":"juniper","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>Using scrapligo with kubectl exec</li> </ul>"},{"location":"tags/#kvm","title":"kvm","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#l3vpn","title":"l3vpn","text":"<ul> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> </ul>"},{"location":"tags/#lacp","title":"lacp","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#ldp","title":"ldp","text":"<ul> <li>LDP. Ordered Label Distribution Control explained</li> </ul>"},{"location":"tags/#libvirt","title":"libvirt","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#logging","title":"logging","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"tags/#m3u8","title":"m3u8","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#mkdocs-material","title":"mkdocs-material","text":"<ul> <li>Projectdocs</li> </ul>"},{"location":"tags/#mpls","title":"mpls","text":"<ul> <li>LDP. Ordered Label Distribution Control explained</li> </ul>"},{"location":"tags/#nanog","title":"nanog","text":"<ul> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#netconf","title":"netconf","text":"<ul> <li>NETCONF console in a docker container</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Easily exposing your local resources with ngrok and fwd</li> <li>NETCONF subtree filtering by example</li> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#netrel","title":"netrel","text":"<ul> <li>Decoding gNMI with Wireshark</li> <li>DIY YANG Browser</li> </ul>"},{"location":"tags/#nginx","title":"nginx","text":"<ul> <li>Flask application in a production-ready container</li> </ul>"},{"location":"tags/#ngrok","title":"ngrok","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#nokia","title":"nokia","text":"<ul> <li>SR OS Rootifier or how to flatten 7750 SR config</li> <li>Nokia YANG tree and Path Browser</li> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> <li>Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#openconfig","title":"openconfig","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMI Map</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> </ul>"},{"location":"tags/#openstack","title":"openstack","text":"<ul> <li>Prepping up for and passing the Certified Openstack Administrator Exam</li> <li>OpenStack Client Container Image</li> </ul>"},{"location":"tags/#ospf","title":"ospf","text":"<ul> <li>OSPF. Neighbors on a \"point-to-broadcast\" network</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> </ul>"},{"location":"tags/#ovs","title":"ovs","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#paramiko","title":"paramiko","text":"<ul> <li>Waiting for SSH service to be ready with Paramiko</li> </ul>"},{"location":"tags/#pygments","title":"pygments","text":"<ul> <li>Creating a syntax highlighter for SR Linux CLI snippets</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>How to install python3 in Amazon Linux AMI</li> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Waiting for SSH service to be ready with Paramiko</li> <li>Creating Google Cloud Platform Function with Python and Serverless</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#qemu","title":"qemu","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#scrapli","title":"scrapli","text":"<ul> <li>Using scrapligo with kubectl exec</li> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#serverless","title":"serverless","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#sonic","title":"sonic","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#sr-os","title":"sr os","text":"<ul> <li>SR OS Rootifier or how to flatten 7750 SR config</li> <li>Nokia YANG tree and Path Browser</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> <li>Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> </ul>"},{"location":"tags/#srlinux","title":"srlinux","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>SR Linux, JSON-RPC and Ansible</li> <li>SR Linux logging with ELK</li> <li>Creating a syntax highlighter for SR Linux CLI snippets</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#tc","title":"tc","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#testing","title":"testing","text":"<ul> <li>Test coverage for Go integration tests</li> </ul>"},{"location":"tags/#textfsm","title":"textfsm","text":"<ul> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#ubuntu","title":"ubuntu","text":"<ul> <li>Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial</li> <li>How to patch Ubuntu 20.04 Focal Fossa with UKSM?</li> </ul>"},{"location":"tags/#uksm","title":"uksm","text":"<ul> <li>How to patch Ubuntu 20.04 Focal Fossa with UKSM?</li> </ul>"},{"location":"tags/#uwsgi","title":"uwsgi","text":"<ul> <li>Flask application in a production-ready container</li> </ul>"},{"location":"tags/#video","title":"video","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#virsh","title":"virsh","text":"<ul> <li>Changing Libvirt bridge attachment in a running domain aka on-the-fly</li> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#vpls","title":"vpls","text":"<ul> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#vprn","title":"vprn","text":"<ul> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> </ul>"},{"location":"tags/#vrnetlab","title":"vrnetlab","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#vs-code","title":"vs code","text":"<ul> <li>How to make VS Code Go extension to work in your cloud folder on different platforms?</li> </ul>"},{"location":"tags/#vsftpd","title":"vsftpd","text":"<ul> <li>Saturating the network with FTP</li> </ul>"},{"location":"tags/#wireshark","title":"wireshark","text":"<ul> <li>Using Wireshark remote capture with EVE-NG</li> </ul>"},{"location":"tags/#xrdp","title":"xrdp","text":"<ul> <li>Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial</li> </ul>"},{"location":"tags/#yang","title":"yang","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> <li>Arista EOS gNMI Tutorial</li> <li>gNMIc got better with YANG-completions</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Nokia YANG tree and Path Browser</li> <li>DIY YANG Browser</li> </ul>"},{"location":"tags/#yang-explorer","title":"yang explorer","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> </ul>"},{"location":"tags/#youtube-dl","title":"youtube-dl","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#yum","title":"yum","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"2017/how-to-install-python3-in-amazon-linux-ami/","title":"How to install python3 in Amazon Linux AMI","text":"<p>While Amazon Linux AMI has <code>yum</code> as a package manager, it is not that all compatible with any RHEL or CentOS distributive. A lot of changes that AWS team brought into this image made it a separate distro, so no eyebrows should be given when battle-tested procedure to install python3 will fail on Amazon Linux. (Yeah, python3 does not come included yet in Amazon Linux)</p> <p>Fortunately it is very easy to fetch (while not the latest release) python3:</p> <pre><code># list available packages that have python3 in their name\nyum list | grep python3\n\n# install python3+pip, plus optionally packages to your taste\nsudo yum install python35 python35-devel python35-pip python35-setuptools python35-virtualenv\n\n# update pip3. optionally set a symbolic link to pip3\nsudo pip-3.5 install --upgrade pip\n</code></pre> <p>And that is it!</p>","tags":["aws","python"]},{"location":"2017/how-to-count-lines-of-code-in-a-git-repo/","title":"How to count lines of code in a Git repo?","text":"<p>Nothing bad in knowing how many lines of code or text out there in your repo. You don't even need your VCS to convey this analytics. All you need is <code>git</code>, <code>grep</code> and <code>wc</code>.</p> <pre><code># count lines in .py and .robot files in /nuage-cats dir of the repo\n$ git ls-files nuage-cats/ | grep -E \".*(py|robot)\" | xargs wc -l\n       0 nuage-cats/robot_lib/__init__.py\n     817 nuage-cats/robot_lib/lib/NuageQoS.py\n     409 nuage-cats/robot_lib/lib/NuageVCIN.py\n    1841 nuage-cats/robot_lib/lib/NuageVNS.py\n    2964 nuage-cats/robot_lib/lib/NuageVSD.py\n    # OMITTED\n      26 nuage-cats/test_suites/0910_fail_bridges_kvm_vms/0910_fail_bridges_kvm_vms.robot\n   13636 total\n</code></pre>","tags":["git"]},{"location":"2017/how-to-make-vs-code-go-extension-to-work-in-your-cloud-folder-on-different-platforms/","title":"How to make VS Code Go extension to work in your cloud folder on different platforms?","text":"<p>I started to play with Go aka Golang. Yeah, leaving the comfort zone, all that buzz. And for quite some time I've been engaged with VS Code whenever/wherever I did dev activities.</p> <p>VS Code has a solid Go support via its official extension:</p> <p>Info</p> <p>This extension adds rich language support for the Go language to VS Code, including:</p> <ul> <li>Completion Lists (using gocode)</li> <li>Signature Help (using gogetdoc or godef+godoc)</li> <li>Quick Info (using gogetdoc or godef+godoc)</li> <li>Goto Definition (using gogetdoc or godef+godoc)</li> <li>Find References (using guru)</li> <li>File outline (using go-outline)</li> <li>Workspace symbol search (using go-symbols)</li> <li>Rename (using gorename)</li> <li>Build-on-save (using go build and go test)</li> <li>Lint-on-save (using golint or gometalinter)</li> <li>Format (using goreturns or goimports or gofmt)</li> <li>Generate unit tests skeleton (using gotests)</li> <li>Add Imports (using gopkgs)</li> <li>Add/Remove Tags on struct fields (using gomodifytags)</li> <li>Semantic/Syntactic error reporting as you type (using gotype-live)</li> </ul> <p>Mark that gotools in the brackets, these ones are powering all that extra functionality and got installed into your <code>GOPATH</code> once you install them via VS Code.</p> <p>And here you might face an issue if you want to use Go + VS Code both on Mac and Linux using the Dropbox folder (or any other syncing service). The issue is that binaries for Mac and Linux will overwrite themselves once you decide to install the extension on your second platform. Indeed, by default VS Code will fetch the source code of the tools and build them, placing binaries in the <code>$GOPATH/bin</code>.</p> <p>Lucky we, the Go Extension developers have a special setting to put extension dependencies to a different <code>$GOPATH</code>:</p>","tags":["go","vs code"]},{"location":"2017/how-to-make-vs-code-go-extension-to-work-in-your-cloud-folder-on-different-platforms/#tools-this-extension-depends-on","title":"Tools this extension depends on","text":"<p>This extension uses a host of Go tools to provide the various rich features. These tools are installed in your <code>GOPATH</code> by default. If you wish to have the extension use a separate <code>GOPATH</code> for its tools, provide the desired location in the setting <code>go.toolsGopath</code>. Read more about this and the tools at Go tools that the Go extension depends on.</p> <p>And thats it, open your <code>settings.json</code>, put something like</p> <pre><code>\"go.toolsGopath\": \"~/.gotools\"\n</code></pre> <p>and thats it, next time you hit \"install\" of Go Extension dependencies, they will be stored outside your Dropbox-powered <code>$GOPATH</code> and won't interfere with each other.</p>","tags":["go","vs code"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/","title":"Setting up a Hugo blog with GitLab and CloudFlare","text":"<p>Hugo gets a lot of attention these days, it is basically snapping at the Jekyll' heels which is still the king of the hill! I don't know if Hugo' popularity coupled with the fastest static-site-generator statement, but for me \"speed\" is not the issue at all. A personal blog normally has few hundreds posts, not even close to thousands to be worried about slowness.</p> <p>Then if it is not for speed then why did I choose Hugo? Because it became a solid product with a crowded community and all the common features available. (To be honest I also got an illusion that one day I might start sharpen my Go skills through Hugo as well).</p> <p>As you already noticed, this blog is powered by Hugo, is running on GitLab pages, with SSL certificate from CloudFlare and costs me $0. And I would like to write down the key pieces that'll probably be of help on your path to a zero-cost personal blog/cv/landing/etc.</p> <p>The key ingredients of a modern zero-cost blog powered by a Static Site Generator are:</p> <ol> <li>Version Control System -- Git</li> <li>Web-based version control repository -- GitLab/Github</li> <li>Web server -- GitLab Pages/GitHub Pages</li> <li>Static Site Generator Engine -- Hugo/Jekyll/Hexo/Pelican/many-others</li> <li>SSL Certificate provider -- Cloudflare/LetsEncrypt (optional)</li> <li>Custom domain linked to a free one from GitLab/Github (optional)</li> </ol> <p>While Git and GitLab/GitHub are of obvious choice we better discuss the <code>Hugo + GitLab CI + GitLab pages + Cloudflare</code> mix that I chose to enable this blog.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/#hugo","title":"Hugo","text":"<p>Hugo installation is ridiculously easy, thanks to Golang that powers it. Download a single <code>hugo</code> binary from the official repo and thats it. No need for virtualenvs, npms and alike, a single binary is all you need.</p> <p>Once the binary is in your <code>$PATH</code> create a site skeleton with</p> <pre><code>hugo new site &lt;yourBlogName&gt;\n</code></pre> <p>For details refer to the QuickStart guide to get a locally running site under 5 minutes.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/#theme","title":"Theme","text":"<p>Hugo community produced over 100+ themes for different needs. As to me, most of them are ugly, or as minimalistic as the blogspot. Probably the hardest thing in the whole process is to find a theme that suits you. This blog uses a Tranquilpeak theme.</p> <p>To onboard a chosen theme follow the quickstart guide' step 3.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/#gitlab","title":"GitLab","text":"<p>Now when you have an engine and a theme coupled together its GitLab' part to present your content to the world. GitLab has the GitLab Pages service created just for what we need and highligted by being:</p> <ul> <li>free</li> <li>SSG-agnostic</li> <li>SSL &amp; custom domains ready</li> </ul> <p>It has more whistles than Github pages and is completely free without any limitations.</p> <p>There is a comprehensive guide about onboarding a static generated site within GitLab, I will boil it down to a few steps (you can always peer into my repo for a complete code and settings):</p> <ol> <li> <p>Create a <code>.gitlab-ci.yml</code> file at the root of your repo with the <code>pages</code> job</p> <pre><code>pages:\n# this image is slimmer than official one; based on Alpine\nimage: fundor333/hugo \nscript:\n    - hugo\n# send all files from public directory to the CI server\nartifacts:\n    paths:\n      - public\nonly:\n    - master  # this job will affect only the 'master' branch\n</code></pre> <p>Note, that you can put the exact same content in your file, there are no custom parts here.</p> </li> <li> <p>Find your <code>baseurl</code> and put it into <code>config.toml</code> of your Hugo site. A base URL depends on how did you create a GitLab project. Is it a project placed under you personal account or a under a group? All the options are outlined in the official docs.</p> </li> <li> <p>Make a commit with the contents of your blog and push the changes to the master branch <code>git push origin master</code>. That will automatically trigger the <code>pages</code> job to build your site and start serving it from <code>https://yournamespace.gitlab.io</code></p> </li> <li> <p>At this point you are good to go, you have TLS certificate provided by Gitlab for <code>*.gitlab.io</code> namespace, your posts will be automatically generated once you push to <code>master</code> branch and your texts are in VSC. WIN!</p> </li> </ol> <p>You can stop here and start generate the content, but if you are up to custom domain or custom TLS certificate -&gt; continue to read.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/#custom-domain","title":"Custom domain","text":"<p>Having your site to render by myawesome.blog URL instead of gitlab.io is solid. For that you just need a <code>A</code> or <code>CNAME</code> DNS record provisioned as explained in the docs. I chose to delegate my netdevops.me domain to Cloudflare, since they provide a TLS certificate for free.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/setting-up-a-hugo-blog-with-gitlab-and-cloudflare/#tls-ssl-certificates","title":"TLS (SSL) certificates","text":"<p>Two common free options when we talk about TLS certs are LetsEncrypt and Cloudflare certs. I am no security expert to claim that one is better than other, I chose a path that is easier, which is Cloudflare FlexSSL in my case.</p> <p></p> <p>Flexible SSL encrypts traffic from Cloudflare to end users of your website, but not from Cloudflare to your origin server. This is the easiest way to enable HTTPS because it doesn\u2019t require installing an SSL certificate on your origin. While not as secure as the other options, Flexible SSL does protect your visitors from a large class of threats including public WiFi snooping and ad injection over HTTP.</p> <p>Implications are clear, FlexSSL is free but does not make secure connection end-to-end, which is fine with me.</p> <p>FlexSSL configuration is as easy as going to Crypto pane in the Cloudflare admin panel and enabling Flexible SSL: </p> <p>In that case nothing is needed to be configured in GitLab, just enjoy your TLS-enabled site.</p> <p>In case you want to enable end-to-end encryption (Strict SSL) there is a thorough guide from Gitlab covering every step.</p>","tags":["hugo","gitlab","cloudflare"]},{"location":"2017/flask-application-in-a-production-ready-container/","title":"Flask application in a production-ready container","text":"<p>Flask documentation is very clear on where is the place for its built-in WSGI application server:</p> <p>Note</p> <p>When running publicly rather than in development, you should not use the built-in development server (flask run). The development server is provided by Werkzeug for convenience, but is not designed to be particularly efficient, stable, or secure.</p> <p>So how about I share with you a Dockerfile that will enable your Flask application to run properly and ready for production-like deployments? As a bonus, I will share my findings discovered along the way of building this container image.</p> <p></p> <p>But before we dive in and start throwing words like uwsgi, nginx and sockets lets set up our vocabulary. As DigitalOcean originally wrote:</p> <ul> <li>WSGI: A Python spec that defines a standard interface for communication between an application or framework and an application/web server. This was created in order to simplify and standardize communication between these components for consistency and interchangeability. This basically defines an API interface that can be used over other protocols.</li> <li>uWSGI: An application server container that aims to provide a full stack for developing and deploying web applications and services. The main component is an application server that can handle apps of different languages. It communicates with the application using the methods defined by the WSGI spec, and with other web servers over a variety of other protocols. This is the piece that translates requests from a conventional web server into a format that the application can process.</li> <li>uwsgi: A fast, binary protocol implemented by the uWSGI server to communicate with a more full-featured web server. This is a wire protocol, not a transport protocol. It is the preferred way to speak to web servers that are proxying requests to uWSGI.</li> </ul>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#why-do-we-even-need-nginx-and-uwsgi-in-front-of-flask","title":"Why do we even need nginx and uWSGI in front of Flask?","text":"<p>That is the question everyone should ask. Main reason is performance, of course. The Flasks built-in web server is a development server by Werkzeug which was not designed to be particularly efficient, stable, or secure. And by all means Werkzeug was not optimized to serve static content, that is why production deployments of Flask apps rely on the following stack:</p> <ol> <li>Front-end web-server (nginx or Apache): load balancing, SSL termination, rate limiting, HTTP parsing and serving static content.</li> <li>WSGI application server (uWSGI, Gunicorn, CherryPy): runs WSGI compliant web applications and does it in a production-grade manner. Handling concurrent requests, process management, cluster membership, logging, configuration, shared memory, etc.</li> </ol> <p>Obviously, development server which comes with Flask simply does not bother about all these tasks that production deployments face. That is why it is so strongly advised against using Flask' server in any kind of production.</p> <p>Speaking about the performance I suggest to check out this presentation from Pycon IE '13 called Maximum Throughput (baseline costs of web frameworks) that explains how number of queries per second depends on web stack you choose.</p> <p>While there are many alternatives to <code>nginx</code>+<code>uWSGI</code> pair, I will focus on these two in this post.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#do-i-need-a-production-grade-flask-app-for-a-pet-project","title":"Do I need a production grade Flask app for a pet project?","text":"<p>While you may go with built-in Flask server for the little projects of your own, this container is so simple that you would not need to use the Built-in server anymore. Why opting out for testing server, if it is easy to launch it in a production-ready way?</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#configuring-nginx","title":"Configuring nginx","text":"<p>We start with configuration of <code>nginx</code> server that will face incoming traffic and handle it for us.</p> <p>We also keep in mind that our nginx server will run in an Alpine Linux docker container.</p> <p>nginx config consists of two parts:</p> <ul> <li>global nginx config file (<code>nginx.conf</code>)</li> <li>site-specific config file (<code>flask-site-nginx.conf</code>)</li> </ul>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#nginx-global-config","title":"nginx global config","text":"<p>For the global nginx config file I combined the recommendations gathered online with nginx configuration samples from uWSGI docs.</p> <p>A little caveat that you might encounter when deploying nginx in Alpine Linux renders itself like that:</p> <pre><code>Error: nginx: [emerg] open() \"/run/nginx/nginx.pid\" failed (2: No such file or directory)\n</code></pre> <p>All you need to do is to to change pid file location since <code>/run/</code> path is not available in Alpine Linux.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#nginx-site-config","title":"nginx site config","text":"<p>Site config (<code>flask-site-nginx.conf</code>) is short and simple:</p> <pre><code>server {\n    location / {\n        try_files $uri @yourapplication;\n    }\n    location @yourapplication {\n        include uwsgi_params;\n        uwsgi_pass unix:///tmp/uwsgi.sock;\n    }\n    # Configure NGINX to deliver static content from the specified folder\n    location /static {\n        alias /app/static;\n    }\n}\n</code></pre> <p>Basically, all you saying here is that your application will be served at <code>/</code> endpoint and use <code>uwsgi</code> wire protocol via unix socket at <code>unix:///tmp/uwsgi.sock</code>.</p> <p>Also we ask nginx to serve static content that is stored in <code>/app/static</code>.</p> <p>Communication path between nginx and WSGI app server can be configured with different sockets and protocols, but <code>unix_socket + uwsgi protocol</code> tends to be the most appropriate way.</p> <p> The uwsgi protocol is derived from SCGI but with binary string length representations and a 4-byte header that includes the size of the var block (16 bit length) and a couple of general-purpose bytes. Binary management is much easier and cheaper than string parsing.</p> <p>So far we dealt with the first bastion, which is nginx config. Our configuration path can be depicted as that:</p> <p></p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#uwsgi-configuration","title":"uWSGI configuration","text":"<p>uWSGI documentation is extensive, you may find all the tweaks and recommendations for the wide range of deployment scenarios. Since this container we build is of general purpose, a sensible uWSGI configuration file (<code>uwsgi.ini</code>) could look as follows:</p> <pre><code>[uwsgi]\nmodule = main\ncallable = app\nplugins = /usr/lib/uwsgi/python\n\nuid = nginx\ngid = nginx\n\nsocket = /tmp/uwsgi.sock\nchown-socket = nginx:nginx\nchmod-socket = 664\n\ncheaper = 1\nprocesses = %(%k + 1)\n</code></pre> <p>This configuration file consists of uWSGI options each of which is documented quite extensively.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#module-and-callable","title":"Module and Callable","text":"<p>We start with defining where is an entry point for uWSGI server to call our app. The <code>module</code> directive corresponds to the name of the python module holding your app. In my case the demo Flask app I built is contained in the <code>main.py</code> file, hence the <code>main</code> module name. On the other hand, <code>callable</code> is the name of an object inside your module, which is a Flask application entry point.</p> <pre><code># coding: utf-8\nfrom flask import Flask\n\napp = Flask(__name__)\n# rest output is omitted\n</code></pre> <p>For me, its the <code>app</code> variable that should be populated to the <code>callable</code> parameter.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#plugins","title":"Plugins","text":"<p>uWSGI is modular and language-agnostic. In Apline Linux deployments it comes with core features built in, but python support is not one of them.</p> <p>uWSGI can include features in the core or as loadable plugins. uWSGI packages supplied with OS distributions tend to be modular. In such setups, be sure to load the plugins you require with the plugins option.</p> <p>That is why <code>plugins</code> parameter is needed where we specify where to find the python plugin. I installed <code>uwsgi-python</code> via apt package manager, this step will be covered as we move to Dockerfile explanation section.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#uid-gid","title":"uid, gid","text":"<p>Common sense: do not run uWSGI instances as root. You can start your uWSGIs as root, but be sure to drop privileges with the uid and gid options.</p> <p>I dropped privileges to <code>nginx</code> user level.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#socket-configuration","title":"Socket configuration","text":"<p>As you remember, we agreed that uwsgi protocol over unix socket will be used as a communication suite between nginx and uWSGI. We already told so to nginx, now its time for uWSGI.</p> <p>Same <code>/tmp/uwsgi.sock</code> is referenced in this <code>uwsgi.ini</code> file. Moreover, we change permissions to that socket file to be readable for <code>nginx</code> user.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#processes-configuration","title":"Processes configuration","text":"<p>uWSGI can spawn multiple processes to run your Flask app, being very productive. But, you need to thoroughly calculate how many processes and threads works for your particular situation.</p> <p>There is no magic rule for setting the number of processes or threads to use. It is very much application and system dependent. Simple math like <code>processes = 2 * cpucores</code> will not be enough. You need to experiment with various setups and be prepared to constantly monitor your apps. <code>uwsgitop</code> could be a great tool to find the best values.</p> <p>In our config file these two lines will do the trick:</p> <pre><code>cheaper = 1\nprocesses = %(%k + 1)\n</code></pre> <p>With <code>cheaper = 1</code> we activate the The uWSGI cheaper subsystem which allows to dynamically scale the number of running workers (processes). So under the minimum load uWSGI will spawn just one workers.</p> <p>The upper limit is dictated by <code>processes = %(%k + 1)</code> statement. The <code>%k</code> is a magic variable, which will be resolved by uWSGI to the number of available cores. So for a single core system, number of max workers will be <code>1 + 1 = 2</code>.</p> <p>We finished another configuration block:</p> <p></p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#supervisord-to-rule-them-all","title":"Supervisord to rule them all","text":"<p>A cherry on a pie is to use the <code>supervisord</code> service to manage nginx and uWSGI. For that we create <code>supervisord.conf</code> with a plain and simple config.</p> <p>Supervisord will watch for these process and restart/start them automatically if things go south for one of them.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#lets-load-it-in-a-container","title":"Lets load it in a container?","text":"<p>Our final part will be creating a lightweight Alpine Linux Docker container image that will have all these parts inside ready to consume.</p> <p>Refer to this comments-rich Dockerfile where we glue together all the things we discussed above in a docker image.</p> <p>One thing to mention here is that python2 and python3 uWSGI plugins are separate packages in Alpine packages system.</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#enjoying-the-result","title":"Enjoying the result","text":"<p>I built two container images for python2 and python3 respectively along with a sample python application. Lets taste them out:</p> <pre><code># pull the image (tagged py2 or py3 respectively)\n[ec2~]$ sudo docker pull hellt/nginx-uwsgi-flask-alpine-docker:py3\npy3: Pulling from hellt/nginx-uwsgi-flask-alpine-docker\nb56ae66c2937: Already exists\n# omitted\nStatus: Downloaded newer image for hellt/nginx-uwsgi-flask-alpine-docker:py3\n</code></pre> <p>The image is very lightweight (62 MB):</p> <pre><code>REPOSITORY                              TAG                 IMAGE ID            CREATED             SIZE\nhellt/nginx-uwsgi-flask-alpine-docker   py3                 7fb6af3baf0e        6 minutes ago       62.5 MB\n</code></pre> <p>Since docker image contains a sample application we can run it to test that everything works as expected:</p> <pre><code>sudo docker run -p 38080:80 hellt/nginx-uwsgi-flask-alpine-docker:py3\n</code></pre> <p> Voila</p>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/flask-application-in-a-production-ready-container/#how-do-i-use-this-one","title":"How do I use this one?","text":"<p>First of all, there is no need to use the image from the docker hub, it was created for demonstration purposes. To create the same container but for your application, consider the following steps:</p> <ol> <li>Clone the repo with the Dockerfile and configuration files</li> <li>Tune the config files if necessary:<ol> <li>Tune <code>uwsgi.ini</code> config: i.e. <code>cheaper</code> number and <code>processes</code> to match your hardware</li> <li>Enhance nginx config</li> </ol> </li> <li>Copy your app to the <code>/app</code> subdirectory and you are good to build your image</li> </ol>","tags":["flask","nginx","uwsgi","docker"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/","title":"Building AWS Lambda with Python, S3 and serverless","text":"<p>Cloud-native revolution pointed out the fact that the microservice is the new building block and your best friends now are Containers, AWS, GCE, Openshift, Kubernetes, you-name-it. But suddenly micro became not that granular enough and people started talking about serverless functions!</p> <p></p> <p>When I decided to step in the serverless property I chose AWS Lambda as my instrument of choice. As for experimental subject, I picked up one of my existing projects - a script that tracks new documentation releases for Nokia IP/SDN products (which in the past I aggregated at <code>nokdoc.github.io</code> (now closed)).</p> <p>Given that not so many posts are going deeper than onboarding a simplest function, I decided to write down the key pieces I needed to uncover to push a real code to the Lambda.</p> <p>Buckle up, our agenda is fascinating:</p> <ul> <li>testing basic Lambda onboarding process powered by Serverless framework</li> <li>accessing files in AWS S3 from within our Lambda with <code>boto3</code> package and custom AWS IAM role</li> <li>packaging non-standard python modules for our Lambda</li> <li>exploring ways to provision shared code for Lambdas</li> <li>and using path variables to branch out the code in Lambda</li> </ul>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#init","title":"Init","text":"<p>What I am going to lambdsify is an existing python3 script called nokdoc-sentinel which has the following Lambda-related properties:</p> <ul> <li>uses non standard python package -- <code>requests</code></li> <li>reads/writes a file.</li> </ul> <p>I specifically emphasized this non-std packages and relying on persistence since these aspects are not covered in 99% of Lambda-related posts, so, filling the spot.</p> <p>AWS Lambda is a compute service that lets you run code without provisioning or managing servers. AWS Lambda executes your code only when needed and scales automatically, from a few requests per day to thousands per second.</p> <p>Multiple choices are exposed to you when choosing an instrument to configure &amp; deploy an AWS Lambda:</p> <ul> <li>AWS Console (web)</li> <li>AWS CLI</li> <li>Multiple frameworks (Serverless, Chalice, Pywren)</li> </ul> <p>While it might be good to feel the taste of a manual Lambda configuration process through the AWS Console, I decided to go \"everything as a code\" way and use the Serverless framework to define, configure and deploy my first Lambda.</p> <p>The Serverless Framework helps you develop and deploy your AWS Lambda functions, along with the AWS infrastructure resources they require. It's a CLI that offers structure, automation and best practices out-of-the-box, allowing you to focus on building sophisticated, event-driven, serverless architectures, comprised of Functions and Events.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#serverless-installation-and-configuration","title":"Serverless installation and configuration","text":"<p>First things first, install the framework and configure AWS credentials. I already had credentials configured for AWS CLI thus skipped that part, if that is not the case for you, the docs are comprehensive and should have you perfectly covered.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#creating-a-service-template","title":"Creating a Service template","text":"<p>Once serverless is installed, start with creating an <code>aws-python3</code> service:</p> <p>A <code>service</code> is like a project. It's where you define your AWS Lambda Functions, the events that trigger them and any AWS infrastructure resources they require, all in a file called serverless.yml.</p> <pre><code>serverless create --template aws-python3 --name nokdoc-sentinel\n</code></pre> <p>Two files will be created:</p> <ul> <li><code>handler.py</code> -- a module with Lambda function boilerplate code</li> <li><code>serverless.yml</code> -- a service definition file</li> </ul>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#making-lambda-instance-out-of-a-template","title":"Making lambda instance out of a template","text":"<p>I renamed <code>handler.py</code> module to <code>sentinel.py</code>, also changed the enclosed function' name and deleted redundant code from the template. For starters I kept the portion of a sample code just to test that deploying to AWS via serverless actually works.</p> <pre><code># sentinel.py\nimport json\n\ndef check(event, context):\n    body = {\n        \"message\": \"Sentinel is on watch!\",\n    }\n\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(body)\n    }\n\n    return response\n</code></pre> <p>Thing to remember is that you also must to make appropriate changes in the <code>serverless.yml</code>, once you renamed the module and the function names:</p> <pre><code>functions:\n# name of the func in the module\n  check:\n    # `handler: sentinel.check` reads as \n    # \"`check` function in the `sentinel` module\n    handler: sentinel.check\n</code></pre>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#deploying-and-testing-aws-lambda","title":"Deploying and Testing AWS Lambda","text":"<p>Before adding some actual load to the Lambda function, lets test that the deployment works. To trigger Lambda execution I added HTTP GET event with the <code>test</code> path in the <code>serverless.yml</code> file. So a call to <code>https://some-aws-hostname.com/test</code> should trigger our lambda function to execute.</p> <pre><code>functions:\n  hello:\n    handler: handler.hello\n    # add http GET trigger event\n    events:\n      - http:\n          path: test\n          method: get\n</code></pre> <p>Read all about supported by serverless framework events in the official docs.</p> <p>And we are coming to the first test deployment with the following assets:</p> <pre><code>$ tree -L 1\n.\n|-- sentinel.py\n`-- serverless.yml\n</code></pre> <p>Lets go and deploy:</p> <pre><code>$ serverless deploy\nServerless: Packaging service...\nServerless: Creating Stack...\nServerless: Checking Stack create progress...\n.....\nServerless: Stack create finished...\nServerless: Uploading CloudFormation file to S3...\nServerless: Uploading artifacts...\nServerless: Uploading service .zip file to S3 (0.33 MB)...\nServerless: Validating template...\nServerless: Updating Stack...\nServerless: Checking Stack update progress...\n..............................\nServerless: Stack update finished...\nService Information\nservice: nokdoc-sentinel\nstage: dev\nregion: us-east-1\napi keys:\n  None\nendpoints:\n  GET - https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/test\nfunctions:\n  check: nokdoc-sentinel-dev-check\n</code></pre> <p>Note the endpoint URL at the bottom of the output, using this API endpoint we can check if our Lambda is working:</p> <pre><code>curl https://xxxxxxxx.execute-api.us-east-1.amazonaws.com/dev/test\n{\"message\": \"Sentinel is on watch!\"}\n</code></pre>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#exploring-serverless-artifacts","title":"Exploring Serverless artifacts","text":"<p>Serverless deployed the Lambda using some defaults parameters (region: us-east-1, stage: dev, IAM role); plus serverless did some serious heavy-lifting in order to deploy our code to AWS. In particular:</p> <ul> <li>archived the project files as a zip archive and loaded it to AWS S3</li> <li>created CloudFormation template that defines all the steps needed to onboard a Lambda and setup an API gateway to respond to <code>GET</code> requests</li> </ul> <p>Key artifacts that were created by serverless in AWS can be browsed with the AWS CLI:</p> <pre><code># exploring deployed Lambda\n$ aws --region us-east-1 lambda list-functions\n{\n    \"Functions\": [\n        {\n            \"FunctionName\": \"nokdoc-sentinel-dev-check\",\n            \"FunctionArn\": \"arn:aws:lambda:us-east-1:446595173912:function:nokdoc-sentinel-dev-check\",\n            \"Runtime\": \"python3.6\",\n            \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-us-east-1-lambdaRole\",\n            \"Handler\": \"sentinel.check\",\n            \"CodeSize\": 1395199,\n            \"Description\": \"\",\n            \"Timeout\": 6,\n            \"MemorySize\": 1024,\n            \"LastModified\": \"2017-07-17T19:06:59.405+0000\",\n            \"CodeSha256\": \"QrFOl8eBL8HipGRCkN/P7wsxkn8/LDIMCAQLxAVmFfI=\",\n            \"Version\": \"$LATEST\",\n            \"TracingConfig\": {\n                \"Mode\": \"PassThrough\"\n            }\n        }\n    ]\n}\n\n\n# exploring S3 artifacts\n$ aws s3 ls | grep sentinel\n2017-07-17 22:05:13 nokdoc-sentinel-dev-serverlessdeploymentbucket-moviajl407hw\n\n$ aws s3 ls nokdoc-sentinel-dev-serverlessdeploymentbucket-moviajl407hw/serverless/nokdoc-sentinel/dev/1500318307598-2017-07-17T19:05:07.598Z/\n2017-07-17 22:05:40       3578 compiled-cloudformation-template.json\n2017-07-17 22:05:41    395199 nokdoc-sentinel.zip\n\n# exploring CloudFormation stack\n$ aws --region us-east-1 CloudFormation list-stacks\n{\n    \"StackSummaries\": [\n        {\n            \"StackId\": \"arn:aws:cloudformation:us-east-1:446595173912:stack/nokdoc-sentinel-dev/da010710-6b22-11e7-aa95-500c20fef6d1\",\n            \"StackName\": \"nokdoc-sentinel-dev\",\n            \"TemplateDescription\": \"The AWS CloudFormation template for this Serverless application\",\n            \"CreationTime\": \"2017-07-17T19:05:08.875Z\",\n            \"LastUpdatedTime\": \"2017-07-17T19:05:45.283Z\",\n            \"StackStatus\": \"UPDATE_COMPLETE\"\n        }\n    ]\n}\n</code></pre> <p>Are you interested what is in this archive <code>nokdoc-sentinel.zip</code>?</p> <pre><code>$ ls -la ~/Downloads/nokdoc-sentinel/\ntotal 16\ndrwx------@  6 romandodin  staff   204 Jul 18 09:51 .\ndrwx------+ 54 romandodin  staff  1836 Jul 18 09:51 ..\ndrwxr-xr-x@  3 romandodin  staff   102 Jul 18 09:51 .vscode\n-rw-r--r--@  1 romandodin  staff   208 Jan  1  1980 sentinel.py\n-rw-r--r--@  1 romandodin  staff  3720 Jan  1  1980 watcher.py\n</code></pre> <p>There are two files we dealt with earlier plus <code>.vscode</code> dir that a text editor created for its settings. Having <code>.vscode</code> in the deployment package actually indicates that by default serverless zipped everything in the project' dir. You can get in control of this process by using include/exclude statements.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#accessing-aws-s3-from-within-a-lambda","title":"Accessing AWS S3 from within a Lambda","text":"<p>It is natural that AWS assumes that Lambdas will be used in a close cooperation with the rest of the AWS family. And for the file storage AWS S3 is a one-stop shop.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#sorting-out-permissions","title":"Sorting out permissions","text":"<p>What you have to sort out before digging into S3 interaction is the permissions that your Lambda has. When serverless deployed our Lambda with a lot of defaults it also handed out a default IAM role to our Lambda:</p> <pre><code>aws --region us-east-1 lambda list-functions | grep Role\n            # role name is nokdoc-sentinel-dev-us-east-1-lambdaRole\n            \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-us-east-1-lambdaRole\",\n</code></pre> <p>To be able to interact with AWS S3 object model, this Role should have access to S3. Lets investigate:</p> <pre><code>aws iam get-role-policy --role-name nokdoc-sentinel-dev-us-east-1-lambdaRole --policy-name dev-nokdoc-sentinel-lambda\n{\n    \"RoleName\": \"nokdoc-sentinel-dev-us-east-1-lambdaRole\",\n    \"PolicyName\": \"dev-nokdoc-sentinel-lambda\",\n    \"PolicyDocument\": {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\n                    \"logs:CreateLogStream\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:logs:us-east-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*\"\n                ],\n                \"Effect\": \"Allow\"\n            },\n            {\n                \"Action\": [\n                    \"logs:PutLogEvents\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:logs:us-east-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*:*\"\n                ],\n                \"Effect\": \"Allow\"\n            }\n        ]\n    }\n}\n</code></pre> <p>As you see S3 access is not a part of default permissions, so we must grant it to our Lambda. Instead of adding additional permissions to the existing role manually, we can re-deploy the Lambda with updated <code>serverless.yml</code> file. In this edition I specified availability zone, set existing S3 bucket as a deployment target and included IAM role configuration allowing full-access to S3 objects:</p> <pre><code># serverless.yml\nprovider:\n  name: aws\n  runtime: python3.6\n  stage: dev\n  region: eu-central-1\n  # deploy Lambda function files to the bucket `rdodin`\n  deploymentBucket: rdodin\n  # IAM Role configuration to allow all-access for S3 objects of bucket `rdodin`\n  iamRoleStatements:\n    - Effect: \"Allow\"\n      Action: \"s3:*\"\n      Resource: \"arn:aws:s3:::rdodin/*\"\n</code></pre> <p>Now the re-deployment will create another Lambda (hence the availability zone has changed), deploy the code in the existing bucket <code>rdodin</code> and apply a policy that allows S3 interaction.</p> <pre><code># checking the inline policy of the IAM Role bound to Lambda\naws lambda get-function --function-name nokdoc-sentinel-dev-check | grep Role\n        \"Role\": \"arn:aws:iam::446595173912:role/nokdoc-sentinel-dev-eu-central-1-lambdaRole\",\n\naws iam list-role-policies --role-name nokdoc-sentinel-dev-eu-central-1-lambdaRole\n{\n    \"PolicyNames\": [\n        \"dev-nokdoc-sentinel-lambda\"\n    ]\n}\n\naws iam get-role-policy --role-name nokdoc-sentinel-dev-eu-central-1-lambdaRole --policy-name dev-nokdoc-sentinel-lambda\n{\n    \"RoleName\": \"nokdoc-sentinel-dev-eu-central-1-lambdaRole\",\n    \"PolicyName\": \"dev-nokdoc-sentinel-lambda\",\n    \"PolicyDocument\": {\n        \"Version\": \"2012-10-17\",\n        \"Statement\": [\n            {\n                \"Action\": [\n                    \"logs:CreateLogStream\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:logs:eu-central-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*\"\n                ],\n                \"Effect\": \"Allow\"\n            },\n            {\n                \"Action\": [\n                    \"logs:PutLogEvents\"\n                ],\n                \"Resource\": [\n                    \"arn:aws:logs:eu-central-1:446595173912:log-group:/aws/lambda/nokdoc-sentinel-dev-check:*:*\"\n                ],\n                \"Effect\": \"Allow\"\n            },\n            {\n                \"Action\": \"s3:*\",\n                \"Resource\": \"arn:aws:s3:::rdodin/*\",\n                \"Effect\": \"Allow\"\n            }\n        ]\n    }\n}\n</code></pre> <p>Now as the S3 permissions are there, we are free to list bucket contents and modify the files in it.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#using-boto3-to-readwrite-files-in-aws-s3","title":"Using Boto3 to read/write files in AWS S3","text":"<p>AWS provides us with the boto3 package as a Python API for AWS services. Moreover, this package comes pre-installed on the system that is used to run the Lambdas, so you do not need to provide a package.</p> <p>I put a file (releases_current.json) that my script expects to read to the directory created by the serverless deployment script:</p> <pre><code>$ aws s3 ls rdodin/serverless/nokdoc-sentinel/\n                           PRE dev/\n2017-07-22 15:57:07       3424 releases_current.json\n</code></pre> <p>Lets see if we can access it from within the Lambda using <code>boto3</code> and its documentation:</p> <pre><code># sentinel.py\nimport json\nimport boto3\n\n\ndef check(event, context):\n    s3 = boto3.resource('s3')\n    bucket = s3.Bucket('rdodin')\n    # reading a file in S3 bucket\n    original_f = bucket.Object(\n        'serverless/nokdoc-sentinel/releases_current.json').get()['Body'].read()[:50]\n    # writing to a file\n    new_f = bucket.put_object(\n        Key='serverless/nokdoc-sentinel/newfile.txt', Body='Hello AWS').get()['Body'].read()\n\n    body = {\n        \"message\": \"Sentinel loaded a file {} and created a new file {}\"\n        .format(original_f, new_f),\n    }\n\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(body)\n    }\n\n    return response\n</code></pre> <p>Re-deploy and check:</p> <pre><code>$ curl https://xxxxx.execute-api.eu-central-1.amazonaws.com/dev/test\n{\"message\": \"Sentinel loaded a file b'{\\\"nuage-vsp\\\": [\\\"4.0.R8\\\", \\\"4.0.R7\\\", \\\"4.0.R6.2\\\", \\\"4.' and created a new file b'Hello AWS'\"}\n</code></pre> <p>So far, so good. We are now capable of reading/writing to a file stored in AWS S3.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#adding-python-packages-to-lambda","title":"Adding python packages to Lambda","text":"<p>We were lucky to use only the packages that either standard (<code>json</code>) or comes preinstalled in Lambda-system (<code>boto3</code>). But what if we need to use packages other from that, maybe your own packages or from PyPI?</p> <p>Well, in that case you need to push these packages along with your function' code as a singe deployment package. As official guide says you need to copy packages to the root directory of your function and zip everything as a single archive.</p> <p>What comes as a drawback of this recommendation is that</p> <ul> <li>your project dir will be dirty with all these packages sitting in the root</li> <li>you will have to .gitignore these packages directory to keep your packages out of a repository</li> </ul> <p>I like the solution proposed in the \"Building Python 3 Apps On The Serverless Framework\" post. Install your packages in a some directory in your projects dir and modify your <code>PYTHONPATH</code> to include this directory.</p> <pre><code># install `requests` package in a `vendored` dir at the projects root\npip install -t vendored/ requests\n\n# `requests` and its dependencies are there\n$ ls vendored/\ncertifi                     chardet                     idna                        requests                    urllib3\ncertifi-2017.4.17.dist-info chardet-3.0.4.dist-info     idna-2.5.dist-info          requests-2.18.1.dist-info   urllib3-1.21.1.dist-info\n</code></pre> <p>Now modify your code to include <code>vendored</code> directory in your <code>PYTHONPATH</code></p> <pre><code>import boto3\n\nhere = os.path.dirname(os.path.realpath(__file__))\nsys.path.append(os.path.join(here, \"vendored\"))\n# now it is allowed to add a non-std package\nimport requests\n\ndef check(event, context):\n# output omitted\n</code></pre> <p>Note, that if a package has a native binary code, it must be compiled for the system that is used to run Lambdas.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#shared-code-for-lambdas","title":"Shared code for Lambdas","text":"<p>Even though a Lambda often assumed as an independent function, a real application you might want to transfer to Lambda quite likely will have dependencies on some common code. Refer to the \"Writing Shared Code\" section of the above mentioned blog post to see how its done.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#handling-arguments-in-lambdas","title":"Handling arguments in Lambdas","text":"<p>Another common practice in a classic util function is to have some arguments (argparse) that allow to branch out the code and make an app' logic feature-rich. In Lambdas, of course, you have no CLI exposed, so to make a substitution for the arguments you can go two ways:</p> <ul> <li>create several functions for your project and bind different API endpoints to each of them</li> <li>use a single function and add a variable part to the API endpoint</li> </ul> <p>I will show how to handle the latter option. First, create a variable parameter for your API endpoint in the <code>serverless.yml</code>:</p> <pre><code>    events:\n      - http:\n          # `{command}` is a variable part here\n          path: go/{command}\n          method: get\n</code></pre> <p>Now you Lambda can be branched out like that, using the part that you will place in the end of your API endpoint as an argument.</p> <pre><code>def check(event, context):\n    # a variable that we referenced as {command} in serverless.yml\n    # can be accessed by a `command` key of event['pathParameters'] dict\n    if 'branch1' in event['pathParameters']['command']:\n        body = {\n            \"message\": \"Argument `A` execution block\"\n        }\n\n    if 'branch2' in event['pathParameters']['command']:\n        body = {\n            \"message\": \"Argument `B` execution block\"\n        }\n\n    response = {\n        \"statusCode\": 200,\n        \"body\": json.dumps(body)\n    }\n\n    return response\n</code></pre> <p>Now adding an arbitrary text after the <code>go/</code> path will be evaluated in your Lambda allowing you to conditionally execute some parts of your code.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#summary","title":"Summary","text":"<p>With the above explained concepts I successfully transferred nokdoc-sentinel script from a standalone cron-triggered module to the AWS Lambda. You can check out the project' code and the <code>serverless.yml</code> file at github repo.</p>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/building-aws-lambda-with-python-s3-and-serverless/#links","title":"Links","text":"<ol> <li>Benny Bauer -- Python in The Serverless Era PyCon 2017</li> <li>Ryan S. Brown -- Building Python 3 Apps On The Serverless Framework</li> <li>Serverless Framework AWS Guide</li> <li>AWS Lambda Developers Guide</li> <li>AWS CLI Command Reference</li> <li>Keeping secrets out of Git with Serverless</li> </ol>","tags":["aws","aws lambda","boto3","python","serverless"]},{"location":"2017/sr-os-rootifier-or-how-to-flatten-7750-sr-config/","title":"SR OS Rootifier or how to flatten 7750 SR config","text":"<p>Back in the days when I mostly did routing stuff I spent the whole day configuring SROS devices via SSH. And once in a while I saw that SSH session or its server part (or even underlying connection) glitched, resulting in a corrupted lines feeded to the device.</p> <p>What was also quite common is to make a mistake (i.e. syntax one) in a single line and watch like the rest of config got applied to the wrong context.</p> <p>These sad facts pushed me to create a rootifier CLI script, that was converting tree-like SROS config into flattented (aka rooted) fashion.</p> <p></p> <p>update 2023</p> <p>The web service that was available publicly but has been now decommissioned due to the transition to MD-CLI.</p>","tags":["sr os","nokia"]},{"location":"2017/sr-os-rootifier-or-how-to-flatten-7750-sr-config/#sros-config-structure","title":"SROS config structure","text":"<p>As you well aware, SROS config is of indent-based tree-like structure:</p> <pre><code>configure\n--------------------------------------------------\necho \"System Configuration\"\n--------------------------------------------------\n    system\n        name \"ntdvps\"\n        location \"netdevops.me\"\n        chassis-mode d\n</code></pre> <p>It is readable for a human, but it is much safer to apply batch config using the flattened structure, where each command is given in a full context fashion. Passed through a rootifier our example will transform as displayed:</p> <pre><code>/configure system name \"ntdvps\"\n/configure system location \"netdevops.me\"\n/configure system chassis-mode d\n</code></pre> <p>Now each command has a full path applied and even making an error in a single command will not affect the rest of them, making configuration safer.</p> <p>Yeah, probably applying rootifier to a short config snippets make a little sense, but pushing a solid 300+ lines config to a fresh box would definitely benefit from rootifying.</p> <p>Take a look at this diff made for a real-life config of SROS box before and after rootifying. Not only it downsized from 1600 lines to 600, it also became safer to push via console/SSH connection.</p>","tags":["sr os","nokia"]},{"location":"2017/sr-os-rootifier-or-how-to-flatten-7750-sr-config/#usage-scenarios-and-limitations","title":"Usage scenarios and limitations","text":"<p>As I explain in the Usage and Limitations section rootifier accepts</p> <ul> <li>either the whole config file content</li> <li>or any part of it, that starts under <code>configure</code> section</li> </ul> <p>For instance, valid config portions are:</p> <p>1. Full config</p> <p>As you see it via <code>admin display-config</code> or in the config file you can copy it it as a whole, or from the beginning to the desired portion</p> <pre><code># TiMOS-B-14.0.R4 both/x86_64 Nokia 7750 SR Copyright (c) 2000-2016 Nokia.\n# All rights reserved. All use subject to applicable license agreements.\n# Built on Thu Jul 28 17:26:11 PDT 2016 by builder in /rel14.0/b1/R4/panos/main\n\n# Generated WED NOV 22 12:22:35 2017 UTC\n\nexit all\nconfigure\n#--------------------------------------------------\necho \"System Configuration\"\n#--------------------------------------------------\n    system\n        name \"pe.pod62.cats\"\n        chassis-mode d\n        dns\n        exit\n        snmp\n        exit\n        time\n            ntp\n                server 10.167.55.2\n                no shutdown\n            exit\n            sntp\n                shutdown\n            exit\n            dst-zone CEST\n                start last sunday march 02:00\n                end last sunday october 03:00\n            exit\n            zone UTC\n        exit\n</code></pre> <p>2. Portion of the config that starts with 4 spaces exactly</p> <pre><code>    system\n        name \"pe.pod62.cats\"\n        chassis-mode d\n        dns\n        exit\n        snmp\n        exit\n        time\n            ntp\n                server 10.167.55.2\n                no shutdown\n            exit\n</code></pre> <p>3. Any part of the config with specified context</p> <p>Since rootifier does not know the config structure and makes decision only by indentations in the passed config, it can not say what context was this snippet from:</p> <pre><code>#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"loopback\"\n                prefix 1.1.1.1/32 exact\n            exit\n            policy-statement \"export_loopback\"\n                entry 10\n                    from\n                        prefix-list \"loopback\"\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n</code></pre> <p>Thus, rootifier will not render the rooted version of this snippet correctly.</p> <p>Now we, of course, know that policies are configured under the <code>/configure router</code> context, so we can help rootifier by setting the context:</p> <pre><code>    # put a missing context before your snippet\n    router\n        policy-options\n            begin\n            prefix-list \"loopback\"\n                prefix 1.1.1.1/32 exact\n            exit\n            policy-statement \"export_loopback\"\n                entry 10\n                    from\n                        prefix-list \"loopback\"\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n</code></pre> <p>You can even extract deeply nested config portions and rootify them, just specify the missing context:</p> <pre><code>    # missing context\n    router policy-options\n            # original snippet\n            prefix-list \"Customer_1\"\n                prefix 10.10.55.0/24 exact\n                prefix 10.10.66.0/24 exact\n            exit\n            prefix-list \"Customer_2\"\n                prefix 172.10.55.0/24 exact\n                prefix 172.10.66.0/24 exact\n            exit\n            community \"East\" members \"65510:200\"\n            community \"West\" members \"65510:100\"\n            community \"Customer_2\" members \"65510:2\"\n</code></pre>","tags":["sr os","nokia"]},{"location":"2017/sr-os-rootifier-or-how-to-flatten-7750-sr-config/#ps","title":"PS","text":"<p>Rootifier web service is a Flask application deployed in a container in ElasticBeanstalk on AWS. Probably I will write about this way of deploying the code in a later post.</p> <p>Rootifier source code is hosted on Github.</p> <p>A similar work (CLI version) was done by honorable David Roy - transcode-sros.</p>","tags":["sr os","nokia"]},{"location":"2017/changing-libvirt-bridge-attachment-in-a-running-domain-aka-on-the-fly/","title":"Changing Libvirt bridge attachment in a running domain aka on-the-fly","text":"<p>At work I always prefer KVM hosts for reasons such as flexible, free and GUI-less. Yet I never bothered to go deeper into the networking features of Libvirt, so I only connect VMs to the host networks via Linux Bridges or OvS. Far far away from fancy virtual libvirt networks.</p> <p>Even with this simple networking approach I recently faced a tedious task of reconnecting VMs to different bridges on-the-fly. My use case came from a need to connect a single traffic generator VM to the different access ports of virtual CPEs. Essentially this meant that I need to reconnect my traffic generator interfaces to different bridges back and forth:</p> <p> </p> <p>Apparently there is no such <code>virsh</code> command that will allow you to change bridge attachments for networking devices, so a bit of bash-ing came just handy.</p> <p>You know network interface device definition grepped from Libvirt XML format holds bridge association:</p> <pre><code>&lt;!-- OMITTED --&gt;\n  &lt;devices&gt;\n&lt;!-- OMITTED --&gt;\n    &lt;interface type='bridge'&gt;\n      &lt;mac address='52:54:00:cd:75:4f'/&gt;\n      &lt;source bridge='br12'/&gt;\n      &lt;model type='virtio'/&gt;\n      &lt;address type='pci' domain='0x0000' bus='0x00' slot='0x04' function='0x0'/&gt;\n    &lt;/interface&gt;\n&lt;!-- OMITTED --&gt;\n</code></pre> <p>The brute force approach would be to change the bridge identified in the domain definition, restart the domain and be with it. Not sporty at all!</p> <p>Instead I decided that it would be good to have a script that for starters will take <code>domain_name</code>, <code>interface_name</code> and <code>new_bridge_id</code> and do the rest. So <code>vifmove.sh</code> (virsh interface move) was born.</p> <p>This is a tiny bash script which does the job for me just fine:</p> <p> </p> <p>Underneath its all simple, I leveraged <code>virsh update-device</code> command and just templated the interface definition XML file:</p> <p>If you find this one useful, feel free to add your ideas in the gist comments.</p>","tags":["virsh"]},{"location":"2017/destroy-and-undefine-kvm-vms-in-a-single-run/","title":"Destroy and Undefine KVM VMs in a single run","text":"<p><code>virsh</code> is a goto console utility for managing Qemu/KVM virtual machines. But when it comes to deletion of the VMs you better keep calm - there is no single command to destroy the VM, its definition XML file and disk image.</p> <p>Probably not a big problem if you have a long-living VMs, but if you in a testing environment it is naturally to spawn and kill VMs quite often. Lets see how <code>xargs</code> can help us with that routine.</p> <p>Right now my KVM hypervisor is busy with virtualizing small Nuage VNS environment where <code>4010_DEMO*</code> VMs are virtual SDN gateways:</p> <pre><code>[root@srv601 ~]# virsh list\n Id    Name                           State\n----------------------------------------------------\n 1     1-vsc1.pod60.cats              running\n 2     1-vsc2.pod60.cats              running\n 3     g1pe.play.cats                 running\n 4     jenkins                        running\n 5     dns                            running\n 6     1-es1.pod60.cats               running\n 7     1-vsd1.pod60.cats              running\n 8     1-util1.pod60.cats             running\n 18    4010_DEMO_I2                   running\n 19    4010_DEMO_I1                   running\n 22    4010_DEMO2_NSGI1               running\n 23    4010_DEMO2_NSGI2               running\n 24    4010_DEMO_1upl_testNSGI1       running\n 25    4010_DEMO_1upl_testNSGI2       running \n</code></pre> <p>And I really need to get these last six boys gone for good. With <code>virsh</code> one need to perform the following set of commands to achieve the goal:</p> <pre><code># removing the last VM with a virsh domain name 4010_DEMO_1upl_testNSGI2\n# first destroy the domain (you even cant pass multiple names)\n$ virsh destroy 4010_DEMO_1upl_testNSGI2\nDomain 4010_DEMO_1upl_testNSGI2 destroyed\n\n# now undefine the domain\n$ virsh undefine 4010_DEMO_1upl_testNSGI2\nDomain 4010_DEMO_1upl_testNSGI2 has been undefined\n\n# deleting the disk images\n# assuming that all the VM-related data resides in that dir\n$ rm -rf /var/lib/libvirt/images/4010_DEMO_1upl_testNSGI2\n</code></pre> <p>Too much typing for a simple task... Lets see how <code>xargs</code> comes into play!</p>","tags":["virsh","qemu","kvm","libvirt"]},{"location":"2017/destroy-and-undefine-kvm-vms-in-a-single-run/#grep-and-xargs","title":"grep and xargs","text":"<p>What we need to do is to filter out the target domain names and pass these names to the <code>virsh destroy &amp;&amp; virsh undefine &amp;&amp; rm -rf</code> commands.</p> <p>First things first, lets get the names of the domains. <code>grep</code> is the tool of choice.</p> <pre><code># grep flags:\n#   -o -- return only the matched group (not the whole line with match highlighted)\n#   -E -- regular expression\nvirsh list --all | grep -o -E \"(4010_DEMO\\w*)\"\n\n# OUTPUT:\n4010_DEMO_I2\n4010_DEMO_I1\n4010_DEMO2_NSGI1\n4010_DEMO2_NSGI2\n4010_DEMO_1upl_testNSGI1\n</code></pre> <p>Bingo, now its <code>xargs</code> time:</p> <p><code>xargs</code> reads items from the standard input, delimited by blanks or newlines, and executes the command (default is /bin/echo) one or more times with any initial-arguments followed by items read from standard input. Blank lines on the standard input are ignored.</p> <pre><code>virsh list --all | grep -o -E \"(4010_DEMO\\w*)\" | \\\nxargs -I % sh -c 'virsh destroy % &amp;&amp; virsh undefine % &amp;&amp; rm -rf /var/lib/libvirt/images/%;'\n</code></pre> <p>The <code>xargs</code> flag <code>-I %</code> here allows us to substitute each <code>%</code> sign in the command with the <code>xargs</code> input argument. This effectively destroys the virsh domain along with its definition and disk image.</p>","tags":["virsh","qemu","kvm","libvirt"]},{"location":"2017/waiting-for-ssh-service-to-be-ready-with-paramiko/","title":"Waiting for SSH service to be ready with Paramiko","text":"<p>Today I faced a task which required first to establish an SSH tunnel in a background process and later use this tunnel for SSH connection. What seemed like a child's play first actually had some fun inside.</p> <p>A problem were hidden right between the moment you spawned <code>ssh</code> process in the background and the next moment you tried to use this tunnel. In other words, it takes literally no time to spawn a process in the background, but without checking that tunnel is ready, you will quite likely receive an error, since your next instructions will be executed immediately after.</p> <p>Consequently, I needed a way to ensure that the SSH service is ready before I try to consume it.</p> <p> </p> <p>But how do you check if there is a server behind some <code>host:port</code> and that this server is of SSH nature? In Ansible we could leverage <code>wait_for</code> module that can poke a socket and see if OpenSSH banner is there. But in my case Python &amp; Paramiko was all I had.</p> <p>It turned out that with Paramiko it is possible to achieve the goal with most straightforward and probably least elegant code:</p> <p>I found it sufficient to setup a timer-driven while loop where Paramiko tries to open a connection without credentials. In order to detect if socket is opened I catch different type of exceptions that Paramiko emits:</p> <ul> <li>if there is nothing listening on a particular socket, then Paramiko emits <code>paramiko.ssh_exception.NoValidConnectionsError</code></li> <li>if the socket is open, but the responding service is not SSH, then Paramiko emits <code>paramiko.ssh_exception.SSHException</code> with a particular message Error reading SSH protocol banner</li> <li>if the socket is open and SSH service responding on the remote part - we are good to go! This time still <code>paramiko.ssh_exception.SSHException</code> is emitted, but the error message would be No authentication methods provided.</li> </ul> <p>And that quite does the trick:</p> <p> </p>","tags":["paramiko","python"]},{"location":"2017/installing-xrdp-091-on-ubuntu-1604-xenial/","title":"Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial","text":"<p>xrdp is defacto the default RDP server for Linux systems sharing with VNC the remote access solution olympus. I personally found it more resource friendly and feature rich compared to VNC solutions I tried.</p> <p>The only problem I found with <code>xrdp</code> is that current Ubuntu LTS release Xenial 16.04 has a way outdated 0.6.1-2 version of xrdp in the packages repo. This version has no shared clipboard support, which makes remote support/remote access a tedious task.</p> <p>xrdp currently in its 0.9.3 version and it would be really nice to have a more recent package, rather than installing it from sources, like many solutions propose.</p> <p>Well, no need to compile <code>xrdp</code> from sources (unless you want to), because you can leverage a ppa from hermlnx that has <code>xrdp 0.9.1-7</code> already built for amd64 and i386 systems</p> <pre><code># all you need is\nsudo add-apt-repository ppa:hermlnx/xrdp\nsudo apt-get update\nsudo apt-get install xrdp\n</code></pre> <p>You can also try a <code>deb</code> package of <code>xrdp 0.9.2</code> -- https://github.com/suminona/xrdp-ru-audio</p>","tags":["xrdp","ubuntu"]},{"location":"2017/how-to-add-yaml-highlight-in-highlightjs/","title":"How to add YAML highlight in Highlight.js?","text":"<p>Haters gonna hate YAML, thats for sure. I am on the other hand in love with YAML; when one have to manually write/append config files I find YAML easier than JSON (and you have comments too).</p> <p>Ansible, various static-site-generators and quite a lot of opensource tools use YAML syntax for the configuration purposes. But still, YAML syntax highlighting is not a part of the Common languages shipped with highlight.js compiled package.</p> <p>Hugo also uses the hljs to colorize code snippets, but it uses the default pack of languages that lacks YAML support.</p> <p>Look at this greyish snippet, looks ugly.</p> <pre><code>---\n- name: Prepare linux virt host\n  gather_facts: no\n  hosts: localhost\n  tasks:\n    - name: Include packages/services to install/set\n      include_vars: main.yml\n</code></pre> <p>Luckily, we can add custom languages using Cloudflare CDN collection of pre-built packages.</p> <p>To do so, add this config portion to your Hugo' <code>config.toml</code>:</p> <pre><code># double check, that you have \n# syntaxHighlighter = \"highlight.js\" in your config.toml\n\n# note, [[params.customJS]] is nested under [params] section\n  [[params.customJS]]\n      src = \"https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js\"\n      integrity = \"sha256-tvm0lHsuUZcOfj/0C9xJTU4OQx5KpUgxUcAXLX5kvwA=\"\n      crossorigin = \"anonymous\"\n      async = true\n      defer = true\n</code></pre> <p>And now our YAML looks a bit better:</p> <pre><code>---\n- name: Prepare linux virt host\n  gather_facts: no\n  hosts: localhost\n  tasks:\n    - name: Include packages/services to install/set\n      include_vars: main.yml\n</code></pre> <p>I changed the <code>style-*-.min.css</code> property to highlight string portions in green, instead of dark blue. A proper way would be to use a custom HLjs theme, but building it in Tranquilpeak theme is kinda tedious, so I picked up a shortcut changing the compiled css instead:</p> <pre><code># change the color code for this class\n# in my case I changed to #a5c261\n.codeblock .string,figure.highlight .string{color:#a5c261}\n</code></pre> <p>Thanks to Tranquilpeack for Hugo theme maintainer, who shared with me this option for custom highlighting </p> <p>PS. Since the method to load custom JS described in this article has a bug when it comes to Chrome browser, I changed the way Hugo loads custom JS as suggested in the referenced issue.</p>","tags":["highlightjs","hugo"]},{"location":"2017/yang-explorer-in-a-docker-container-based-on-alpine/","title":"Yang Explorer in a docker container based on Alpine","text":"<p>I wrote about the Yang Explorer in a docker quite some time ago, Yang Explorer was v0.6 at that time. Back then the motivation to create a docker image was pretty simple -- installation was a pain in v0.6, it is still a pain, but the official version bumped to 0.8(beta).</p> <p>So I decided to re-build an image, now using Alpine Linux as a base image to reduce the size.</p> <p>Just take a look how noob-ish I was to publish a <code>Dockerfile</code> like this:</p> <pre><code>FROM ubuntu:14.04\nMAINTAINER Roman Dodin &lt;dodin.roman@gmail.com&gt;\nRUN DEBIAN_FRONTEND=noninteractive apt-get update; apt-get install -y python2.7 python-pip python-virtualenv git graphviz libxml2-dev libxslt1-dev python-dev zlib1g-dev\nRUN DEBIAN_FRONTEND=noninteractive git clone https://github.com/CiscoDevNet/yang-explorer.git\nWORKDIR /yang-explorer\nRUN bash setup.sh -y\nRUN sed -i -e 's/HOST=\\x27localhost\\x27/HOST=$HOSTNAME/g' start.sh\nCMD [\"bash\", \"start.sh\"]\n</code></pre> <p>Several unnecessary layers, using Ubuntu as a base -- these are the Docker-novice errors.</p> <p>Few things changed in the Yang Explorer regarding the setup process, now you do not need to install explicitly all the dependencies, they will be installed using the packaged <code>requirements.txt</code> file, so our Dockerfile could be as short as this:</p> <pre><code>FROM alpine\n\nLABEL maintainer=\"dodin.roman@gmail.com, netdevops.me\"\n\nRUN apk add --no-cache bash git python &amp;&amp; \\\n    python -m ensurepip &amp;&amp; \\\n    rm -r /usr/lib/python*/ensurepip &amp;&amp; \\\n    git clone https://github.com/CiscoDevNet/yang-explorer.git\n\nWORKDIR /yang-explorer\n\nRUN apk add --no-cache gcc py-crypto python-dev libffi-dev musl-dev openssl-dev libxml2-dev libxslt-dev &amp;&amp; \\\n    bash setup.sh -y &amp;&amp; \\\n    sed -i -e 's/HOST=\\x27localhost\\x27/HOST=$HOSTNAME/g' start.sh &amp;&amp; \\\n    apk del musl-dev gcc\n\nCMD [\"bash\", \"start.sh\"]\n</code></pre> <p>In the first <code>RUN</code> we write a layer with the tools that are needed to clone the official repo and in the second <code>RUN</code> we install build dependencies, go through setup process and uninstall unnecessary build dependencies to reduce the size.</p> <p>Compressed image size is 358Mb. Uncompressed size is 1.9Gb</p> <p></p>","tags":["yang","docker","yang explorer","alpine linux"]},{"location":"2017/yang-explorer-in-a-docker-container-based-on-alpine/#usage","title":"Usage","text":"<p>To use this image:</p> <ol> <li> <p>Start the container</p> <pre><code>docker run -p 8088:8088 -d hellt/yangexplorer-docker\n</code></pre> </li> <li> <p>Navigate your flash-capable browser to <code>http://&lt;ip_of_your_docker_host&gt;:8088</code></p> </li> </ol>","tags":["yang","docker","yang explorer","alpine linux"]},{"location":"2017/yang-explorer-in-a-docker-container-based-on-alpine/#differences-with-robert-csapo-image","title":"Differences with Robert Csapo image","text":"<p>Main differences are in the size:</p> <ul> <li>Compressed = 358Mb vs 588Mb</li> <li>Uncompressed = 1.9Gb vs 2.51Gb</li> </ul>","tags":["yang","docker","yang explorer","alpine linux"]},{"location":"2017/yang-explorer-in-a-docker-container-based-on-alpine/#links","title":"Links","text":"<ul> <li>My image on Docker Hub</li> <li>Robert' image on Docker hub</li> <li>Official Yang Explorer repo</li> </ul>","tags":["yang","docker","yang explorer","alpine linux"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/","title":"Prepping up for and passing the AWS Certified Solution Architect - Associate","text":"<p>On May 11<sup>th</sup> I passed the AWS Certified Solution Architect Associate exam which was harder then I expected. In this memo I will outline how I prepared to this exam, what topics you better pay more attention to and some tips and hints I could give to anyone going for AWS CSA exam.</p> <p>Disclaimer: I took the original AWS CSA exam, not the one that was launched in Feb 2018; this older version is only available to schedule till August 2018. After that date the newer version of this exam will be the only one available. Watch out, it has a new set of objectives.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#preparation","title":"Preparation","text":"<p>Its worth to mention that prior the preparation to this exam I had a little hands on experience with AWS and almost no theoretical knowledge. So I approached the preparation course as an AWS noob.</p> <p>Q: Do I need to pay for AWS services if I want to pass this exam? A: no, you can pass it without having a real hands on, for instance by reading the AWS Study Guide alone. Though, the Free Tier offering by AWS will make the costs for AWS practice really close to $0. Without laying your hands on basic configuration stuff it could be challenging to pass the exam.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#main-course","title":"Main course","text":"<p>Since I have an active Linux Academy subscription (is now A Cloud Guru) I took theirs certification prep course which I found rather complete and well composed. Its the 20hrs length course with interactive quizzes and the real Hands On Labs. Labs are a great way to gain practice by having the access to AWS Console provided by the Linux Academy.</p> <p> Each section will test you on the outlined concepts. Most of them have real Labs</p> <p>Quizzes really help you to check that the material you've just listened to was actually absorbed by your memory. I went through all the videos on 1.5x speed and cleared all quizzes rather easily. At the very end of the course you can simulate the real exam by answering 60 questions in 80 minutes time frame. While the questions I had during the exam were harder, this simulation can help you to feel the real exam atmosphere.</p> <p>While Hands On Labs are super useful to get the practical knowledge about the AWS Console and the configuration of the various services, I did not complete them all for the sake of time. But in general, I would highly suggest to clear Hands On Labs, most of the training providers offer them.</p> <p>Linux Academy also had an assessment service called Cloud Assessments, now when LA merged with A Cloud Guru this is not available anymore. I completed the offered set of practice tasks and loved the gamification part they embedded. Solving the real practical tasks within the AWS Console can help you if you had no previous AWS management activities.</p> <p> Cloud Assessments challenge you to solve different practice tasks for each major AWS service","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#main-course-alternatives","title":"Main course alternatives","text":"<p>Of course, Linux Academy is not the only service who made these AWS CSA prep courses, the most popular one I saw on the Internet was the A Cloud Guru course. Their students praise this course rather highly, so you can take that one as well. Check out the pricing and the course offerings to pick up the right provider for you.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#additional-resources","title":"Additional resources","text":"<p>I hate to break it, but my experience showed that the course alone won't make you pass the exam easily. And not because the courses are not good enough, they are good, but they do not cover all the aspects or do not explain all the details of some service, which you might encounter during the exam.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#whitepapers","title":"Whitepapers","text":"<p>As a recommended supplement many courses suggest to go over the AWS Whitepapers to get more knowledgeable on the various AWS concepts. I've peered into the two of the papers and left the rest untouched. While the whitepapers are good and useful, they are ~70 pages long and reading them takes quite some time.</p> <p>But if time allows, you better read them, since they cover a lot of the concepts you will be tested against. I hadn't that much time, so I went reading the Study Guide instead.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#aws-csa-official-study-guide","title":"AWS CSA Official Study Guide","text":"<p>The Study Guide offers the right amount of information to prepare you for the exam. I personally did not read the book, since I had a pretty good understanding of AWS services by finishing the LinuxAcademy course but if you don't want to pay for video course and Labs access, the Guide will do just fine.</p> <p>I highly recommend to get your hands on this guide since it has a brilliant set of the Exam Essentials chapters and the straight-to-the-point quizzes after each chapter.</p> <p></p> <p>All I did with this book was that I cleared all the quizzes after each chapter and read few \"Exam essentials\" chapters the day before the exam. And again, go over the quizzes, they are very good and if you'll cover them all, I would say you'd pass the exam with 80%+ score.</p> <p>The quizzes in this Study Guide are better/harder than the ones I solved in the LinuxAcademy course, at the same time they are a good addition to the quizzes in the LinuxAcademy course. I must say that the quizzes had the most positive impact for me to clear the exam, since they helped me to discover my weak spots and focus on the topics where I made a lot of mistakes from the first attempt.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#aws-faqs","title":"AWS FAQs","text":"<p>Now few posts on the Internet suggested to go over the FAQ section for each AWS service that is tested in AWS CSA exam. Thats a very good suggestion, since the FAQ section actually quite resembles the questions you might encounter during the exam. I breezed over two or three FAQs for VPC, RDS and SQS to see whats there; due to the time constraints I left other FAQs unread.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#catch-up-with-a-community","title":"Catch up with a community","text":"<p>Its a wise move to explore how others mastered the exam and pick theirs preparation practices that might work for you as well. I found this two articles quite good and comprehensive, with lots of useful links and suggestions:</p> <ul> <li>My AWS Solution Architect Associate exam experience by Viktorious</li> <li>AWS CSA discussion board on A Cloud Guru</li> </ul>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#analyze-the-weak-spots","title":"Analyze the weak spots","text":"<p>As I said, quizzed helped me to discover my weak spots. So I watched over some videos again and read few more topics, then I went over the quizzes again to make sure that I understood the problem well enough.</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#taking-the-exam","title":"Taking the exam","text":"","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#what-topics-should-i-pay-attention-to-most","title":"What topics should I pay attention to most?","text":"<p>As I mentioned in the beginning, the actual questions I encountered during the exam were not easy-peasy. While, of course, I had a lot of basic questions that tested my ability to identify the key application area for the various AWS services, there were questions that required practical experience or detailed knowledge on the topic.</p> <p>My point here, is that you will a fair share of questions that you could easily prepare to by reading the \"Exam Essentials\" topics of the above mentioned Study Guide, but will it be enough to safely pass the exam? Hard to tell.</p> <p>Exam blueprint states that you will be tested for the following topics:</p> <p>Topic Level Scoring: (with my marks) 1.0\u2008\u2008Designing highly available, cost-efficient, fault-tolerant, scalable systems:\u200879% 2.0\u2008\u2008Implementation/Deployment:\u2008100% 3.0\u2008\u2008Data Security:\u200877% 4.0\u2008\u2008Troubleshooting:\u200880%</p> <p>No doubts, the most popular services will be tested the most:</p> <ul> <li>EC2</li> <li>S3</li> <li>RDS</li> <li>DynamoDB</li> <li>AutoScaling</li> <li>VPC</li> </ul> <p>At the same time I was caught off guard by hitting quite a few questions dedicated to detailed knowledge of the following services:</p> <ul> <li>API Gateway</li> <li>ECS</li> <li>Lambda</li> <li>CloudFormation</li> </ul> <p>My advice would be to pay a bit more attention to these topics, since they seem to appear frequently in the current versions of the CSA exam. And when I say \"detailed knowledge\", this means that questions were testing some configuration steps or your ability to identify the details about some service.</p> <p>Below I mention a few AWS concepts across various services that I did not master and was tested against, you better make sure to get familiar with these topics, since they are not normally stressed enough in the various prep courses.</p> <p>IAM I had a false sense of simplicity when read IAM chapters. It seemed like everything is obvious with IAM. But then I got a question about the AWS STS service and I was unprepared. Check this one out, don't be like me. So questions like temporary access are possible, and you need to know something about it.</p> <p>If you are not coming from Systems Administration background you might shiver when you see AD/LDAP abbreviations in the question (I do). With this said, check that you know how AD policies can be integrated with AWS IAM.</p> <p>Its very important to understand the huge importance of IAM Roles, I got the impression that I've been tested on this more than once. A tricky question was how can you let users from one AWS account access resources in another AWS account?</p> <p>EC2 Auto Scaling Elasticity and High Availability are the corner stones of a modern cloud-based application, therefore you will face lots of Auto Scaling questions. Most of them are basic, but the interesting one happened to appear to me.</p> <p>How does AS terminate the instance? In what order? Does it pick up a VM to terminate randomly or does it take into account active connections to the instance or maybe number of instances in the AZ? This is all about Default Termination Policy and my bad I did not pay attention to that specific angle of the EC2 Auto Scaling.</p> <p>S3 Normally S3 should come easy, aspects like Life-cycle policies and default data replication across AZs in a selected region must be emphasized by every course material. But ensure that you know what is S3 Cross-Region replication and why it might be needed.</p> <p>Don't fool yourself that S3 Encryption questions will avoid you. Be ready to answer few questions on that.</p> <p>EBS Do note, even though EBS in general might appear to you as an easy topic, AWS CSA tests you hard on EBS data encryption and protection. Spend few more hours on learning encryption options for EBS and the snapshots handling.</p> <p>Databases Yes, fair share of DB related questions, mostly about RDS and DynamoDB. Be prepared to answer the multi AZ deployment questions. And do keep in mind that AWS RDS does not let you to access the underlying Operating System.</p> <p>VPC Networks is my background, therefore VPC never got me much trouble. But for an Average Joe it would be nice to ensure that topics like NAT Gateway and VPN/Direct Connect are not the strangers.</p> <p>VPC endpoints concept is also something you need to know, as this might not stick in your memory from the first read.</p> <p>ECS I believe that ECS related questions appeared in the AWS CSA exam not that long ago, moreover, the course materials (if they were not updated recently) probably will not emphasize enough on that topic, but do expect to see some questions on ECS as well.</p> <p>The question that made me guess was something like the following: Can EC2 instances launched as the Ubuntu Linux servers be used as the Container Instances for the ECS?</p> <p>Cloudfront The CDN concepts is easy to understand, but details might be not articulated well in the courses. For instance, you should know rather well, what services can leverage the CloudFront distribution, i.e. what are the backend services that CloudFront can provide CDN services for?</p> <p>Also make sure that you understand what is CloudFront distribution and how to configure it. Specifically focus on Cache behaviors and Path Patterns. Note that the order of rules in the Path Patterns is crucial to the CloudFront, the <code>*</code> pattern should go after the more specific patterns!</p> <p>API Gateway To see &gt;1 question on that topic (and Lambda) was totally unexpected. I was under the impression that API GW will be tested on the very surface, but do expect some funny questions like the CORS problem and the API Gateway</p> <p>Lambda As with API Gateways, you will most probably see some Lambda questions thanks to the ever emerging Serverless concept. With Labmda its crucial to now about its scalability. Say you have a Lambda function provisioned to do some task with 150Mb of memory provisioned. If they ask you would you need to increase the memory per-function to handle the increased demand for that Lambda, the proper answer would be No, since AWS will scale this Lambda accordingly.</p> <p>Miscellaneous My least favorite part is some corner services like AWS EMR, Kinesis, Redshift, etc.</p> <p>For Kinesis you need to know the difference between the Kinesis Firehose and Kinesis Streams.</p> <p>For Redshift you need to have just a basic understanding about the sevice itself and when to choose it over RDS.</p> <p>Generic security and compliance questions are easy, Shared Responsibility model is all you need to know I think. Even if you fail few of these questions, it won't matter much.</p> <p>Do know how the role of <code>user-data</code> when provisioning EC2 instances and understand that it could be used for launching/passing scripts.</p> <p>AWS KMS was also on the list with some shady question I couldn't even recall. CloudHSM was not there, but it might be in your exam...</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#passing-score-for-aws-csa-exam","title":"Passing score for AWS CSA exam","text":"<p>AWS does not disclose what is the passing score, I hit 81% and passed, but seems like the threshold varies quite significantly, check this topic for various reports of a pass/fail marks. Some passed with 60%, others failed with 70%...</p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-aws-certified-solution-architect---associate/#watch-out-time-could-be-pressing","title":"Watch out, time could be pressing","text":"<p>Apparently, 80 minutes for 55 questions could be a problem. I had 20 minutes left when I finished the last question, but I saw lots of comments, when others failed to finish in time. Some questions have lengthy explanation, so you loose time by reading it once/twice, then you could loose some more time on ruling out the right options.</p> <p>So my suggestion would be to skip the questions you can't answer in 1.5-2 mins interval. You can come back to skipped questions when you will deal with the rest of them.</p> <p>Good luck with your exam!</p> <p></p>","tags":["aws","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/","title":"Prepping up for and passing the Certified Openstack Administrator Exam","text":"<p>Shortly after I passed AWS CSA exam I went on a hunt for the next certification to claim. Decided to tackle the Openstack COA certification first saving the Docker/Kubernetes certification for a later occasion.</p> <p>There is a common joke floating around: \"Oh, is Openstack still a thing?\" - yes, its pretty much still a thing, especially in the Telecom area where VMs are the only viable option for the most of the cases (think VNFs). Openstack also powers our teams public SDN lab that allows to provision a fully functional Nuage environment in a matter of minutes. So I wanted to get a better operational knowledge of Openstack to be able to support and tune the platform if necessary.</p> <p>Disclaimer: I took the recently updated COA exam which is based on the Openstack Pike release, although I did not face any Pike-specific questions during the exam. This does not mean that the exam content will stay the same throughout the course evolution, so watch out for the updates.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#key-takeaways","title":"Key takeaways","text":"<p>Before going into detailed explanation of the preparation steps and the questions I found most challenging to prepare for, check out this list of COA key takeaways:</p> <ul> <li>Although it is a practice, scenario-based exam, its an easy one. You can prepare for it without being exposed to a production-grade Openstack operations.</li> <li>Focus on the questions that can't be done via Horizon, as they would eat most of the exam time.</li> <li>Use the Horizon all the time (unless the task assumes CLI approach); if you are not a CLI jockey I ensure you, you will loose the precious time.</li> <li>Make use of the embedded notebook to mark the questions left unfinished; you can get back to them later if time permits.</li> <li>Verify the connection requirements, as the exams virtual-room environment is sensible to a browser version and connection quality.</li> <li>Think of a backup internet connection.</li> <li>Clean the desk completely!</li> </ul>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#preparation","title":"Preparation","text":"<p>The exam is a relatively easy one; compared to AWS CSA it has far less material to learn and being a scenario-based exam it doesn't expect you to memorize things as you can wander in the Horizon dashboard working out a correct solution for the task.</p> <p>I actually liked the scenario-based approach more, it tests your hands-on skills rather than the ability to hold a lot of theory in your head (which wears off really fast anyway).</p> <p>So how did I prepare for it? Being a LinuxAcademy user I first tried their COA courses, and they were completely useless. The course author spent 80% of the time in the CLI basically reading out load the CLI commands. Add on top the poor explanation of the basics and you get a perfect example of a bad course. So I do not recommend it to anyone.</p> <p>When LinuxAcademy failed me on that front I went looking for a preparation book, remembering that AWS CSA book was rather awesome. And found the one that I read cover-to-cover and can recommend to anyone looking for a single source of preparation: Preparing for the Certified OpenStack Administrator Exam by Matt Dorn. Here are the links where you can read/buy it:</p> <ul> <li>Amazon</li> <li>Safaribooks</li> <li>Packt</li> </ul> <p>In contrast to the online course I referred above, this book gives a really good theoretical background on the Openstack services that one would need to configure during the exam, supplementing it with the step-by-step configuration explanation. And yes, it also comes with a VirtualBox-based Openstack installation with pre-configured scenarios for each chapter.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#horizon-all-the-way","title":"Horizon all the way","text":"<p>Although every exam study guide will most likely present you with the GUI and CLI ways of solving a particular task my recommendation is to solve everything in the Horizon, leaving CLI-specific tasks to the CLI. The reason is simple, a regular user will configure things much faster within Horizon dashboard, rather than copy-pasting UUIDs in an unfriendly CLI emulator.</p> <p>There are tasks that can only be done within the CLI and these are the only ones that I recommend to solve with it.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#cli-only-tasks","title":"CLI-only tasks","text":"<p>There are tasks that has no way around but using the CLI to crack them. Tasks that require to interact with the objects like:</p> <ul> <li><code>domains</code></li> <li><code>endpoints</code>,</li> <li>Downloading <code>glance</code> images</li> <li>Managing <code>swift</code> ACL rules and expiration dates</li> </ul> <p>will require you to open the CLI, but these are the only tasks that will require it. The rest can be done much faster within the Horizon dashboard.</p> <p>On the other hand I really encourage you to focus on these tasks, especially on the Swift's ACLs and object expiration tasks. Swift is the only service that will require you to use the <code>swift</code> CLI client instead of a common <code>openstack</code> CLI client. And to make your life harder there is no built-in help for <code>swift</code> commands to manage ACL and expiration, so you need to memorize the exact commands.</p> <p>I also strongly suggest to pay additional attention to the tasks that test your ability to work with and analyze the Openstack Logs. You might make a mistake skipping it over, or paying a little attention to it.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#troubleshooting-tasks","title":"Troubleshooting tasks","text":"<p>Across the 40 scenarios that you would see during your exam there would be a troubleshooting section. As explained in the book I shared above you (most likely) will not see anything harder than the communication problem for a set of VMs. And the golden there rule is to check the Security Group rules to see if the rule is there and the protocol that must be working is set in the policy.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#taking-the-exam","title":"Taking the exam","text":"<p>Now this is very important, if you dont want to be derailed by the proctor, make sure of the following:</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#desk-stays-clean","title":"Desk stays clean","text":"<p>Clean the desk completely. I mean completely, nothing is allowed to be on the desk. The proctor gave me hard time asking me to clean the room and the desk. For the moment I thought that the proctor is actually my wife.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#backup-your-internet-link","title":"Backup your internet link","text":"<p>If possible, have a backup internet connection. During the exam you would need to share your desktop, if you have &gt;1 monitors you would need to share them all. For some mysterious reason, my perfect internet connection had been flagged as slow by the LinuxFoundation proctor and they asked me to provide another one, since the desktop stream was lagging.</p> <p>Now this could caught you off-guard, I ended up setting up a mobile hot spot, so think about the backup.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#intermittent-connectivity-is-ok","title":"Intermittent connectivity is OK","text":"<p>Connection drops happened to me more than once; dont be scared, this is OK. You will be able to get back where you stopped, although the time could be only adjusted by the proctor.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#using-the-integrated-notebook","title":"Using the integrated notebook","text":"<p>The exam environment allows you to use the integrated notebook. I used it to mark the questions that I left/skipped so I could come back to them later. Yes, you got it right, the exam environment does not mark which questions were unanswered, so you need to write down the question numbers for those which you unfinished.</p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#ps","title":"PS","text":"<p>Go and pursue the COA certification, its not the easiest one out there, but certainly its not one of the toughest. Depending on your exposure to the Openstack basics you can prepare only for the rare things like <code>swift</code> expiration configuration and maybe some other CLI-only tasks and sit the exam. Since it sports one free re-take, you can safely get a taste of it and probably pass it from the first attempt. Good luck!</p> <p></p>","tags":["openstack","certification"]},{"location":"2018/prepping-up-for-and-passing-the-certified-openstack-administrator-exam/#useful-links","title":"Useful links","text":"<ol> <li>Preparing for the Certified OpenStack Administrator Exam</li> <li>Preparing to COA Git repo</li> </ol>","tags":["openstack","certification"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/","title":"Uploading multiple files to AWS S3 in parallel","text":"<p>Have you ever tried to upload thousands of small/medium files to the AWS S3? If you had, you might also noticed ridiculously slow upload speeds when the upload was triggered through the AWS Management Console. Recently I tried to upload 4k html files and was immediately discouraged by the progress reported by the AWS Console upload manager. It was something close to the 0.5% per 10s. Clearly, the choke point was the network (as usual, brothers!).</p> <p>Comer here, Google, we need to find a better way to handle this kind of an upload.</p> <p>To set a context, take a look at the file size distribution I had (thanks to this awk magic):</p> <pre><code>  Size, KB   Num of files\n       256   2\n       512   2\n      1024   8\n      2048 1699\n      4096 1680\n      8192 579\n     16384 323\n     32768 138\n     65536  34\n    131072   6\n    262144   1\n   1048576   1\n   2097152   1\n   4194304   1\n</code></pre> <p>My thought was that maybe there is a way to upload a tar.gz archive and unpack it in an S3 bucket, unfortunately this is not supported by the S3. The remaining options were (as per this SO thread):</p> <ol> <li> <p>You could mount the S3 bucket as a local filesystem using s3fs and FUSE (see article and github site). This still requires the files to be downloaded and uploaded, but it hides these operations away behind a filesystem interface.</p> </li> <li> <p>If your main concern is to avoid downloading data out of AWS to your local machine, then of course you could download the data onto a remote EC2 instance and do the work there, with or without s3fs. This keeps the data within Amazon data centers.</p> </li> <li> <p>You may be able to perform remote operations on the files, without downloading them onto your local machine, using AWS Lambda.</p> </li> </ol> <p>Hands down, these three methods could give you the best speeds, since you could upload tar archive and do the heavy lifting on the AWS side. But none of them were quite appealing to me considering the one-time upload I needed to handle. I hoped to find kind of a parallel way of the multiple uploads with a CLI approach.</p> <p>So what I found boiled down to the following CLI-based workflows:</p> <ol> <li><code>aws s3 rsync</code> command</li> <li><code>aws cp</code> command with <code>xargs</code> to act on multiple files</li> <li><code>aws cp</code> command with <code>parallel</code> to act on multiple files</li> </ol> <p>TL;DR: First option won the competition (# of cores matters), but lets have a look at the numbers. I created 100 files 4096B each and an empty test bucket to do the tests:</p> <pre><code># create 100 files size of 4096 bytes each\nseq -w 1 100 | xargs -n1 -I% sh -c 'dd if=/dev/urandom of=file.% bs=1 count=4096'\n</code></pre> <pre><code>$ find . -type f -print0 | xargs -0 ls -l | awk '{size[int(log($5)/log(2))]++}END{for (i in size) printf(\"%10d %3d\\n\", 2^i, size[i])}' | sort -n\n\n      4096 100\n</code></pre>","tags":["aws"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/#1-aws-management-console","title":"1. AWS Management Console","text":"<p>As a normal human being I selected all these 100 files in the file dialog of the AWS Management Console and waited for 5 minutes to upload 100 of them. Horrible.</p> <p>The rest of the tests were run on an old 2012 MacBook Air with 4vCPUs.</p>","tags":["aws"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/#2-aws-s3-sync","title":"2. aws s3 sync","text":"<p>A <code>aws s3 sync</code> command is cool when you only want to upload the missing files or make the remote part in sync with a local one. In case when a bucket is empty a sequential upload will happen, but will it be fast enough?</p> <pre><code>time aws s3 sync . s3://test-ntdvps\n\nreal 0m10.124s\nuser 0m1.470s\nsys 0m0.273s\n</code></pre> <p>10 seconds! Not bad at all!</p>","tags":["aws"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/#3-aws-s3-cp-with-xargs","title":"3. aws s3 cp with xargs","text":"<pre><code>ls -1 | time xargs -I % aws s3 cp % s3://test-ntdvps\n294.05 real        68.76 user         9.27 sys\n</code></pre> <p>5 mins! As bad as the AWS Management Console way!</p>","tags":["aws"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/#4-aws-s3-cp-with-parallel","title":"4. aws s3 cp with parallel","text":"<p><code>parallel</code> is a GNU tool to run parallel shell commands.</p> <pre><code># parallel with 60 workers\nls -1 | time parallel -j60 -I % aws s3 cp % s3://test-ntdvps --profile rdodin-cnpi\n39.32 real       108.41 user        14.46 sys\n</code></pre> <p>~40 seconds, better than <code>xargs</code> and worse than <code>aws s3 sync</code>. With an increasing number of the files <code>aws s3 sync</code> starts to win more, and the reason is probably because <code>aws s3 sync</code> uses one tcp connection, while <code>aws s3 cp</code> opens a new connection for an each file transfer operation.</p>","tags":["aws"]},{"location":"2018/uploading-multiple-files-to-aws-s3-in-parallel/#5-what-if-i-had-some-more-cpu-cores","title":"5. What if I had some more CPU cores?","text":"<p>You can increase the number of the workers, and if you have a solid amount of threads available you might win the upload competition:</p> <pre><code># 48 Xeon vCPUs, same 100 files 4KB each\n\naws s3 sync: 6.5 seconds\naws s3 cp with parallel and 128 jobs: 4.5 seconds\n\n# now 1000 files 4KB each\naws s3 sync: 40 seconds\naws s3 cp with parallel and 252 jobs: 21.5 seconds\n</code></pre> <p>So you see that the <code>aws s3 cp</code> with <code>parallel</code> might come handy if you have enough of vCPUs to handle that many parallel workers. But if you are sending your files from a regular notebook/PC the <code>aws s3 sync</code> command will usually be of a better choice.</p>","tags":["aws"]},{"location":"2018/saturating-the-network-with-ftp/","title":"Saturating the network with FTP","text":"<p>While working on the Ipanema Wan Opt VNF integration with Nuage Networks I stumbled upon an interesting case which required to max out the network with FTP traffic. The tricky point there was to create the FTP connection which won't be limited by the disk IO performance. Especially, considering that the disks were kind of slow in the setup I had.</p> <p>It turns out, you can use the in-memory devices in the FTP communication path <code>/dev/zero -&gt; /dev/null</code>, ruling out the slowliness that could have been added by the disks. Lets figure out how to do that!</p> <p>Software-wise my setup consisted of a single FTP server <code>vsftpd</code> and the FTP client <code>ftp</code> all running on Centos7-based VMs. These VMs were equipped with a network namespace <code>ns-data</code> which host the datapath interface <code>eth1</code>.</p>","tags":["vsftpd","ftp"]},{"location":"2018/saturating-the-network-with-ftp/#devzero-devnull","title":"/dev/zero -&gt; /dev/null","text":"<p>I found this \"ftp to dev null to test bandwidth\" blog post explaining how to use <code>/dev/zero</code> as a source file and <code>/dev/null</code> as a destination within a running FTP session.</p> <p>The example there (executed on the FTP client side) demonstrates the following technique:</p> <pre><code>#!/bin/bash\n/usr/bin/ftp -n &lt;IP address of machine&gt; &lt;&lt;END\nverbose on\nuser &lt;username&gt; &lt;password&gt;\nbin\nput \"|dd if=/dev/zero bs=32k\" /dev/null\nbye\nEND\n</code></pre> <p>So this example did not work for me right out of the box so let me augment it with the few findings I came across while trying to make this one work.</p>","tags":["vsftpd","ftp"]},{"location":"2018/saturating-the-network-with-ftp/#vsftpd-configuration","title":"vsftpd configuration","text":"<p>The <code>put \"|dd if=/dev/zero bs=32k\" /dev/null</code> command is the transfer operation from the client to the server. On the server side the data that comes from the client is saved in <code>/dev/null</code> device.</p> <p>First thing to check there is that your FTP server configuration allows a client to use the <code>dev/null</code> device as the destination. I used the <code>vsftpd</code> as a server, so the config that worked for me (using the local user authentication) is as follows:</p> <pre><code># cat /etc/vsftpd/vsftpd.conf\nanonymous_enable=NO\nlocal_enable=YES\nwrite_enable=YES\nanon_upload_enable=YES\nanon_mkdir_write_enable=YES\nanon_other_write_enable=YES\nanon_world_readable_only=YES\nconnect_from_port_20=YES\nhide_ids=YES\npasv_min_port=40000\npasv_max_port=60000\nlocal_umask=022\ndirmessage_enable=YES\nxferlog_enable=YES\nxferlog_std_format=YES\nlisten=YES\nxferlog_enable=YES\nls_recurse_enable=NO\nascii_download_enable=NO\nasync_abor_enable=YES\none_process_model=NO\nidle_session_timeout=120\ndata_connection_timeout=300\naccept_timeout=60\nconnect_timeout=60\npam_service_name=vsftpd\ntcp_wrappers=YES\n</code></pre> <p>This enables me to authenticate using the local user credentials on the server and write the data to the <code>/dev/null</code> device.</p>","tags":["vsftpd","ftp"]},{"location":"2018/saturating-the-network-with-ftp/#500-oops-ftruncate","title":"500 OOPS: ftruncate","text":"<p>Once I found the workable vsftpd config, I run the script and received <code>500 OOPS: ftruncate</code> error from the server. This problem, as it seems, only affects RHEL-based distros, and as explained here the workaround is to use <code>append</code> command instead of a <code>put</code>.</p> <p>This brings me to the final version of the script I used:</p> <pre><code># cat ./ftp.sh\n#!/bin/bash\nip netns exec ns-data /usr/bin/ftp -n 192.168.99.101 &lt;&lt;END\nverbose on\nuser myuser mypassword\nbin\nappend \"|dd if=/dev/zero bs=32k\" /dev/null\nbye\nEND\n</code></pre> <p>And the result I got on the 10Mbps uplinks:</p> <pre><code>$ sudo bash ftp.sh\nVerbose mode on.\n331 Please specify the password.\n230 Login successful.\n200 Switching to Binary mode.\nlocal: |dd if=/dev/zero bs=32k remote: /dev/null\n227 Entering Passive Mode (192,168,99,101,222,205).\n150 Ok to send data.\n^C277+0 records in\n276+0 records out\n9043968 bytes (9.0 MB) copied, 7.06165 s, 1.3 MB/s\n\nsend aborted\nwaiting for remote to finish abort\n226 Transfer complete.\n8986624 bytes sent in 7.03 secs (1278.00 Kbytes/sec)\n221 Goodbye.\n</code></pre> <p>And this saturates my uplinks completely.</p>","tags":["vsftpd","ftp"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/","title":"How to download an M3U/M3U8 playlist (stream) without a special software","text":"<p>I love the tech conferences that share the recordings of the sessions without hiding behind the registration or a pay wall. Luckily the trend to share the knowledge with a community openly is growing, yet you still can find a nice talk hidden behind the above mentioned walls.</p> <p>Sometimes an easy registration is all it takes, but then, how do you watch it offline? For example, I do love to catch up with with the recent trends and experiences while being on a plane, meaning that I just cant afford to be hooked up to the Internet.</p> <p>If a talk is published on the YouTube, you are good to go and download it with any web service that pops up in the google search by the term \"Youtube download\". But what do we do when the video is hosted somewhere in the CDN and is served as a dynamic playlist of <code>*.ts</code> files?</p> <p>Here I share with you an easy way to download the videos from an m3u/m3u8 playlist.</p> <p>The dynamic playlist format - M3U/M3U8 - is a way to tell the browser how to download the pieces of the video that will comprise the whole recording. So the download process is actually as easy as:</p> <ol> <li>Get the m3u8 link</li> <li>Download every file from that playlist and glue them into a single video.</li> </ol>","tags":["m3u8","video","youtube-dl"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/#getting-the-playlist-url","title":"Getting the playlist URL","text":"<p>Now the first part is easy, you go to the page where a vide player is rendered and search for the <code>m3u8</code> file using the developers tools console of your browser.</p> <p></p> <p>Make sure to get the master playlist request url and copy it in your clipboard.</p>","tags":["m3u8","video","youtube-dl"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/#video-quality","title":"Video quality","text":"<p>In the master playlist body you can see the different versions of the playlists, typically they differ with the quality settings. Consider the following m3u8 file contents:</p> <pre><code>#EXTM3U\n#EXT-X-VERSION:4\n#EXT-X-STREAM-INF:PROGRAM-ID=0,BANDWIDTH=180400,CODECS=\"mp4a.40.2,avc1.4d001e\",RESOLUTION=720x294,AUDIO=\"audio-0\",CLOSED-CAPTIONS=NONE\nhttps://manifest.prod.boltdns.net/...\n#EXT-X-STREAM-INF:PROGRAM-ID=0,BANDWIDTH=335500,CODECS=\"mp4a.40.2,avc1.4d001f\",RESOLUTION=1200x490,AUDIO=\"audio-1\",CLOSED-CAPTIONS=NONE\nhttps://manifest.prod.boltdns.net/...\n</code></pre> <p>The first cropped link is for the playlist with 720x294 resolution, whereas the second one is a HQ version with \"1200x490\" stream. If you see that for some reason you are downloading the low quality stream, extract the HQ stream URL and use it instead of the master playlist URL.</p>","tags":["m3u8","video","youtube-dl"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/#downloading-the-files","title":"Downloading the files","text":"","tags":["m3u8","video","youtube-dl"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/#with-vlc","title":"with VLC","text":"<p>The title of this post says \"... with no special software\", yet we will use the VLC player here which I deliberately categorize as a software that everyone can get on every platform, so its not a special software.</p> <p>What you need to do next is to choose File -&gt; Open Network dialog and paste the URL of the m3u8 playlist from the prev. step. Now you can either play it in the VLC right away, or check the Stream Output checkbox and click Settings.</p> <p></p> <p>This will open a new dialog where you can choose:</p> <ul> <li>the path to a resulting video file</li> <li>the video container format</li> <li>and, optionally, the audio/video codecs if you want to do transcoding</li> </ul> <p></p> <p>Click Ok and the files will start to download and encode in your resulting video container by the path you specified. This is not a particularly fast process, so just wait till the progress bar reaches its end and enjoy the video!</p>","tags":["m3u8","video","youtube-dl"]},{"location":"2019/how-to-download-an-m3um3u8-playlist-stream-without-a-special-software/#with-youtube-dl","title":"with youtube-dl","text":"<p>The VLC-way is good for a one-time quick download, but if you have a list of playlists you want to download, then youtube-dl python tool is just unmatched. Judging by the name, the tool was developed for youtube downloads originally but outgrew it quickly enough to be a swiss knife for online video downloads.</p> <p>You can use a container image or install it as a python package or as a pre-compiled binary, so the installation is really a breeze and won't take long. Additionally, the tool brings an endless amount of features:</p> <ul> <li>automatically detect playlist URL by crawling the HTML page (no need to manually look for m3u8 URL)</li> <li>cli interface to scriptify bulk downloads</li> <li>extensive encoding support via ffmpeg and aconv</li> <li>filtering and sorting for videos in the playlists (if the playlist has more than one vide, i.e. Youtube playlist)</li> <li>and many more.</li> </ul> <p>For example, to download a video that you would normally watch at <code>http://example.com/vid/test</code> a single CLI command is all it takes:</p> <pre><code># name the output file test.&lt;container_format&gt;\nyoutube-dl -o 'test.%(ext)s' --merge-output-format mkv http://example.com/vid/test\n</code></pre> <p>and the rest is handled by the marvelous youtube-dl:</p> <pre><code>[download] Downloading playlist: Search query\n[Search] Downloading search JSON page 1\n[Search] Downloading search JSON page 2\n[Search] Downloading search JSON page 3\n[Search] playlist Search query: Downloading 75 videos\n[download] Downloading video 1 of 75\n[author:new] 6047188571001: Downloading webpage\n[author:new] 6047188571001: Downloading JSON metadata\n[author:new] 6047188571001: Downloading m3u8 information\n[author:new] 6047188571001: Downloading m3u8 information\n[author:new] 6047188571001: Downloading MPD manifest\n[author:new] 6047188571001: Downloading MPD manifest\n[hlsnative] Downloading m3u8 manifest\n[hlsnative] Total fragments: 245\n[download] Destination: test.fhls-430-1.mp4\n[download]  69.0% of ~81.31MiB at 577.84KiB/s ETA 01:57\n</code></pre> <p>Sometimes, though, you can't just specify the URL of a page where the player is normally loaded in your browser, due to the cookies presented in your browser and who knows what black magic this frontenders invented while we were not watching. Then you still need to manually fetch the m3u8 link and feed it to the <code>youtube-dl</code>. The rest stays the same, the tool will handle the download/encoding process in the most effective and pleasant way.</p> <p>Note, you also might need to download the <code>ffmpeg</code> for youtube-dl to merge the different streams in a single container. Anyway, <code>youtube-dl</code> will tell you if its the case for you.</p> <p>Leveraging containers is my preferred way of working with tools, since all the pesky deps are packaged neatly inside, for youtube-dl you can use <code>mikenye/youtube-dl</code> image, which will make using youtube-dl a breeze:</p> <pre><code>\u276f alias yt-dl='docker run \\\n                  --rm -i \\\n                  -e PGID=$(id -g) \\\n                  -e PUID=$(id -u) \\\n                  -v \"$(pwd)\":/workdir:rw \\\n                  mikenye/youtube-dl'\n\n\u276f yt-dl -o 'test.%(ext)s' --merge-output-format mkv http://example.com/vid/test.m3u8\n</code></pre>","tags":["m3u8","video","youtube-dl"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/","title":"Creating a Bootstrap based front-end for your simple REST service","text":"<p>Not a single day goes by without me regretting I haven't mastered any front-end technology like React/Angular or the likes.  Why would a network engineer want to step into the game that seems orthogonal to its main area of expertise, one might ask?</p> <p>Truth be told, I wasn't born with an urge to learn anything that has javascript under the hood, but over the years, working within the network/backend silos, I realized, that being able to create a simple front-end service is a jewel that fits every crown, no matter what title you wear.</p> <p>This tutorial is based on the task real task of building up a web interface (pycatjify.netdevops.me) for the <code>pycatjify</code> REST API service deployed as a serverless function. The end result is a simple, completely free and reusable Bootstrap based front-end boilerplate which can be used as a foundation for a similar task.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#1-benefits-of-knowing-how-to-front-end","title":"1 Benefits of knowing how to front-end?","text":"<p>Lets me first explain why I think that even a basic experience with any front-end technology is beneficial to virtually anyone.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#11-get-your-tool-a-web-interface","title":"1.1 Get your tool a web interface","text":"<p>We often start with an idea of a tool and work it to a completion by publishing a command line interface to it, sometimes the CLI is all the tool needs, it is just best consumed that way. Other times even the CLI is not needed, as the tool is only used as a plugged-in library.</p> <p>But quite often the tool can benefit greatly by having its own web interface. You can broaden the horizons of your project audience vastly by simply creating a web service out of it. I can name a handful number of tools that I consume via web instead of using their CLI counterparts, it is just more convenient to me and so might think the users of your tools.</p> <p>The <code>pycatj</code> is a perfect example of a CLI-first tool that can be conveniently consumed via web as well. Thus I set myself on a journey to create a web facade for it and at the same time reinforcing my very basic web skills.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#12-take-your-pitch-or-demo-to-a-next-level","title":"1.2 Take your pitch or demo to a next level","text":"<p>Not everyone of us is working in an environment where the bosses have engineering background and can equally enjoy a demo of a new service by looking at the terminal full of <code>curl</code> requests. Even if your bosses are the ones who contribute to the cutting edge technologies, your customers can easily be made of a different dough.</p> <p>Therefore it might be equally important to supplement your neat idea with a proper visualization; my experience says that a great tool or a service attracts audience way better when it is wrapped in a shiny package. So having a prototyped web UI might give you some bonus points even if it is not going to be consumed via the Web UI after all.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#13-learn-how-they-do-it-on-the-other-side-of-a-fence","title":"1.3 Learn how they do it on the other side of a fence","text":"<p>A classic, book-pictured network automation engineer is an all Python-shop customer. Although Python is a natural fit for the network automation activities, it is also important to not less yourself be constrained by a singe language or a toolchain.</p> <p>Educating yourself on a different technology with a different set of the instruments and/or the views means a lot. Even by scratching the surface of the Javascript, its package managers and the front-end frameworks could make you better understand the pros and cons of the ecosystem you are in.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#2-front-end-javascript-frameworks","title":"2 Front-end &amp; Javascript frameworks","text":"<p>So how do one start if they want to learn any of that shiny front-end witchery given that there are so many frameworks around? In spite to answer this question I compiled the following list of options that when I approached the task of making a <code>pycatjify</code> web service:</p> <ol> <li> <p>Frameworkless: bare HTML/CSS/JS This is the most straightforward way of creating a web service. You basically write everything by yourself without relying on any framework. On the pros side this is the most lightweight and bloat-less approach, as you are in the full control of what contributes to the end result. The cons side is substantial though, you need to be well experienced in the HTML/CSS/JS to create something less minimalistic than a blank page with the elements on it.</p> </li> <li> <p>Front-end frameworks Front-end frameworks provide a shortcut for a web service creation drastically reducing time to create one. Also known as CSS frameworks they come across with the lego-like blocks (components implemented with CSS/JS/HTML) that you use to build a web service from. Dozens of front-end frameworks have been created over the time, from the minimalistic ones to the monstrous software bundles. Bootstrap, Foundation, Skeleton, Materialize are one of the few that one can find in the numerous \"top front-end frameworks\" charts. A major benefit that all above mentioned frameworks share is that they don't need to be compiled and can be run by all modern browsers. All it takes is to put the framework' HTML/CSS/JS files along with your project and open the <code>index.html</code>.</p> </li> <li> <p>Javascript frameworks and libraries: React/Angular/Vue/etc These are the modern age Javascript frameworks (often referred as libraries) that enable you to build modern web/mobile applications with a feature-rich logic. With the great power, though, comes the great size and complexity; installing a sample React app can easily add thousands of JS packages that framework depends on. The learning curve for these frameworks is steep as opposed to the front-end frameworks listed in [2]. But mastering one of them would enable you to create versatile and breathtaking Web UIs as well as mobile applications. Notable frameworks in that category are React, Vue, Angular, Ember.</p> </li> </ol> <p>Since I am not a front-end developer the sweet spot for me lies with the front-end frameworks that I can install/run without a specific harness. They are lightweight, easy to work with, and all it takes to start is the basic HTML/CSS/JS knowledge. At the same time they provide just enough features to handle not overly complicated tasks a network engineer might encounter in a small size projects.</p> <p>For the pycatjify.netdevops.me I decided to use a \"Material Design\" flavored Bootstrap based framework called mdbootstrap.</p> <p>Also I had some past experience with the Bootstrap 3 framework when I worked on a Web UI for the python scripts quite some time ago.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#3-mdbootstrap","title":"3 Mdbootstrap","text":"<p> Mdbootstrap (MDB) offers the Material-UI themed components for the various JS frameworks such as Bootstrap/JQuery, Angular, React and Vue. For the reasons outlined in section 2, I decided to go with a Bootstrap/JQuery version of the MDB framework as this is the easiest way to put up a simple front-end service for me.</p> <p>MDB offers a free quick start guide as well as a full-length tutorial if you want to refresh the bootstrap basics or follow an authored paid course.</p> <p>Bootstrap popularity also makes it extremely easy to find a lot of guides and tutorials that tremendously help to understand the basics of this framework.</p> <p>MDB, being based on the Bootstrap 4, obviously follows its ancestors paradigms when it comes to the Grid system, CSS, components, etc. If you worked with the Bootstrap before then the MDB won't be a problem at all. Moreover, the elements I used in this project are not MDB specific, the same components are available in the original Bootstrap library.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#31-install-the-framework","title":"3.1 Install the framework","text":"<p>Its extremely easy to \"install\" mdbootstrap/bootstrap framework, its hardly an installation even, as all you need is to download the archive and extract the framework' files. Once done, the framework contents is nothing more than a small number of the folders and files:</p> <pre><code>.\n\u251c\u2500\u2500 [drwxr-xr-x]  css\n\u251c\u2500\u2500 [drwxr-xr-x]  font\n\u251c\u2500\u2500 [drwxr-xr-x]  img\n\u251c\u2500\u2500 [drwxr-xr-x]  js\n\u251c\u2500\u2500 [drwxr-xr-x]  scss\n\u2514\u2500\u2500 [-rwxr-xr-x]  index.html\n\n5 directories, 4 files\n</code></pre> <p>Yes, thats literally all you need, no packages installation no dependency management, just static files, classy! You can open the <code>index.html</code> with your browser and it'll just work.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#32-framework-structure","title":"3.2 Framework structure","text":"<p>The framework comes with the following important components that make it all work in a unison to display the web page built with it:</p> <ul> <li>CSS files in the <code>css</code> folder that define the styling of the framework elements and controls</li> <li>Javascript files in the <code>js</code> folder that comprise the dynamic logic that the framework relies on</li> <li>Static images in the <code>img</code> folder as well as the fonts in the <code>font</code> directory</li> <li>Index HTML file that in a simple case will have all the website contents</li> </ul> <p>Take a look at the <code>index.html</code> file that comes with a framework:</p> <p>In the <code>&lt;head&gt;</code> section of this HTML file the CSS files are being loaded. These CSS files comprise a big portion of the framework itself, as they govern the styling that the components have.</p> <p>Then the <code>&lt;body&gt;</code> section of the HTML file holds the default web page's content. The <code>index.html</code> file that comes with a template has a <code>div</code> with a few headers and a paragraph of text. You will replace then with the framework components like Navigation bars, input forms, tables, text elements, modal dialogs when you start to build your front-end service.</p> <p>In the ending of the <code>&lt;body&gt;</code> section you would find the <code>&lt;script&gt;</code> elements that load the Javascript code the framework relies on. The custom JS code that your service most likely will have will also be added in the body's tail section (see section 4 for an example).</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#33-bootstrap-components-are-your-lego-blocks","title":"3.3 Bootstrap components are your lego blocks","text":"<p>The framework's library has a lot of components that might be treated like the lego blocks with which you build the web facade. The benefit of having the pre-created components is huge; you don't need to create these common components yourself from the ground up, just browse the library and pick the right ones.</p> <p> Example of the tabs &amp; pills components</p> <p>To understand which components I'd need for the pycatjify I imagined what layout would I want my page to have. Since <code>pycatj</code> is a tool that works on an input JSON/YAML data and produces a multi line output, the simple layout could consist of a navigation bar with the project logo, the two input fields for input and output data and the modal dialog with cards.</p> <p>Knowing the needed basic building blocks we can now browse the framework' documentation section in search for the right ones. The MDB docs are just great for that - lots of examples on how to use the various components in different kinds of shapes and sizes. Basically you copy the example from the docs, paste it to your HTML file and tune it as per the components options which are explained in the docs.</p> <p>When building pycatjify front-end I just removed the default contents of the <code>&lt;body&gt;</code> section of the <code>index.html</code> file and started to throw in the components as per the layout I had in my head. Thats what the <code>index.html</code> for the pycatjify.netdevops.me started to look like when I added all the components I talked above.</p> <p>It looks like a lot of lines of code, but everything was just pasted from the examples section. First time it takes some time to get to know the components and their behavior, but do it once and the next project would be an effortless task.</p> <p> pycatjify web ui</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#4-hooking-up-the-back-end","title":"4 Hooking up the back-end","text":"<p>As implied by the name of this post, the communication between the front-end and the back-end is happening using the REST API. In the previous post I wrote about the way I packaged the <code>pycatj</code> tool into a Google Cloud Function which exposes a single API endpoint. Now it is time to make our front-end to be a REST API client that talks to the back-end and displays the results it receives back.</p> <p>This is a breakdown of a communication logic between the front and back ends:</p> <ol> <li>Capture the user input (which is a YAML or JSON formatted text) from the input field</li> <li>Send it via <code>HTTP POST</code> request to the back-end API endpoint in a JSON format</li> <li>Back-end to receives a request and does the transformation of the received data</li> <li>It then sends the transformed data back as a string packed in a JSON body as well.</li> </ol>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#41-rest-api-client-with-jqueryajax","title":"4.1 REST API client with JQuery/AJAX","text":"<p>There are several ways of making an asynchronous HTTP request from within the front-end service. One approach would be by using the JQuery's AJAX function. Since MDB framework has JQuery as its dependency and this library is already loaded into our page we can use it right away.</p> <p>Lets add a Javascript file at the <code>js/pycatjify.js</code> path that will implement the logic of a REST client.</p> <p>This little unnamed function is bound to the button with id <code>convert_btn</code> by the means of the <code>#convert_btn</code> selector. Specifically to its <code>click</code> action. That means that when a click action occurs on the <code>convert_btn</code> button, this JS code kicks in.</p> <p>In the very beginning the code reads the data from the input element text area into the <code>data[\"pycatj_data\"]</code> object. Next, it serializes the variable value into the JSON string since we chose to use JSON payload with our POST request.</p> <p>And then the actual AJAX request (which is essentially a JQuery name for the async HTTP call) comes into play:</p> <pre><code>$.ajax({\n    url: \"https://us-central1-pycatj.cloudfunctions.net/pycatjify\",\n    contentType: \"application/json\",\n    data: body,\n    dataType: \"json\",\n    type: 'POST',\n    success: function (response) {\n        $('#out_form').val(response.data)\n    }\n});\n</code></pre> <ul> <li>With the <code>url</code> parameter we say what is the URL of our REST API endpoint</li> <li><code>contentType</code> set to <code>application/json</code> narrows down the type of the content we will convey through HTTP messages</li> <li>the <code>data</code> that we send with this specific request is contained in the <code>body</code> variable computed before</li> <li><code>dataType: \"json\"</code> allows us to treat the returned response as the JSON object, and since our pycatj serverless function returns the JSON it is exactly what we need.</li> <li>the request <code>type</code> is <code>POST</code></li> </ul> <p>If our POST request succeeds and we receive a <code>response</code>, we call a function that displays the results received as a JSON. Because of our serverless function returns the data in a <code>data</code> field, we select this field with the <code>response.data</code> selector in the <code>$('#out_form').val(response.data)</code> expression.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#5-hosting-the-web-application","title":"5 Hosting the web application","text":"<p>Since our back-end code is hosted by a GCP Function, the front-end itself is nothing more than a bunch of static files (CSS, JS, HTML), and that means that it can easily be hosted for free with the beautiful Gitlab Pages service.</p> <p>For that I added a <code>.gitlab-ci.yml</code> file that has a single <code>pages</code> job responsible for copying the web service related files to the <code>public</code> directory which, in its turn, tells Gitlab to start serving these files over HTTP.</p> <p>Now with every push to the master branch Gitlab will restart the web server to ensure that the most recent files are being served.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-a-bootstrap-based-front-end-for-your-simple-rest-service/#6-summary","title":"6 Summary","text":"<p>This pretty much concludes the Minimum Viable Product of the web front-end for the simple REST API service:</p> <ul> <li>by leveraging the Google Cloud Platform Functions we deployed a python code that implements a back-end REST API service - $0</li> <li>the front-end is built with a simple Bootstrap/JQuery based MDB framework and hosted with Gitlab Pages - $0</li> <li>the wildcard TLS certificate is provided by Cloudflare - $0</li> </ul> <p>As you see, the process of putting a simple front-end service is simple and completely free. It goes without saying, that the example presented in this topic uses a very basic layout and a straightforward design - hence the overall simplicity. For instance the it does not handle any errors and does not perform input validation. Adding the spinner element to the UI to indicate the processing time would also enhance the UX. You can imagine, that adding all of these features increases the complexity of the code base and might require additional components and/or libraries.</p> <p>I hope this \"how to create a front-end being not a front-ender\" post helps you with the basics of a simple front-end machinery. Remember, its important to start, and its easier to start small, you can always grow later. And I think the Bootstrap-like frameworks are a good choice for that.</p> <p>Checkout the project's source code and leave the comments or ask questions below if I missed something.</p>","tags":["javascript","frontend","bootstrap"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/","title":"Creating Google Cloud Platform Function with Python and Serverless","text":"<p>Two years ago I shared my experience on building the AWS Lambda function for a python project of my own. And a few days ago I stumbled upon a nice opensource CLI tool that I immediately wanted to transform in a web service.</p> <p></p> <p>Naturally, a simple, single-purpose tool is a perfect candidate for function-as-a-service (FaaS), and since I had past experience with AWS Lambda, this time I decided to meet its Google's sibling - Google Cloud Function.</p> <p>In this post we'll discover how to take a python package with 3<sup>rd</sup> party dependencies, make a GCP Function from it and deploy it without a single click in the UI - all without leaving the IDE.</p> <p>[Project's source code]</p> <p>The python tool I considered a natural fit for a Cloud Function is a <code>pycatj</code> by David Barroso that he released just recently.</p> <p><p>For those with problems accessing yaml/json data: https://t.co/IbbS3x05bq</p>\u2014 David Barroso (@dbarrosop) June 26, 2019 </p> <p>This tool helps you to map a JSON/YAML file to a Python dictionary highlighting the keys you need to access the nested data:</p> <pre><code>$ cat tests/data/test_1.json\n{\n    \"somekey\": \"somevalue\",\n    \"somenumber\": 123,\n    \"a_dict\": {\n        \"asd\": \"123\",\n        \"qwe\": [1, 2, 3],\n        \"nested_dict\": {\n            \"das\": 31,\n            \"qwe\": \"asd\"\n        }\n    }\n}\n\n$ pycatj --root my_var tests/data/test_1.json\nmy_var[\"somekey\"] = \"somevalue\"\nmy_var[\"somenumber\"] = 123\nmy_var[\"a_dict\"][\"asd\"] = \"123\"\nmy_var[\"a_dict\"][\"qwe\"][0] = 1\nmy_var[\"a_dict\"][\"qwe\"][1] = 2\nmy_var[\"a_dict\"][\"qwe\"][2] = 3\nmy_var[\"a_dict\"][\"nested_dict\"][\"das\"] = 31\nmy_var[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\"\n</code></pre> <p>I felt like having a single-page web service that would do these transformations leveraging the <code>pycatj</code> would be helpful to somebody sometime. Probably the easiest way would be to rewrite the same code with JavaScript and create a static page with that code without any backend, but does it spark joy?. Not even a bit.</p> <p>And as a starting point I decided to create a serverless function that will rely on <code>pycatj</code> and will be triggered later by a Web frontend with an HTTP request carrying the content for pycatj-ifying.</p> <p>In a nutshell, the function should behave something like that:</p> <pre><code>curl -X POST https://api-endpoint.com -d '{\"data\":{\"somekey\":\"value1\"}}'\n# returns\nmy_var[\"somekey\"] = \"value1\"\n</code></pre> <p>To add some sugar to the mix I will leverage the serverless framework to do the heavy lifting in a code-first way. The plan is set, lets go see it to completion.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#agenda","title":"Agenda","text":"<p>The decomposition of the service creation and deployment can be done as follows:</p> <ol> <li>Google Cloud Platform</li> <li>Create a GCP account (if needed) and acquire the API credentials</li> <li>Create a project in GCP that will host a Function and enable the needed APIs for serverless to be able to create the Function and its artifacts</li> <li>Function creation and testing</li> <li>create the code in conformance with the GCP Function handlers/events rules</li> <li>Manage code dependencies</li> <li>Function deployment</li> <li>leveraging serverless framework to deploy a function to GCP</li> <li>Add a frontend (in another blog post) that will use the serverless function.</li> </ol>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#1-google-cloud-platform","title":"1 Google Cloud Platform","text":"<p>Following the agenda, ensure that you have a working GCP account (trial gives you $300, and GCP Function is perpetually FREE with the sane usage thresholds). Make sure that you have a billing account created, this is set up when you opt in the free trial program, for example. Without a linked billing account the Functions won't work.</p> <p>Once you have your account set, you should either continue with a default project or create a new one. In either case you need to enable the APIs that will be leveraged by serverless framework for a function deployment process. Go thru this guide carefully on how to enable the right APIs.</p> <p>API credentials</p> <p>Do not forget to download your API credentials, as nothing can be done without them. This guide's section explains it all. The commands you will see in the rest of this post assume that the credentials are stored in <code>~/.gcould</code> directory.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#2-function-creation","title":"2 Function creation","text":"<p>Since we are living on the edge, we will rely on the serverless framework to create &amp; deploy our function. The very same framework I leveraged for the AWS Lambda creation, so why not try it for GCP Function?</p> <p>The notable benefit of serverless framework is that it allows you to define your Function deployment as a code and thus making it repeatable, versionable and fast.</p> <p>But nothing comes cheap, for all these perks you need to pay; and the serverless toll is in being a Javascript package =|. Don't know about you, but no glove - no love is the principle I try to stick to with JS. So why not quarantine it in a docker container jail and keep your machine npm-free?</p> <pre><code>docker pull amaysim/serverless:1.45.1\n</code></pre>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#21-serverless-service-template","title":"2.1 Serverless service template","text":"<p>The way I start my serverless journey is by telling the serverless to generate a service template in the programming language of my choice. Later we can tune bits and pieces of that service, but if you start from a zero-ground, its easier to have a scaffolding to work on.</p> <pre><code># Create service with `google-python` template in the folder ~/projects/pycatj-web\ndocker run --rm \\\n -v ~/projects/pycatj-web:/opt/app \\\n amaysim/serverless:1.45.1 \\\n serverless create --template google-python --path pycatj-serverless\n</code></pre> <p>The result of the <code>serverless create --template &lt;template&gt;</code> command will be a directory with a boilerplate code for our function and a few serverless artifacts.</p> <pre><code># artifacts created by `serverless create --template`\n$ tree pycatj-serverless/\npycatj-serverless/\n\u251c\u2500\u2500 main.py\n\u251c\u2500\u2500 package.json\n\u2514\u2500\u2500 serverless.yml\n</code></pre> <p>We need to take a closer look at the generated <code>serverless.yml</code> template file where we need to make some adjustments:</p> <ol> <li>the project name should match the project name you have created in the GCP</li> <li>the path to your GCP credentials json file should be valid</li> </ol> <p>Given that the project in my GCP is called <code>pycatj</code> and my credentials file is <code>~/.gcloud/pycatj-d6af60eda976.json</code> the <code>provider</code> section of the <code>serverless.yml</code> file would look like this:</p> <pre><code># serverless.yml file\n# with project name and credentials file specified\nprovider:\n  name: google\n  stage: dev\n  runtime: python37\n  region: us-central1\n  project: pycatj\n  credentials: ~/.gcloud/pycatj-d6af60eda976.json\n</code></pre> <p>As to the <code>main.py</code> generated by the framework, then its a simple boilerplate code with a text reply to an incoming HTTP request wrapped in a Flask object.</p> <pre><code># main.py\ndef http(request):\n    \"\"\"Responds to any HTTP request.\n    Args:\n        request (flask.Request): HTTP request object.\n    Returns:\n        The response text or any set of values that can be turned into a\n        Response object using\n        `make_response &lt;http://flask.pocoo.org/docs/1.0/api/#flask.Flask.make_response&gt;`.\n    \"\"\"\n    return f'Hello World!'\n</code></pre> <p>Lets test that our modifications work out so far by trying to deploy the template service.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#22-testing-function-deployment","title":"2.2 Testing function deployment","text":"<p>Before we start pushing our function and its artifacts to the GCP, we need to tell serverless how to talk to the cloud provider. To do that, we need to install the <code>serverless-google-cloudfunctions</code> plugin that is referenced in the <code>serverless.yml</code> file.</p> <p>Install the google cloud functions plugin with the <code>npm install</code> command using the directory with a generated serverless service files:</p> <pre><code>docker run --rm \\\n  -v ~/.gcloud/:/root/.gcloud \\\n  -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\\n  amaysim/serverless:1.45.1 npm install\n</code></pre> <p>Note, here I mount my GCP credentials that are stored at <code>~/.gcloud</code> dir to a containers <code>/root/.gcloud</code> dir where serverless container will find them as they are referenced in the <code>serverless.yml</code> file. And secondly I bind mount my project's directory <code>~/projects/pycatj-web/pycatj-serverless</code> to the <code>/opt/app</code> dir inside the container that is a <code>WORKDIR</code> of that container.</p> <p>Now we have a green flag to try out the deployment process with <code>serverless deploy</code>:</p> <pre><code>docker run --rm \\\n  -v ~/.gcloud/:/root/.gcloud \\\n  -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\\n  amaysim/serverless:1.45.1 serverless deploy\n</code></pre> <p>If the deployment fails with the Error Not Found make sure that you don't have stale failed deployments by going to Cloud Console -&gt; Deployment Manager and deleting all deployments created by Serverless</p> <p>Upon a successful deployment you will have a Cloud Function deployed and reachable by the service URL:</p> <pre><code>Deployed functions\nfirst\n  https://us-central1-pycatj.cloudfunctions.net/http\n</code></pre> <p><code>curl</code>-ing that API endpoint will return a simple \"Hello world\" as coded in our boilerplate <code>main.py</code> function:</p> <pre><code># main.py\ndef http(request):\n    return f'Hello World!'\n</code></pre> <pre><code>curl -s https://us-central1-pycatj.cloudfunctions.net/http\nHello World!\n</code></pre> <p>You can also verify the resources that were created by this deployment by visiting the Deployment Manager in the GCP console as well as navigating to the functions page and examine the deployed function and its properties:</p> <p></p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#23-writing-a-function","title":"2.3 Writing a Function","text":"<p>That was a template function that we just deployed with the HTTP event acting as a trigger.</p> <p>Lets see how the the actual Python function is coupled to a service definition inside the serverless file. How about we give our function a different name by first changing the <code>functions</code> section of the <code>serverless.yml</code> file:</p> <pre><code># changing the function name and handler to `pycatjify`\nfunctions:\n  pycatjify:\n    handler: pycatjify\n    events:\n      - http: path\n</code></pre> <p>Since we changed the function and the handler name to <code>pycatjify</code> we should do the same to our function inside the <code>main.py</code> file:</p> <pre><code>def pycatjify(request):\n    return f\"We are going to give pycatj its own place on the web!\"\n</code></pre> <p>Deploying this function will give us a new API endpoint aligned to a new function name we specified in the <code>serverless.yml</code>:</p> <pre><code>Deployed functions\npycatjify\n  https://us-central1-pycatj.cloudfunctions.net/pycatjify\n\n# testing\n$ curl https://us-central1-pycatj.cloudfunctions.net/pycatjify\nWe are going to give pycatj its own place on the web!\n</code></pre>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#231-managing-code-dependencies","title":"2.3.1 Managing code dependencies","text":"<p>Up until now we played with a boilerplate code with a few names changed to give our function a bit of an identity. We reached the stage when its time to onboard the <code>pycatj</code> package and make our function benefit from it.</p> <p>Since the Functions are executed in the sandboxes on the cloud platforms, we must somehow tell what dependencies we want these sandbox to have when running our code. In the AWS Lambda example we packaged the 3<sup>rd</sup> party libraries along the function (aka vendoring).</p> <p>In GCP case the vendoring approach is also possible and is done in the same way, but it is also possible to ship a pip <code>requirements.txt</code> file along your <code>main.py</code> that will specify your function dependencies as pythonistas used to.</p> <p>Read more on GCP python dependency management</p> <p>Unfortunately, the PIP version that GCP currently uses does not support PEP 517, so it was not possible to specify <code>-e git+https://github.com/dbarrosop/pycatj.git#egg=pycatj</code> in a requirements file, thus I continued with a good old vendoring technique:</p> <pre><code># executed in ~/projects/pycatj-web/pycatj-serverless\npip3 install -t ./vendored git+https://github.com/dbarrosop/pycatj.git\n</code></pre> <p>This installs <code>pycatj</code> package and its dependencies in a <code>vendored</code> directory and will be considered as Function's artifact and pushed to GCP along the <code>main.py</code> with the next <code>serverless deploy</code> command.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#232-events","title":"2.3.2 Events","text":"<p>Every function should be triggered by an event or a trigger that is supported by a cloud provider. When serverless is used the event type is specified for each function in the <code>serverless.yml</code> file:</p> <pre><code># pycatjify function is triggered by an event of type `http`\n# note that they key `path` is irrelevant to the serverless\nfunctions:\n  pycatjify:\n    handler: pycatjify\n    events:\n      - http: path\n</code></pre> <p>With this configuration we expect our function to execute once an HTTP request hits the function API endpoint.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#233-writing-a-function","title":"2.3.3 Writing a function","text":"<p>Yes, a thousand words later we finally at a milestone where we write actual python code for a function. The template we generated earlier gives us a good starting point - a function body with a single Flask <code>request</code> argument:</p> <pre><code>def pycatj(request):\n    return f\"We are going to give pycatj its own place on the web!\"\n</code></pre> <p>The logic of our serverless function that we are coding here is:</p> <ol> <li>parse the contents of an incoming HTTP request extracting the contents of a JSON file passed along with it</li> <li>transform the received data with <code>pycatj</code> package and send back the response</li> </ol> <p>With a few additions to access the <code>pycatj</code> package in a <code>vendored</code> directory and being able to test the function locally, the resulting <code>main.py</code> file looks as follows:</p> <p>This code has some extra additions to a simple two-step logic I mentioned before. I stuffed a default <code>data</code> value that will be used when the incoming request has no body, then we will use this dummy data just for demonstration purposes. To let me test the function code locally I added the <code>if __name__ == \"__main__\":</code> condition and lastly I wrote some <code>print</code> functions for a trivial logging. Speaking of which...</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#24-logging","title":"2.4 Logging","text":"<p>Logging is a bless! Having a chance to look what happens with your function in a cloud platform sandbox is definitely a plus. With GCP the logging can be done in the simple and advanced modes. A simple logging logs everything that is printed by a function into <code>stdout/stderr</code> outputs -&gt; a simple <code>print()</code> function would suffice. In a more advanced mode you would leverage a GCP Logging API.</p> <p>The logs can be viewed with the Web UI Logging interface, as well as with the <code>gcloud</code> CLI tool.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#3-function-deployment","title":"3 Function deployment","text":"<p>We previously already tried the deployed process with a boilerplate code just to make sure that the serverless framework works. Now that we have our <code>pycatj</code> package and its dependencies stored in a <code>vendored</code> folder and the function body is filled with the actual code, lets repeat the deployment and see what we get:</p> <pre><code>docker run --rm \\\n  -v ~/.gcloud/:/root/.gcloud \\\n  -v ~/projects/pycatj-web/pycatj-serverless:/opt/app \\\n  amaysim/serverless:1.45.1 serverless deploy\n</code></pre> <p>All goes well and serverless successfully updates our function to include the vendored artifacts as well as the new code in the <code>main.py</code>. Under the hood, the deployment process took the code of our Function and packaged it into a directory, zipped and uploaded to the deployment bucket.</p> <p>As demonstrated above, the serverless framework allows a user to express the deployment in a code, making the process extremely easy and fast.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#4-usage-examples","title":"4 Usage examples","text":"<p>Time to give our Function a roll by bombing it with HTTP requests. In this section I will show you how you can use the pycatjify service within a CLI and in a subsequent post we will write a simple Web UI using the API that our function provides.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#41-empty-get-request","title":"4.1 empty GET request","text":"<pre><code>curl -s https://us-central1-pycatj.cloudfunctions.net/pycatjify | jq -r .data\n\n# returns\nmy_dict[\"data\"] = \"test_value\"\nmy_dict[\"somenumber\"] = 123\nmy_dict[\"a_dict\"][\"asd\"] = \"123\"\nmy_dict[\"a_dict\"][\"qwe\"][0] = 1\nmy_dict[\"a_dict\"][\"qwe\"][1] = 2\nmy_dict[\"a_dict\"][\"qwe\"][2] = 3\nmy_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31\nmy_dict[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\"\n</code></pre> <p>With an empty GET request the function delivers a demo of its capabilities by taking a hardcoded demo JSON and making a transformation. The returned string is returned in a JSON object accessible by the <code>data</code> key.</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#42-post-with-a-root-and-pycatj_data-specified","title":"4.2 POST with a root and pycatj_data specified","text":"<p>Getting a demo response back is useless, to make use of a pycatjify service a user can specify the <code>root</code> value and pass the original JSON data in a POST request body using the <code>pycatj_data</code> key:</p> <pre><code>curl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"root\":\"POST\",\"pycatj_data\":{\"somekey\":\"somevalue\",\"a_dict\":{\"qwe\":[1,2],\"nested_dict\":{\"das\":31}}}}' \\\n  | jq -r .data\n\n# returns\nPOST[\"somekey\"] = \"somevalue\"\nPOST[\"a_dict\"][\"qwe\"][0] = 1\nPOST[\"a_dict\"][\"qwe\"][1] = 2\nPOST[\"a_dict\"][\"nested_dict\"][\"das\"] = 31\n</code></pre>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#43-post-without-root-with-pycatj_data","title":"4.3 POST without root, with pycatj_data","text":"<p>It is also allowed to omit the <code>root</code> key, in that case a default root value will be applied:</p> <pre><code>curl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"pycatj_data\":{\"somekey\":\"somevalue\",\"a_dict\":{\"qwe\":[1,2],\"nested_dict\":{\"das\":31}}}}' \\\n  | jq -r .data\n\n# returns\nmy_dict[\"somekey\"] = \"somevalue\"\nmy_dict[\"a_dict\"][\"qwe\"][0] = 1\nmy_dict[\"a_dict\"][\"qwe\"][1] = 2\nmy_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31\n</code></pre>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#44-post-with-json-file-as-a-body","title":"4.4 POST with json file as a body","text":"<p>My personal favorite is dumping a JSON file in a request. In that case a lengthy <code>curl</code> is not needed and you can specify a path to a file with a <code>@</code> char. This example leverages the logic embedded in a function that treats the whole body of an incoming request as a data for <code>pycatj</code> transformation.</p> <pre><code>$ cat test/test1.json\n{\n    \"somekey\": \"localfile\",\n    \"a_dict\": {\n        \"asd\": \"123\",\n        \"qwe\": [\n            1,\n            2\n        ],\n        \"nested_dict\": {\n            \"das\": 31,\n            \"qwe\": \"asd\"\n        }\n    }\n}\n\ncurl -sX POST https://us-central1-pycatj.cloudfunctions.net/pycatjify \\\n  -H \"Content-Type: application/json\" \\\n  -d \"@./test/test1.json\" \\\n  | jq -r .data\n\n# returns\nmy_dict[\"somekey\"] = \"localfile\"\nmy_dict[\"a_dict\"][\"asd\"] = \"123\"\nmy_dict[\"a_dict\"][\"qwe\"][0] = 1\nmy_dict[\"a_dict\"][\"qwe\"][1] = 2\nmy_dict[\"a_dict\"][\"nested_dict\"][\"das\"] = 31\nmy_dict[\"a_dict\"][\"nested_dict\"][\"qwe\"] = \"asd\"\n</code></pre>","tags":["gcp","gcp function","python","serverless"]},{"location":"2019/creating-google-cloud-platform-function-with-python-and-serverless/#whats-next","title":"What's next?","text":"<p>Having <code>pycatj</code> functionality available withing a HTTP call reach makes it possible to create a simple one-page web frontend that will receive the users input and render the result of the pycatj-web service we deployed in this post.</p> <p>I will make another post covering the learning curve I needed to climb on to create a modern Material UI frontend that leverages the serverless function.</p> <p>[Project's source code]</p>","tags":["gcp","gcp function","python","serverless"]},{"location":"2020/arista-eos-gnmi-tutorial/","title":"Arista EOS gNMI Tutorial","text":"<p>We were pleasantly surprised by the way community appreciated gNMIc release. Thank you \ud83d\ude4f! That solidifies the fact that a well-formed, documented and easy to use gNMI tool was needed.</p> <p>Now with gNMIc available to everybody its easy like never before to test gNMI implementation of different routing OSes. And in this post we will get our hands on Arista vEOS.</p> <p>For this journey we pack:</p> <ol> <li>vEOS router (either physical or virtual)</li> <li>gNMIc documentation</li> <li>and a gNMI-map to navigate through the gNMI realm.</li> </ol> <p>Arista vEOS-for-labs is freely distributed and you can download the vmdk image from the official software portal.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#veos-configuration","title":"vEOS configuration","text":"<p>Once your vEOS starts with a blank config (credentials: <code>admin</code> and an empty pass) we ought to add a minimal config to it before gNMI fun starts:</p> <pre><code>username admin privilege 15 secret admin\n!\ninterface Ethernet1\n   no switchport\n   ip address 10.2.0.21/24\n!\nmanagement api gnmi\n   transport grpc default\n</code></pre> <p>With this config snippet we do a few things important from the gNMI standpoint:</p> <ul> <li>enabling password for <code>admin</code> to authenticate with a router</li> <li>configuring IP address for the <code>Ethernet1</code> interface to let gNMIc reach the router</li> <li>enabling <code>gnmi</code> management interface with the default transport config</li> <li>default transport doesn't enforce TLS usage and uses <code>6030</code> port</li> </ul> <p>That is all it takes to configure vEOS to start replying to our first gNMI RPCs, ridiculously easy!</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#gnmi-capabilities","title":"gNMI Capabilities","text":"<p>With gNMIc installed, our first stop would be trying out the gNMI Capabilities RPC. The Capabilities RPC is quite instrumental as it uncovers which gNMI version the device runs, what models it is loaded with and which encoding it understands.</p> <pre><code># 6030 - the default gNMI port on vEOS\n# credentials are admin:admin\n# --insecure mode is used to not enforce the TLS transport\n$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure capabilities\ngNMI version: 0.7.0\nsupported models:\n  - openconfig-rib-bgp, OpenConfig working group, 0.7.0\n  - arista-qos-augments, Arista Networks, Inc., \n  - arista-srte-deviations, Arista Networks, Inc., \n\n&lt;CLIPPED&gt;\n\n  - openconfig-platform-linecard, OpenConfig working group, 0.1.1\n  - openconfig-if-tunnel, OpenConfig working group, 0.1.1\nsupported encodings:\n  - JSON\n  - JSON_IETF\n  - ASCII\n</code></pre> <p>Judging by the output returned we see that</p> <ul> <li>vEOS 4.24.1.1F in my lab runs the latest gNMI version 0.7.0</li> <li>it is configured with both openconfig and native models</li> <li>it supports two variants of JSON encoding with a useless ASCII</li> </ul>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#getting-to-know-arista-yang-models","title":"Getting to know Arista YANG models","text":"<p>Before we can dive into the rest RPCs of gNMI service we have to get to know the YANG models vEOS listed as supported in Capabilities response.</p> <p>Arista publishes its YANG models in the aristanetworks/yang repo and by the looks of it it seems they are OpenConfig believers. For the vEOS 4.24.1.1F release that I am running the list of YANG models is definitely angled towards OpenConfig models with native YANG models marked as experimental.</p> <p>Browsing the source OC YANG files in this repo is one way to understand the structure of the models, or we can use pyang or goyang to generate a tree view. Michael Kashin shows here how to use goyang to quickly generate tree views of Arista models.</p> <p>Once we know the structure of the OC YANG models vEOS is equipped with we can finally get to more advanced RPCs fetching, setting and subscribing.</p> <p>If YANG transformation topic is hard on you, ping me in comments and I will expand on this.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#gnmi-get","title":"gNMI Get","text":"<p>Now that we know which models our gear runs we can easily issue a gNMI Get RPC with <code>get</code> command. Lets pretend that we would like to know the configured IP addresses on the vEOS. All it takes is to carefully walk through the OC model to the right leaf:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\\n        --path \"/interfaces/interface[name=*]/subinterfaces/subinterface[index=*]/ipv4/addresses/address/config/ip\"\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"time\": \"1970-01-01T02:00:00+02:00\",\n  \"updates\": [\n    {\n      \"Path\": \"/interfaces/interface[name=Ethernet1]/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=10.2.0.21]/config/ip\",\n      \"values\": {\n        \"interfaces/interface/subinterfaces/subinterface/ipv4/addresses/address/config/ip\": \"10.2.0.21\"\n      }\n    }\n  ]\n}\n</code></pre> <p>The returned output indicates that router has only one IPv4 address <code>10.2.0.21</code> configured and it is contained within the <code>/interfaces/interface[name=Ethernet1]/subinterfaces/subinterface[index=0]/ipv4/addresses/address[ip=10.2.0.21]/config/ip</code> path. The paths in its turn shows that the interfaces that has this IP is <code>Ethernet1</code>.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#gnmi-get-all","title":"gNMI Get ALL","text":"<p>One particular trick that might come very handy is getting the entire config/state of the router with gNMI. That will likely output a lot of data but it will enable you to search through it and find the right path in the model for a more precise get/set/subscribe queries.</p> <p>The trick is to specify the root <code>/</code> path for your Get RPC that will dump all the data from the router. Its better to redirect the output to a file:</p> <pre><code>gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get --path / &gt; /tmp/arista.all.json\n</code></pre> <p>Here is the resulting JSON that I fetched from my lab router. This way you can \"reverse engineer\" the models tree view by letting a router send you all its state and config.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#gnmi-set","title":"gNMI Set","text":"<p>Our next RPC will change the configuration on a vEOS router. This is done with the gNMIc <code>set</code> command.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#updating-configuration","title":"Updating configuration","text":"<p>A quick example would be to add a description to a port. But first lets ensure that its not set:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\\n        --path \"/interfaces/interface[name=Ethernet1]/config/description\"\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"time\": \"1970-01-01T02:00:00+02:00\",\n  \"updates\": [\n    {\n      \"Path\": \"/interfaces/interface[name=Ethernet1]/config/description\",\n      \"values\": {\n        \"interfaces/interface/config/description\": \"\"\n      }\n    }\n  ]\n}\n</code></pre> <p>All good, the description is empty, lets set it to <code>gnmic-example</code> value:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure set \\\n        --update-path \"/interfaces/interface[name=Ethernet1]/config/description\" \\\n        --update-value \"gnmic-example\"\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"timestamp\": 1595749808169752510,\n  \"time\": \"2020-07-26T10:50:08.16975251+03:00\",\n  \"results\": [\n    {\n      \"operation\": \"UPDATE\",\n      \"path\": \"/interfaces/interface[name=Ethernet1]/config/description\"\n    }\n  ]\n}\n</code></pre> <p>With the implicit-type kind of a set operation gNMIc will use the JSON encoding for the value specified with <code>--update-value</code> flag. The result we get back from the box confirms that the UPDATE operation has been applied. Now we can check if the value is indeed set by repeating the get command:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure get \\\n--path \"/interfaces/interface[name=Ethernet1]/config/description\"\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"time\": \"1970-01-01T02:00:00+02:00\",\n  \"updates\": [\n    {\n      \"Path\": \"/interfaces/interface[name=Ethernet1]/config/description\",\n      \"values\": {\n        \"interfaces/interface/config/description\": \"gnmic-example\"\n      }\n    }\n  ]\n}\n</code></pre> <p>Now we see the description set to the value we specified.</p> <p>gNMIc supports many ways to provide the configuration values, check <code>set</code> command docs for all the options.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#deleting-configuration","title":"Deleting configuration","text":"<p>gNMI Set RPC allows not only to update/replace the configuration but also to delete it. Lets remove the description we set before with the <code>delete</code> flag of the set command.:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure set \\\n        --delete \"/interfaces/interface[name=Ethernet1]/config/description\"\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"timestamp\": 1595750232015131720,\n  \"time\": \"2020-07-26T10:57:12.01513172+03:00\",\n  \"results\": [\n    {\n      \"operation\": \"DELETE\",\n      \"path\": \"/interfaces/interface[name=Ethernet1]/config/description\"\n    }\n  ]\n}\n</code></pre>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#gnmi-subscribe","title":"gNMI Subscribe","text":"<p>And we managed to get to the end of it. The crown jewel of gNMI service - Subscribe RPC.</p> <p>To demonstrate gNMI subscriptions done with the corresponding <code>subscribe</code> command we will solve the following tasks:</p> <ul> <li>subscribe to interface counters and see the effect of SAMPLE subscriptions with different sampling intervals</li> <li>subscribe to a protocol admin status and see the effect of ON_CHANGE mode of subscription</li> </ul> <p>The <code>subscribe</code> command has a lot of options which are instrumental to tailor the command behavior to your needs.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#sample-subscriptions","title":"Sample subscriptions","text":"<p>For the sampled subscriptions we expect to receive the value of the subscribed data node with each sampling interval; doesn't matter if the data changes in-between the sampling timestamps or not, we will get it with the cadence specified by the sample-interval.</p> <p>Samples subscriptions are useful for rapidly changing data, like interface counters. Lets subscribe to our only interface <code>Ethernet1</code> with 2 seconds sampling interval.</p> <p>Note, we had to disable QoS marking by setting it to 0, since vEOS does not support marking for gNMI messages.</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure subscribe \\\n        --path \"/interfaces/interface[name=Ethernet1]/subinterfaces/subinterface/state/counters/in-octets\" \\\n        --stream-mode sample --sample-interval 2s \\\n        --qos 0\n</code></pre> <p></p> <p>sampled subscriptions arriving with 2s interval</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#on-change-subscriptions","title":"On Change subscriptions","text":"<p>The other popular subscription mode is ON_CHANGE where the router pushes the data towards the collector when the data changes. With such a mode you don't push the data unnecessarily.</p> <p>A popular use case for ON_CHANGE subscriptions is to subscribe to oper/admin state of control plane protocols to get notified when the state changes (aka trap).</p> <p>To demonstrate this behavior we will</p> <ul> <li>configure BGP process on vEOS</li> <li>subscribe with ON_CHANGE mode to the BGP AS leaf effectively watching its value.</li> </ul> <p>Our trivial BGP configuration:</p> <pre><code>!\nip routing\n!\nrouter bgp 2\n</code></pre> <p>Now lets subscribe to the AS number with:</p> <pre><code>$ gnmic -a 10.2.0.21:6030 -u admin -p admin --insecure subscribe \\\n        --path \"/network-instances/network-instance[name=default]/protocols/protocol[identifier=BGP][name=BGP]/bgp/global/config/as\" \\\n        --stream-mode on_change \\\n        --qos 0\n\n{\n  \"source\": \"10.2.0.21:6030\",\n  \"subscription-name\": \"default\",\n  \"timestamp\": 1595753683536455210,\n  \"time\": \"2020-07-26T11:54:43.53645521+03:00\",\n  \"updates\": [\n    {\n      \"Path\": \"network-instances/network-instance[name=default]/protocols/protocol[identifier=BGP][name=BGP]/bgp/global/config/as\",\n      \"values\": {\n        \"network-instances/network-instance/protocols/protocol/bgp/global/config/as\": 2\n      }\n    }\n  ]\n}\n</code></pre> <p>Our subscription will stand still, as the AS number doesn't change, the router will not update it, unless the value changes.</p> <p>Lets remove the BGP process from the router and see what happens:</p> <p></p> <p>on change subscriptions arrive when data changes</p> <p>Here we receive a notification update about the deletion of the data we subscribed to immediately when the deletion happens.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/arista-eos-gnmi-tutorial/#summary","title":"Summary","text":"<p>In this post we put gNMIc to a good use against Arista vEOS. All of the gNMI service RPCs have been successfully tested against vEOS, we have identified which encoding vEOS supports, found out that it uses OpenConfig models mostly and it can't use QoS markings.</p> <p>On the bright side, vEOS supports ON_CHANGE subscriptions and is capable of delivering subsecond updates.</p>","tags":["gnmi","openconfig","arista","gnmic","yang"]},{"location":"2020/gnmi-map/","title":"gNMI Map","text":"<p>Lately I've been involved in project that required quite a deep understanding of OpenConfig gRPC Network Management Interface (gNMI). Going over the gNMI specification multiple times made me realize that I can't fully build a mental map of all the messages and encapsulations without having a visual representation of it. So I've made one, lets see what it has to offer.</p> <p>gNMI Map is essentially a visual guide to the gNMI service.</p> <p></p> <p>It lays out the protobuf data types that compose the gNMI service and provides the references to the relevant sections of the reference guide and code definitions. For example, if you wondered what are the messages the client sends when it needs to query the Capabilites of the remote gNMI target, you can easily zoom into the Capabilities RPC and identify all the messages and types involved in this RPC:</p> <p></p> <p>The visual connectors help you unwrap the nested messages and collect the whole picture.</p> <p>Moreover, each message and type \"card\"0 has a link to a relevant documentation piece of the reference along with the link to its definition in the <code>gnmi.proto</code> file:</p> <p></p> <p>allowing you to quickly jump either to the explanation paragraph of the spec or dive into the proto definition code piece.</p> <p>Currently the latest gNMI version (0.7.0.) has been \"mapped\", my intention is to release another map when a new version of the gNMI will be available, keeping the old ones versioned. That will allow having a map for each release after 0.7.0.</p> <p>The map comes in a PDF format and is stored at hellt/gnmi-map, you can quickly access the latest version with a shortcut: https://bit.ly/gnmi-map.</p> <p>Happy mapping!</p>","tags":["gnmi","openconfig"]},{"location":"2020/gnmic-got-better-with-yang-completions/","title":"gNMIc got better with YANG-completions","text":"<p><code>gnmic</code> was the first opensource project that I've been part of that got widely adopted. As the maintainers of a public project, Karim and I were wondering when would we get the first external contribution.</p> <p>To our surprise, the very first external contribution laid out the foundation to one of the most exciting features of <code>gnmic</code> - YANG-Completions.</p> <p>I thought that the best way to describe what YANG-completions is showing you a quick demo augmented with some comments. This resulted in this twitter-series:</p> <p><p>\ud83d\udcbbFellow Network Automation engineers,today is a special day for YANG-based automation as we release the \"YANG-completions\" feature with gnmic v0.4I have only 1m30s to convince you that it is a dream turned into reality (or a black magic \ud83d\udc80)https://t.co/Nylyay1pl4 pic.twitter.com/IOnumRCOZi</p>\u2014 Roman Dodin (@ntdvps) October 21, 2020 </p>","tags":["gnmi","openconfig","yang","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/","title":"gNMIc - gNMI CLI client and collector","text":"<p>Despite the fact that gNMI is defacto the go-to interface for a model-driven telemetry collection, we, as a community, had no gNMI tool that was easy to install, pleasure to use, documented and pre-built for common platforms. Until now.</p> <p>I am excited to announce the public release of <code>gnmic</code> - a CLI client and a collector that talks gNMI to your devices.</p> <p>Tip</p> <p>October 2022: gNMIc has joined Openconfig.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#problem-statement","title":"Problem statement","text":"<p>I am not exaggerating, there is a shortage of open source gNMI clients one can find. And when I say gNMI clients I mean the CLI clients that allow you to invoke gNMI service RPCs.</p> <p>Earlier this year I bragged about it, in hope that my google-foo is just broken and the community knows of a gNMI client that I could download and use right away without jumping through hoops:</p> <p><p>So coming back to the OpenConfig/gNMI and what a hot mess it is when it comes to the tooling.You would probably think that there is a top-notch gNMI CLI (or even a shell) for you to query your routers like a pro.</p>\u2014 Roman Dodin (@ntdvps) February 18, 2020 </p> <p>But that was not my google-foo, unfortunately. For the sake of completeness allow me to summarize the landscape of gNMI clients in a pre-gnmic era:</p> <ul> <li>OpenConfig gNMI CLI client - thats the google search top result one gets when looking for gNMI client. A reference implementation which lacks some essential features:<ul> <li>no documentation, no usage examples - you really better know how to read Go code to understand how to use it.</li> <li>Get requests will require you to write in proto syntax instead of a simple <code>get</code> command with a path.</li> <li>additional options like Encoding, Models are not exposed via flags.</li> <li>no ready-made binaries - you need to have a Go tool chain to build the tool.</li> <li>no insecure support - you can kiss goodbye your lab installations without PKI.</li> </ul> </li> <li>Google gnxi - Googles gNxI tools that include gNMI, gNOI.<ul> <li>the gNMI RPCs are split to different CLI tools which is not convenient</li> <li>a list of flags is all you got when it comes to documentation</li> <li>no releases to download, Go toolchain is needed</li> </ul> </li> <li>cisco-gnmi-python - a Cisco Innovative Edge project that is quite decent and complete, good job! But a few improvements could have been made:<ul> <li>client doesn't allow to use insecure gRPC transport, PKI is mandatory.</li> <li>Set requests can't set values specified on the command line.</li> <li>CLI structure is not consistent across the commands</li> <li>No option exposed to set the Subscription mode.</li> </ul> </li> <li>Telegraf and Ansible gNMI module are not qualified to be considered as CLI tools.</li> </ul>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#what-makes-gnmi-tool-nice-to-use","title":"What makes gNMI tool nice to use?","text":"<p>Looking at this landscape, the following essential features a nice gNMI client should have come to mind:</p> <ul> <li>provide a clean and vendor independent interface to gNMI RPCs</li> <li>expose all configuration options the gNMI RPCs have via flags or file-based configurations</li> <li>allow multi-target operations: i.e. a subscription made to a number of the devices</li> <li>implement both TLS enabled and non-secure transport</li> <li>support different output formats (JSON, proto) and destinations (stdout, file, streaming/messaging buses)</li> <li>be documented</li> <li>provide an easy way to install the tool without requiring a dev toolchain to be present.</li> </ul> <p>With these essential features in mind we started to work on gnmic.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#gnmic-and-its-features","title":"gNMIc and its features","text":"<p>The work on <code>gnmic</code> started with analysis of the existing tools shortcomings coupled with collecting requirements from our fellow engineers and our past user experience.</p> <p>For the <code>gnmic</code> features run down go to our beautiful documentation portal - https://gnmic.kmrd.dev. In this post I will go a bit deeper on some core features and design choices we made, so please refer to the documentation if you are looking for a basic usage or command reference guide.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#consistent-command-line-interface","title":"Consistent command line interface","text":"<p>It is easy to spot a CLI tool that got some love from its developers by looking at the way it is composed. Since most of the <code>gnmic</code> users will use it as a CLI tool we took an extra step and wrote it with a Cobra framework that adds a great layer of consistency to the command line applications.</p> <p>With Cobra <code>gnmic</code> gets extra powers such as consistent global and local flags, multi-tiered subcommands, auto-generated and accurate help and overall a \"proper\" CLI behavior.</p> <pre><code>$ gnmic get --help\nrun gnmi get on targets\n\nUsage:\n  gnmic get [flags]\n\nFlags:\n  -h, --help            help for get\n      --model strings   get request model(s)\n      --path strings    get request paths\n      --prefix string   get request prefix\n  -t, --type string     the type of data that is requested from the target. one of: ALL, CONFIG, STATE, OPERATIONAL (default \"ALL\")\n\nGlobal Flags:\n  -a, --address strings             comma separated gnmi targets addresses\n      --config string               config file (default is $HOME/gnmic.yaml)\n  -d, --debug                       debug mode\n  -e, --encoding string             one of [json bytes proto ascii json_ietf]. Case insensitive (default \"json\")\n</code></pre>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#alignment-to-gnmi-specification","title":"Alignment to gNMI specification","text":"<p>For a tool to be generic it must not deviate from a reference specification. Adhering to that promise, we made <code>gnmic</code> commands modelled strictly after the gNMI RPCs. Each RPC has a command with a clear and concise name, and each command's flags are named after the fields of the corresponding proto message. No ambiguous flag names or questionable subcommands, it is clear and guessable what each command and flag does without looking at the documentation:</p> <pre><code>$ gnmic -h\n&lt;snipped&gt;\n\nAvailable Commands:\n  capabilities query targets gnmi capabilities\n  get          run gnmi get on targets\n  help         Help about any command\n  listen       listens for telemetry dialout updates from the node\n  path         generate gnmi or xpath style from yang file\n  set          run gnmi set on targets\n  subscribe    subscribe to gnmi updates on targets\n  version      show gnmic version\n\n&lt;snipped&gt;\n</code></pre> <p>Moreover, we tried to expose every configuration knob gNMI specification has to offer. Again, a generic tool should not limit your capabilities, so if you want to, say, restrict the YANG models the gNMI target should use when replying back to the client - there is a flag for that!</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#tls-and-non-tls-transports","title":"TLS and non-TLS transports","text":"<p>We allowed ourselves to step away from the specification to add one additional generic purpose feature - a insecure transport fo gRPC connection.</p> <p>The need for the non-secured connections is quite reasonable, its cumbersome in many cases to deal with certificates and keys generation if all one is up to is a quick gNMI test.</p> <pre><code>gnmic -a 10.1.0.11:57400 -u admin -p admin --insecure capabilities\ngNMI_Version: 0.7.0\nsupported models:\n  - nokia-conf, Nokia, 19.10.R2\n  - nokia-state, Nokia, 19.10.R2\n  - nokia-li-state, Nokia, 19.10.R2\n  - nokia-li-conf, Nokia, 19.10.R2\n&lt;&lt; SNIPPED &gt;&gt;\nsupported encodings:\n  - JSON\n  - BYTES\n</code></pre>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#flexible-configuration-options","title":"Flexible configuration options","text":"<p>Due to a sheer amount of configuration options <code>gnmic</code> has, it can sometimes be tedious to specify all of them as CLI flags. For such cases we leveraged viper and added support for file-based configuration that is consistent with both local and global flags. Its up to a user to choose the configuration file format: YAML, JSON, HCL - all are welcome!</p> <pre><code>$ cat ~/gnmic.yml\naddress: \"10.0.0.1:57400\"\nusername: admin\npassword: admin\ninsecure: true\n</code></pre> <pre><code># now gnmic can read this cfg file and get the params from it\n$ gnmi get --path /configure/system/name\n</code></pre>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#automation-friendly-output","title":"Automation friendly output","text":"<p>Its quite common to use gnmic in a setting where the output it provides is used as an input to another command. The simple example is getting something out of the network element and processing the result with some other tool.</p> <p>Keeping that case in mind we modelled gnmic output to default to JSON format, so that you can quickly <code>jq</code> the results out and feed it to other tools or processes.</p> <pre><code>gnmic -a 10.1.0.11:57400 -u admin -p admin --insecure \\\n      get --path /state/system/platform\n\n{\n  \"source\": \"10.1.0.11:57400\",\n  \"timestamp\": 1592829586901061761,\n  \"time\": \"2020-06-22T14:39:46.901061761+02:00\",\n  \"updates\": [\n    {\n      \"Path\": \"state/system/platform\",\n      \"values\": {\n        \"state/system/platform\": \"7750 SR-1s\"\n      }\n    }\n  ]\n}\n</code></pre>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#multiple-subscriptions","title":"Multiple subscriptions","text":"<p>To expand on <code>gnmic</code> subscription capabilities and not limiting users to a single subscription per target we added a way to decouple subscriptions from the targets. The Multiple subscriptions feature allows to defined as many subscriptions as needed and later associate them to the targets:</p> <pre><code>targets:\n  router1.lab.com:\n    subscriptions:\n      - port_stats\n      - service_state\n  router2.lab.com:\n    subscriptions:\n      - service_state\nusername: admin\npassword: nokiasr0s\ninsecure: true\n\nsubscriptions:\n  port_stats:\n    paths:\n      - \"/state/port[port-id=1/1/c1/1]/statistics/out-octets\"\n      - \"/state/port[port-id=1/1/c1/1]/statistics/in-octets\"\n    stream-mode: sample\n    sample-interval: 5s\n  service_state:\n    paths:\n       - \"/state/service/vpls[service-name=*]/oper-state\"\n       - \"/state/service/vprn[service-name=*]/oper-state\"\n    stream-mode: on-change\n</code></pre> <p>With this approach subscriptions stay decoupled from the targets, while being fully configurable.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#documentation","title":"Documentation","text":"<p>Writing documentation is hard, but it felt necessary to provide a full-blown documentation portal with basic usage, command reference and advanced use cases examples.</p> <p>Knowing how problematic it might be for a novice to get started with gNMI, we added a lot of examples to each command <code>gnmic</code> has. The documentation portal is built with mkdocs-material theme and is open, so you can request additions or contribute via issues.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#distribution-via-binaries","title":"Distribution via binaries","text":"<p>Not only having documentation is an essential step to break the steep entry barrier, but also the installation process in our opinion must be welcoming and inclusive.</p> <p>Being written in Go, <code>gnmic</code> is distributed as a single binary built for most common architectures and OSes. Our single-command installation script makes it extremely easy to install or upgrade.</p> <pre><code>curl -sL https://github.com/karimra/gnmic/raw/master/install.sh | sudo bash\n</code></pre>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#summary","title":"Summary","text":"<p>At the end of the day, I tend to believe that <code>gnmic</code> will successfully fill the void of standalone gNMI tools available to the public. Starting from a consistent CLI layer with all the gNMI RPCs nicely exposed and finishing with the proper docs and easy installation it checks all the marks I had in mind for a decent gNMI client, and hope it will be to community's satisfaction as well.</p> <p>Oh, and <code>gnmic</code> also has collection capabilities allowing you to export the metrics collected via gNMI to Kafka, NATS, Influx, Prometheus. But that is for another post.</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/gnmic---gnmi-cli-client-and-collector/#authors","title":"Authors","text":"<p>The team behind <code>gnmic</code> consists of Karim Radhouani and Roman Dodin, but we are welcome contributors of all sorts. Be it code, documentation, bug reports or feature requests!</p>","tags":["gnmi","openconfig","go","gnmic"]},{"location":"2020/netconf-console-in-a-docker-container/","title":"NETCONF console in a docker container","text":"<p>Its an engineers core ability to decompose a complex task in a set of a smaller, easy to understand and perform sub-tasks. Be it a feature-rich program that is decomposed to classes, functions and APIs or a huge business operation captured in steps in a Methods Of Procedure document.</p> <p>In a network automation field where the configuration protocols such as NETCONF or gRPC are emerging, it is always needed to have a quick way to validate an RPC or Notification feature before implementing this in a code or a workflow.</p> <p>This blog post is about a handy tool called <code>netconf-console</code> which allows you to interface with your network device using NETCONF quick and easy. And, of course, I packed it in a smallish container so you can enjoy it hassle-free on every docker-enabled host.</p> <p><code>netconf-console</code> is a tool from Tail-f that basically gives you a NETCONF client for your console. That is exactly the packaging that I appreciate to have when I need to play with NETCONF. Cold-starting a python project with <code>ncclient</code> is much slower and you need ensure that you have all the RPCs coded, meh. With the console client you have almost anything you need to start tinkering with NETCONF enabled device.</p> <pre><code># netconf-console --host=example.com --db candidate --lock --edit-config=fragment1.xml \\\n--rpc=commit-confirmed.xml --unlock --sleep 5 --rpc=confirm.xml\n</code></pre> <p>Moreover, you can have an interactive NETCONF console to your device:</p> <pre><code># netconf-console --host=example.com -i\nnetconf&gt; lock\nnetconf&gt; edit-config fragment1.xml --db candidate\nnetconf&gt; rpc commit-confirmed.xml\nnetconf&gt; unlock\nnetconf&gt; get-config\nnetconf&gt; rpc confirm.xml\n</code></pre>","tags":["netconf","docker"]},{"location":"2020/netconf-console-in-a-docker-container/#containerize","title":"Containerize","text":"<p>I've been spoiled by Rust/Go tools that are self-contained, dependency-free and almost platform-agnostic. To achieve the same level of hassle-free for python tool I practice containerization.</p> <p>So I decided to put <code>netconf-console</code> in a whale protected cage by building a multi-stage docker image. Even though there are some images for the netconf-console, they are all outdated, based on an old version of the tool and use python2 under the hood. Its 2020 here, so I wanted to create a fresh, small image based off of the recent netconf-console code and running with python3.</p> <p>Of course you can install the netconf-console with pip as usual: <code>pip install netconf-console</code></p> <p>So here it is, a multi-stage build Dockerfile that builds <code>netconf-console</code> in Alpine linux with python3.7. The result of this build can be found at the relevant docker hub page.</p>","tags":["netconf","docker"]},{"location":"2020/netconf-console-in-a-docker-container/#tags","title":"Tags","text":"<p>The docker image will be tagged in accordance with the release version numbers of the <code>netconf-console</code>; at the time of this writing, the latest version is <code>2.2.0</code>, hence you will find the image with the corresponding tag. Also, the <code>latest</code> tag will point to the most recent version.</p>","tags":["netconf","docker"]},{"location":"2020/netconf-console-in-a-docker-container/#installation","title":"Installation","text":"<p>As with any other docker image, all it takes is to make a pull:</p> <pre><code>docker pull hellt/netconf-console\n</code></pre>","tags":["netconf","docker"]},{"location":"2020/netconf-console-in-a-docker-container/#usage-examples","title":"Usage examples","text":"<p>The entry point of the docker image is the netconf-console itself, therefore you can run it almost in the same way as you'd do with a standalone installation - by providing the arguments to the callable.</p> <pre><code># verify that the tool is properly working,\ndocker run --rm -it hellt/netconf-console --help\n\nusage: netconf-console [-h] [-s [{plain,noaaa} [{plain,noaaa} ...]]] [--db DB]\n                       [--timeout TIMEOUT]\n                       [--with-defaults {explicit,trim,report-all,report-all-tagged}]\n                       [--with-inactive] [-x XPATH]\n                       [-t {test-then-set,set,test-only}]\n                       [-o {merge,replace,create}]\n                       [--del-operation {remove,delete}] [-v VERSION]\n                       [-u USERNAME] [-p [PASSWORD]] [--host HOST]\n                       [-r REPLY_TIMEOUT] [--port PORT]\n                       [--privKeyFile PRIVKEYFILE] [--raw [RAW]] [--tcp]\n                       [-N [NS [NS ...]]] [--debug] [--hello] [--get [GET]]\n                       [--get-config [GET_CONFIG]] [--kill-session SESSION_ID]\n                       [--discard-changes] [--lock] [--unlock]\n                       [--commit [{confirmed}]] [--validate [VALIDATE]]\n                       [--copy-running-to-startup]\n                       [--copy-config [COPY_CONFIG]]\n                       [--edit-config [EDIT_CONFIG [EDIT_CONFIG ...]]]\n                       [--set [SET [SET ...]]]\n                       [--delete [DELETE [DELETE ...]]]\n                       [--create [CREATE [CREATE ...]]]\n                       [--get-schema GET_SCHEMA]\n                       [--create-subscription [CREATE_SUBSCRIPTION]]\n                       [--rpc [RPC]] [--sleep SLEEP] [-e EXPR] [--dry]\n                       [--interactive]\n                       [filename]\n</code></pre> <p>The interactive console mode, of course, also works:</p> <pre><code>docker run -it --rm hellt/netconf-console --host=10.1.0.11 --port=830 -u admin -p admin -i\nnetconf&gt; hello\n&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;nc:hello xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n  &lt;nc:capabilities&gt;\n    &lt;nc:capability&gt;urn:ietf:params:netconf:base:1.0&lt;/nc:capability&gt;\n    &lt;nc:capability&gt;urn:ietf:params:netconf:base:1.1&lt;/nc:capability&gt;\n&lt;&lt;SNIPPED&gt;&gt;\n</code></pre> <p>Now a more real-life example would be to throw a NETCONF RPC to the target device:</p> <pre><code>$ cat test.xml\n&lt;rpc message-id=\"113\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n  &lt;get&gt;\n    &lt;filter&gt;\n      &lt;state xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"&gt;\n        &lt;system&gt;\n          &lt;security&gt;\n            &lt;user-params&gt;\n              &lt;local-user&gt;\n              &lt;user/&gt;\n             &lt;/local-user&gt;\n            &lt;/user-params&gt;\n          &lt;/security&gt;\n        &lt;/system&gt;\n      &lt;/state&gt;\n    &lt;/filter&gt;\n  &lt;/get&gt;\n&lt;/rpc&gt;\n</code></pre> <pre><code>$ docker run -it --rm -v $(pwd):/rpc hellt/netconf-console --host=10.1.0.11 --port=830 -u admin -p admin test.xml\n&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;rpc-reply xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"113\"&gt;\n    &lt;data&gt;\n        &lt;state xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;attempted-logins&gt;13&lt;/attempted-logins&gt;\n                                &lt;failed-logins&gt;0&lt;/failed-logins&gt;\n                                &lt;locked-out&gt;false&lt;/locked-out&gt;\n                                &lt;password-changed-time&gt;2020-01-24T23:27:33.0Z&lt;/password-changed-time&gt;\n                            &lt;/user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;grpc&lt;/user-name&gt;\n                                &lt;attempted-logins&gt;0&lt;/attempted-logins&gt;\n                                &lt;failed-logins&gt;0&lt;/failed-logins&gt;\n                                &lt;locked-out&gt;false&lt;/locked-out&gt;\n                                &lt;password-changed-time&gt;2020-01-24T23:27:35.0Z&lt;/password-changed-time&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/state&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre> <p>Note that the WORKDIR of the container image is set to <code>/rpc</code>, therefore mounting the directory with your RPCs to that mountpoint will allow to refer to the file names directly.</p> <p>And you can create pretty complex ad-hoc RPCs with locking the datastore, committing and discarding the changes effortlessly.</p>","tags":["netconf","docker"]},{"location":"2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/","title":"Getting XML data sample for a given leaf in a YANG model","text":"<p>We can praise YANG as long as we want, but for an end user YANG is useful as the tooling around it and the applications leveraging it. Ask yourself, as a user of any kind of NETCONF/YANG application what was the last time you looked at a <code>*.yang</code> file content and found something that was needed to consume that application? In a user role I personally never look at a YANG source, though, I look at the tree or HTML representation of YANG all the time; Thats is the YANG human interface for me.</p> <p>And even in these human friendly formats you can't find all the answers; for example, looking at the YANG tree, how do you get the XML data sample of a given leaf? Thats what we will discover in this post.</p>","tags":["netconf","docker","yang"]},{"location":"2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/#problem-statement","title":"Problem statement","text":"<p>Getting the XML data sample of a given leaf? What is this, why might I need it?</p> <p>Lets work through a real-life example that should make a perfect point. Suppose you need to get a list of configured users from a given network element (NE). You would normally do this by leveraging <code>&lt;get-config&gt;</code> operation, but in order to get only the users portion of the configuration, you would need to augment your request with a filter.</p> <p>NETCONF defaults to subtree filtering when it comes to filters.</p> <pre><code>&lt;!-- subtree filter example from RFC6241 --&gt;\n&lt;rpc message-id=\"101\"\n    xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n  &lt;get-config&gt;\n    &lt;source&gt;\n      &lt;running/&gt;\n    &lt;/source&gt;\n    &lt;filter type=\"subtree\"&gt;\n      &lt;top xmlns=\"http://example.com/schema/1.2/config\"&gt;\n        &lt;users/&gt;\n      &lt;/top&gt;\n    &lt;/filter&gt;\n  &lt;/get-config&gt;\n&lt;/rpc&gt;\n</code></pre> <p>Now the question comes: how do one know how to craft the subtree filter XML data for a given NE? If you will send the above XML envelope to any NE you will receive an error, because the subtree filter provided is not how the users are modelled in the underlying YANG model.</p> <p>Yes, it boils down to an underlying model used by a given NE; you would need to consult with this model and derive the right nodes to get to the configuration block in question.</p> <p>Actually, that post is a feedback to the question that popped up in my twitter recently:</p> <p><p>Hello @nickrusso42518 @ntdvps whats the best way to transform a YANG file/section into a XML filter to use in a NETCONF get message? I used to export YANG file with pyang to html format, but it bothers me put the exact tree by hand. Are there an easy way?</p>\u2014 Rafael Ganascim (@rganascim) January 31, 2020 </p>","tags":["netconf","docker","yang"]},{"location":"2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/#solving-the-problem-with-pyang","title":"Solving the problem with PYANG","text":"<p>Rafael asked a very practical question that every NETCONF user encounters; ours example follows the same question by asking how do I know which XML data to use in my subtree filter to get users config, are there aby tools for that?</p> <p>It didn't take Rafael long to come up with a solution to his own question, which he explained in the same thread:</p> <p><p>I found an \"easy\" way to get the xml tree, instead of writing by hand</p>\u2014 Rafael Ganascim (@rganascim) January 31, 2020 </p> <p>As you can see, he leveraged PYANG and solved the problem with a grain of <code>sed</code> salt. The steps he took can be categorized with 4 major steps:</p> <ol> <li>Generated HTML view of a YANG model (jstree output format)</li> <li>Copy the path of a node in question</li> <li>Remove the prefix from that path</li> <li>Generate XML skeleton data for that cleaned path</li> </ol> <p>Lets solve our example question following this method and using Nokia SR OS router running 19.10.r2.</p> <p>First, lets enjoy the generated HTML views of Nokia SR OS models provided in the nokia-yangtree repo, no need to generate anything yourself, we value your time and here we got you covered. Few clicks away and you drill down to the <code>user</code> list of the configuration model. Thats where our configured local users live.</p> <p></p> <p>To our grief, PYANG cant digest the path that it produces in its Path column of the HTML view, therefore we need to sanitize it and remove the path prefix (<code>conf</code> in our case) from it:</p> <pre><code># path in the HTML: /conf:configure/conf:system/conf:security/conf:user-params/conf:local-user/conf:user\n/configure/system/security/user-params/local-user/user\n</code></pre> <p>SR OS PRO TIP that makes competition angry You can get the model path right out from the box when you navigate to the context of interest</p> <pre><code>*(ex)[]\nA:admin@R1# configure system security user-params local-user user del\n\n*(ex)[configure system security user-params local-user user \"del\"]\nA:admin@R1# pwc model-path\nPresent Working Context:\n/nokia-conf:configure/system/security/user-params/local-user/user=del\n</code></pre> <p>Now all you need is to copy that path and remove the user key.</p> <p>Having the model path without the context we can generate the XML data using the <code>sample-xml-skeleton</code> output of PYANG.</p> <p>For that step I leverage the open YANG models of SR OS that you can download from the 7x50_YANG_MODELS repo and the PYANG tool in a container:</p> <pre><code>$ docker run --rm -v $(pwd):/yang hellt/pyang pyang -f sample-xml-skeleton --sample-xml-skeleton-path \"/configure/system/security/user-params/local-user/user\" nokia-conf-combined.yang\n&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;data xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n  &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n    &lt;system&gt;\n      &lt;security&gt;\n        &lt;user-params&gt;\n          &lt;local-user&gt;\n            &lt;user&gt;\n              &lt;user-name/&gt;\n              &lt;home-directory/&gt;\n              &lt;password/&gt;\n              &lt;cli-engine&gt;\n                &lt;!-- # entries: 0..2 --&gt;\n              &lt;/cli-engine&gt;\n&lt;!-- &lt;&lt;SNIP&gt;&gt; --&gt;\n            &lt;/user&gt;\n          &lt;/local-user&gt;\n        &lt;/user-params&gt;\n      &lt;/security&gt;\n    &lt;/system&gt;\n  &lt;/configure&gt;\n&lt;/data&gt;\n</code></pre> <p>Pretty neat, right? You have the path to the node you specified as well as all the enclosed containers, lists and leafs so you can filter on them.</p> <p>In our case we can cut everything that sits under the <code>&lt;user&gt;</code> node and get the portion of XML data that is ready to be filled in a subtree filter:</p> <pre><code>&lt;!-- this data is ready to be pasted in a subtree template --&gt;\n  &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n    &lt;system&gt;\n      &lt;security&gt;\n        &lt;user-params&gt;\n          &lt;local-user&gt;\n            &lt;user&gt;\n            &lt;/user&gt;\n          &lt;/local-user&gt;\n        &lt;/user-params&gt;\n      &lt;/security&gt;\n    &lt;/system&gt;\n  &lt;/configure&gt;\n&lt;/data&gt;\n</code></pre> <p>This is a <code>get-config</code> template XML envelope:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;rpc message-id=\"getBGPNBRstate\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n    &lt;get&gt;\n        &lt;filter&gt;\n        &lt;/filter&gt;\n    &lt;/get&gt;\n&lt;/rpc&gt;\n</code></pre> <p>Just paste it in the <code>get-config</code> XML envelope like this and save it in a <code>get-users.xml</code> file:</p> <pre><code>&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;rpc message-id=\"getBGPNBRstate\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n    &lt;get&gt;\n        &lt;filter&gt;\n          &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n              &lt;security&gt;\n                &lt;user-params&gt;\n                  &lt;local-user&gt;\n                    &lt;user&gt;\n                    &lt;/user&gt;\n                  &lt;/local-user&gt;\n                &lt;/user-params&gt;\n              &lt;/security&gt;\n            &lt;/system&gt;\n          &lt;/configure&gt;\n        &lt;/filter&gt;\n    &lt;/get&gt;\n&lt;/rpc&gt;\n</code></pre> <p>Now its ready to be tested (using netconf-console in a docker container):</p> <pre><code>[root@infra ~]# docker run -it --rm -v $(pwd):/rpcs hellt/netconf-console --host=10.1.0.11 --port=830 -u admin -p admin /rpcs/get-users.xml\n&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;rpc-reply xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"getBGPNBRstate\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;password&gt;$2y$10$TQrZlpBDra86.qoexZUzQeBXDY1FcdDhGWdD9lLxMuFyPVSm0OGy6&lt;/password&gt;\n                                &lt;cli-engine&gt;md-cli&lt;/cli-engine&gt;\n                                &lt;cli-engine&gt;classic-cli&lt;/cli-engine&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                    &lt;ftp&gt;true&lt;/ftp&gt;\n                                    &lt;snmp&gt;true&lt;/snmp&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                                &lt;console&gt;\n                                    &lt;member&gt;administrative&lt;/member&gt;\n                                    &lt;member&gt;netconf&lt;/member&gt;\n                                &lt;/console&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre> <p>Works!</p>","tags":["netconf","docker","yang"]},{"location":"2020/getting-xml-data-sample-for-a-given-leaf-in-a-yang-model/#automating-the-solution","title":"Automating the solution","text":"<p>Seeing this in action got me itching; I wanted to automate this process so it would be generic and less manual. For that reason I enriched my Pyang-docker tool with a tiny shell script that will:</p> <ol> <li>Automatically strip the path prefix from the string copied from HTML representation of a model</li> <li>Call pyang with the right flags to generate the xml data</li> </ol> <p>Now when you copy-paster your model path from the HTML you can immediately get the XML data skeleton with:</p> <pre><code>$ docker run --rm -v $(pwd):/yang hellt/pyang xmlsk.sh \"/conf:configure/conf:system/conf:security/conf:user-params/conf:local-user/conf:user/conf:user-name\" nokia-conf-combined.yang\n\n&lt;?xml version='1.0' encoding='UTF-8'?&gt;\n&lt;data xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n  &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n    &lt;system&gt;\n      &lt;security&gt;\n        &lt;user-params&gt;\n          &lt;local-user&gt;\n            &lt;user&gt;\n              &lt;user-name/&gt;\n            &lt;/user&gt;\n          &lt;/local-user&gt;\n        &lt;/user-params&gt;\n      &lt;/security&gt;\n    &lt;/system&gt;\n  &lt;/configure&gt;\n&lt;/data&gt;\n</code></pre>","tags":["netconf","docker","yang"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/","title":"Easily exposing your local resources with ngrok and fwd","text":"<p>I bet every one of you was in a situation when you bloody needed to expose some local resource over internet. Letting a remote colleague to look at your work, delivering a demo being off-VPN, or any other reason to have your service be reachable over Internet.</p> <p>And it was never easy; corporate firewalls stand on-guard ensuring you can't be agile and productive \ud83d\ude09</p> <p>In this post I'll share with you how I glue <code>ngrok</code> and <code>fwd</code> tools together to make my routers management interfaces exposed over Internet in a few clicks for free.</p> <p>My story will be based on the following \"network-automation engineer's\" requirement:</p> <ol> <li>Make router's management interfaces available over internet (SSH, Netconf, gNMI)</li> <li>Make this \"exposure\" process quick to set up/tear down</li> <li>Make it free of charge</li> </ol> <p>In my case I am talking about management interfaces of a router, but it can be any application that relies on TCP/UDP transport that you can expose that way.</p> <p>By the end of this post you will see that we deliver on all of these requirements.</p>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#you-shall-not-pass","title":"You shall (not) pass","text":"<p>You have a router with these shiny management interfaces. But this poor thing is so locked up...</p> <p></p> <p>Most of the times there are zero chances you can configure ingress access due to the myriads of elements out of your control (corp firewall being the most tricky one).</p> <p>There might even not be any EVE-NG or Openstack in the picture, but this BFF9000 fella will be there, and we are about to penetrate it.</p>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#enter-ngrok","title":"Enter ngrok","text":"<p>Since opening the ports or configuring the port forwarding is not gonna help us much, we will use the reverse technique: opening the connections in the egress direction. Sending traffic in the egress direction is usually not a problem, we can leverage ngrok and expose the needed ports through it:</p> <p></p> <p>If you never heard of <code>ngrok</code>, I suggest you go through their website, its a very powerful tool to setup tunnels.</p> <p>That way we can expose any TCP/UDP port of a machine that runs <code>ngrok</code> client via publicly accessible sockets (<code>tcp://xx.ngrok.io:&lt;port&gt;</code>) for free.</p> <p>That looks like a nice idea and quite a realistic one, but not that many routers allow users to install ELF binaries and run them. This means that we can't run <code>ngrok</code> client on the router itself and establish tunnels.</p> <p>But we can definitely install ngrok on a machine that can reach our routers. In my case that will be the EVE-NG hypervisor/VM, since it can reach all the routers that run inside of it.</p>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#enter-fwd","title":"Enter fwd","text":"<p>Since <code>ngrok</code> is only capable of exposing the ports of the host the client runs on, we would end up with ports of the Linux host exposed, but not the router's. The missing piece is the forwarder process that will stitch the <code>ngrok</code>-exposed ports of the linux host with the respective ports of the remote router.</p> <p></p> <p>Let's dissect this two stage process of setting the tunnels up:</p> <ol> <li>expose the local ports of the linux VM with <code>ngrok</code>. This linux VM can reach the router.</li> <li>run <code>fwd</code> tool that will forward the traffic appearing on the local ports we exposed with <code>ngrok</code> in step 1</li> </ol> <p>That way we bridge the linux ports exposed with <code>ngrok</code> with the ports on the router. If that sounds confusing, lets go through the example.</p>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#socat-vs-fwd","title":"socat vs fwd","text":"<p>If you (as me) will experience some issues with <code>fwd</code> reporting broken pipes there is an old-school alternative - <code>socat</code>. As with <code>fwd</code>, you can concatenate connections in the following way:</p> <pre><code># requests coming to localhost:11122 will be forwarded to 10.2.0.11:22\nsocat tcp-listen:11122,reuseaddr,fork tcp:10.2.0.11:22\n</code></pre>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#practical-example","title":"Practical example","text":"<p>We start first with exposing the ports of the machine that runs <code>ngrok</code> client and has IP reachability with our router. To expose multiple ports in a quick and easy way, I suggest you leverage the <code>ngrok</code> configuration file which can resemble smth like this:</p> <pre><code># cat ngrok.cfg\nauthtoken: \"MYTOKEN\"\nlog_level: warn\nlog: /tmp/ngrok.log\nregion: eu\ntunnels:\n  gnmi_r1:\n    addr: 57401\n    proto: tcp\n  nc_r1:\n    addr: 11831\n    proto: tcp\n  ssh_r1:\n    addr: 11122\n    proto: tcp\n</code></pre> <p>With this configuration we command <code>ngrok</code> to expose the TCP ports <code>57401, 831, 11122</code> on the linux VM.</p> <pre><code>ngrok start -config ngrok_cfg.cfg --all\n</code></pre> <p>And voil\u00e1, these ports are now Internet-reachable:</p> <pre><code>ngrok by @inconshreveable\n\nSession Status                online\nAccount                       cats_admin (Plan: Free)\nVersion                       2.3.35\nRegion                        Europe (eu)\nWeb Interface                 http://127.0.0.1:4040\nForwarding                    tcp://0.tcp.eu.ngrok.io:13621 -&gt; localhost:11831\nForwarding                    tcp://0.tcp.eu.ngrok.io:16704 -&gt; localhost:57401\nForwarding                    tcp://0.tcp.eu.ngrok.io:19968 -&gt; localhost:11122\n\nConnections                   ttl     opn     rt1     rt5     p50     p90\n                              0       0       0.00    0.00    0.00    0.00\n</code></pre> <p>But this won't work yet as we wanted.</p> <pre><code>ssh -p 19968 admin@0.tcp.eu.ngrok.io\nssh_exchange_identification: Connection closed by remote host\n</code></pre> <p>As we clarified before, nothing listens on these ports, since they are not of router' but of the linux machine running <code>ngrok</code> client.</p> <p>There is one step left. Start the <code>fwd</code> processes and stitch the local ports with the router' ports. Since <code>fwd</code> cant read (yet) the configuration file, I created a dumb script:</p> <pre><code>fwd --from localhost:57401 --to 10.1.0.11:57400 &amp;\nfwd --from localhost:11831 --to 10.1.0.11:830 &amp;\nfwd --from localhost:11122 --to 10.1.0.11:22 &amp;\n</code></pre> <p>Thats the missing piece to propagate our tunnels all the way to the router with IP address of <code>10.1.0.11</code> in my example. And now we're in business!</p> <p>ssh:</p> <pre><code>$ ssh -p 19968 admin@0.tcp.eu.ngrok.io\n\nadmin@0.tcp.eu.ngrok.io's password:\n\n SR OS Software\n Copyright (c) Nokia 2019.  All Rights Reserved.\n[]\nA:admin@R1#\n</code></pre> <p>netconf:</p> <pre><code>$ ssh -p 13621 admin@0.tcp.eu.ngrok.io -s netconf\n\nadmin@0.tcp.eu.ngrok.io's password:\n&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;\n&lt;hello xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\"&gt;\n    &lt;capabilities&gt;\n        &lt;capability&gt;urn:ietf:params:netconf:base:1.0&lt;/capability&gt;\n        &lt;capability&gt;urn:ietf:params:netconf:base:1.1&lt;/capability&gt;\n</code></pre> <p>gNMI:</p> <pre><code>$ myAwesomegNMIClient -a 0.tcp.ngrok.io:16704 -u admin -p admin --insecure cap\ngNMI_Version: 0.7.0\nsupported models:\n  - nokia-conf, Nokia, 19.10.R2\n  - nokia-state, Nokia, 19.10.R2\n  - nokia-li-state, Nokia, 19.10.R2\n  - nokia-li-conf, Nokia, 19.10.R2\n</code></pre> <p>Now to stop this we simply kill the <code>fwd</code> processes and ngrok:</p> <pre><code>pkill fwd &amp;&amp; pkill ngrok\n</code></pre>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/easily-exposing-your-local-resources-with-ngrok-and-fwd/#summary","title":"Summary","text":"<ul> <li>With <code>ngrok</code> and <code>fwd</code> we have been able to expose the local router' ports with two clicks in the terminal.</li> <li>The tunnels are persistent and free to use.</li> <li>Tear down process is as simple as <code>pkill fwd &amp;&amp; pkill ngrok</code></li> <li>you can monitor established tunnels with ngrok console (free)</li> <li>you can try inlets or argo tunnels for similar capabilities</li> </ul> <p>What are your ways to reach your routers in a lab, share in the comments?</p>","tags":["ngrok","fwd","gnmi","grpc","netconf","ssh"]},{"location":"2020/nokia-yang-tree-and-path-browser/","title":"Nokia YANG tree and Path Browser","text":"<p>Automation Is as Good as the Data Models is a chapter's name in the great book titled \"Network Programmability With YANG\". These days you won't bedazzle anyone by just providing the set of YANG models for the flagship network products. The models alone, albeit a great step forward, do not guarantee that programmability will start flourish. The automation tools leveraging YANG is often a missing link and in this post I am talking about the Nokia YANG tree and Path Browser tools which help both our internal automation squad and our customers to be more effective working with our YANG models.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#1-models-for-machines","title":"1 Models for machines","text":"<p>At Nokia we distribute the YANG models via our nokia/7x50_YangModels repository. This enables us to allow users to simplify the way they get the models. The challenge with these models, or any models provided in <code>.yang</code> format for that matter, is that its extremely hard for a naked eye to browse/evaluate these models when doing network automation. They are great for compilers, and not as much for us - automation engineers.</p> <pre><code>// first lines of ietf-interfaces.yang module\nmodule ietf-interfaces {\n  yang-version 1.1;\n  namespace \"urn:ietf:params:xml:ns:yang:ietf-interfaces\";\n  import ietf-yang-types {\n    prefix yang;\n  }\n  revision 2018-02-20;\n  container interfaces {\n    description\n      \"Interface parameters.\";\n    list interface {\n      key \"name\";\n      leaf name {\n        type string;\n      }\n      leaf description {\n        type string;\n      }\n      leaf enabled {\n        type boolean;\n        default \"true\";\n      }\n</code></pre> <p>Likely, browsing the <code>ietf-interfaces.yang</code> file won't make you sweat, yet it shouldn't led you to a false conclusion that YANG code representation is easy. The reality hits hard when YANG exposes its features such as <code>groupings</code> and <code>uses</code>, custom <code>typedefs</code> and multiple <code>identityrefs</code>, solid layer of <code>XPATH</code> here and there, twisted <code>imports</code> and a composition with dozens of <code>submodules</code>.</p> <p>For example, our combined model for the configuration-only data (nokia-conf-combined.yang) is 15MB in size and has 331000 lines. That is like the opposite of easy. But why is it important to peer inside the models in the first place?</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#11-why-browsing-models-is-important","title":"1.1 Why browsing models is important?","text":"<p>Truth is that every model driven (MD) interface you have in mind such as NETCONF, gNMI, RESTCONF operates on the data that is modelled in YANG. Thus every single operation you make with these interfaces eventually aligned with the underlying YANG model to access the data.</p> <p>And unless your automation suite leverages some advanced orchestrator-provided abstractions or code generated classes, you literally need to look at the YANG modules when using those management interfaces.</p> <ol> <li>NETCONF operations must have the XML envelopes created in conformance with the YANG model (example)</li> <li>gNMI paths are XPATH-like paths modelled after the underlying YANG model (example)</li> <li>RESTCONF URL embeds a model path as dictated by the YANG model (example)</li> </ol>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#2-yang-representations","title":"2 YANG representations","text":"<p>Make no mistake: regardless of the interface you pick, you end browsing YANG models and as you can imagine, scrambling through the raw YANG model representation is not an easy task. Luckily, the better looking representations of the very same models exist.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#21-tree","title":"2.1 Tree","text":"<p>The RFC8340 YANG tree representation is the one you see everywhere in the documentation and blogs. By passing the same <code>ietf-interfaces.yang</code> snippet through the <code>pyang</code> tool we transform the module to a better looking tree-like view:</p> <pre><code>+--rw interfaces\n |  +--rw interface* [name]\n |     +--rw name                        string\n |     +--rw description?                string\n |     +--rw type                        identityref\n |     +--rw enabled?                    boolean\n</code></pre> <p>Compared to the <code>.yang</code> raw view, the tree makes it way easier to glance over the model and understand the parent-child relationships of the data entry nodes and their types.</p> <p>Still, it has some serious UX drawbacks an engineer will face:</p> <ul> <li>the path information is missing. By looking at a certain leaf/container/list of a tree you can't easily say what is the path of that element starting from the root?   Yet it is very important to have this information, since it enables you to create XPATH filters for xCONF or paths to subscribe to with gNMI.</li> <li>it is hard to navigate the large models. Since its the text file you are looking at, you can't expand/collapse the data nodes on request. Its flushed to you in its entirety, and if the model is big enough you will easily loose the sense of where you are.   Consider the following snippet of a single screen of text I captured from a real YANG module; is it easy to understand where are you standing at?</li> </ul>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#22-html-tree","title":"2.2 HTML tree","text":"<p>Fortunately for us, <code>pyang</code> supports many output formats and one of them - <code>jstree</code> - is a mix of the model's tree structure with HTML features. The outcome of this mixture is the self-contained, offline HTML page that crosses off the drawbacks outlined in the previous section.</p> <p>In this mode we are having the full control on which part of the model we want to explore and which branches we want to leave collapsed to not clutter the view. This might sound like a small thing, but actually it boosts the user experience quite substantially.</p> <p>Another improvement over the textual tree view is the path information that is provided for each element of the model. As explained above, these paths are essential to have for the following reasons:</p> <ul> <li>understand the parental path of the element of interest to, say, create the NETCONF XML envelope.</li> <li>use these paths in gNMI subscribe paths.</li> <li>use these paths with the tools that can generate data based on it (like XML skeleton).</li> </ul> <p>And HTML tree delivers on that promise by providing the path information for every element: </p> <p>Its does not really strike like a needed feature when you have a compact module like ietf-interface, but consider a heavier model where a certain leaf is sometimes 10 levels deep from the root:</p> <p></p> <p>On a model like this its dead obvious that a textual tree won't be of help due to the progressively increased nesting of the elements, thus \"HTML tree\" seems like a reasonable view to use.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#3-nokia-yang-tree-repository","title":"3 Nokia YANG tree repository","text":"<p>Nokia distributes the YANG models for 7x50 routers in two forms:</p> <ul> <li>combined models: all the submodules are grouped under the respective top level roots and the following combined YANG modules are produced: <code>nokia-conf-combined.yang</code> and <code>nokia-state-combined.yang</code></li> <li>individual models: the submodules are kept in their own YANG files.</li> </ul> <p>The combined modules provide a unique one-stop shop for the <code>configuration</code> and <code>state</code> YANG view, therefore I always use the combined models as they have all the elements nicely grouped under a single root.</p> <p>Due to the substantial size of the combined models it takes quite some time for <code>pyang</code> to generate the tree views; I quickly got tired of generating the tree views for each new minor release of SR OS myself. So I decided to generate them automatically for each new set of YANG modules Nokia pushes to the nokia/7x50_YangModels repo.</p> <p>That is how hellt/nokia-yangtree repo was born. The repository features:</p> <ol> <li>various views of the Nokia combined YANG models (text tree, xml, html tree) as well as stores the XPATH paths extracted from the models. A user can clone the repo and gain access to all of these formats</li> <li>YANG Browser that serves the \"HTML tree\" views of the combined models so that our users could consume these models online without a need to generate them</li> <li>Path Browser that enables search functionality over the extracted model paths</li> </ol> repository directory layout <p>The repo navigation is built on the basis of Git tags, that means that a certain set of YANG views will be shown to a user when they select a certain tag that matches the SR OS release number:</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#31-yang-browser","title":"3.1 YANG Browser","text":"<p>As briefly explained before, the YANG Browser is merely an HTTP server that serves the HTML tree views of the combined models generated with <code>pyang</code>. I foresee this to be the main interface for the SR OS automation engineers to consume the YANG models, it is always available, easy to navigate, free and requires just a browser.</p> <p>How YANG Browser works:</p> <ol> <li>A user selects an SR OS release as shown in animation above</li> <li>Once the release is selected the <code>HTML tree</code> links in the section 2 for the relevant datastores will point to the right URLs.</li> <li>Clicking on a link will open a new tab with the HTML Tree view (note, it might take a few minutes to a browser to load and render this big HTML file).</li> </ol> HTML tree view for the <code>nokia-state-combined</code> module <p>Using this page a user can answer most of the questions related to the YANG modules used by the Nokia 7750 router.</p> <p>The always on HTML tree view is amazing, but it still has some flaws. One particular case that can't be solved with YANG Browser is filtering the model's paths with a keyword. To answer that request we created the Path Browser.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#32-path-browser","title":"3.2 Path Browser","text":"<p>Imagine a request comes in asking to identify all the leaves that relate to the alarm status of the port/chassis/fan/optics/etc. Quite a standard task for every monitoring activity, which is not easy to answer without the proper tooling.</p> <p>How would you use a YANG Browser if you don't know which containers have or haven't the alarm related leaves inside? Opening all of them will become a nightmare, as well as expanding all the elements and perform a full-text search. What would be nice to have is a search actions on the paths that have some keywords inside them, like alarm.</p> <p>For that particular set of the use cases we created the Path Browser.</p> <p>The Path Browser links are in the section 2 of the repo readme.</p> <p>First, my colleague wrote a tool that extracts a list of XPATH compatible paths for a given model. The text file with the list of paths is part of the hellt/nokia-yangtree repo.</p> <pre><code>$ head -10 sros_20.2.r2-nokia-state-combined-paths.txt\nnokia-state | /state/aaa/radius/statistics/coa/dropped/bad-authentication | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/dropped/missing-auth-policy | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/dropped/invalid | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/dropped/missing-resource | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/received | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/accepted | yang:counter32\nnokia-state | /state/aaa/radius/statistics/coa/rejected | yang:counter32\nnokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/bad-authentication | yang:counter32\nnokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/missing-auth-policy | yang:counter32\nnokia-state | /state/aaa/radius/statistics/disconnect-messages/dropped/invalid | yang:counter32\n</code></pre> <p>The format of the path entries follows the pattern of <code>module_name | path | type</code>. And having this file alone allows you to leverage CLI tools magic to filter on this massive data set:</p> <pre><code>$ grep \"/port.*alarm\" sros_20.2.r2-nokia-state-combined-paths.txt | head -5\nnokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/temperature/high-alarm | decimal64\nnokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/temperature/low-alarm | decimal64\nnokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-bias-current/high-alarm | decimal64\nnokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-bias-current/low-alarm | decimal64\nnokia-state | /state/port[port-id=*]/transceiver/digital-diagnostic-monitoring/transmit-output-power/high-alarm | decimal64\n</code></pre> <p>The paths are XPATH and gNMI compatible. You can paste it to the telemetry collector and they would work.</p> <p>The next step was to build a web service with the same functionality, so I added datatables to the mix and generated the HTML pages with the filtering capabilities built-in.</p> <p>With a service like that you can efficiently and plain easy search through the Nokia modules for the leaves having certain keywords.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/nokia-yang-tree-and-path-browser/#4-summary","title":"4 Summary","text":"<p>By leveraging the opensource tools and by writing our own paths extractor we have created a DIY YANG browsing set of instruments that greatly help network automation engineers working with Nokia gear. Understanding the utter importance of YANG, it was imperative for me to make these models more convenient to consume and, at the same time, keeping it open and free.</p> <p>As a result of that effort, the community now can use YANG Browser to breeze through the Nokia YANG modules and Path Browser comes to help when the users need to perform a search for the certain leaves.</p>","tags":["nokia","sr os","yang"]},{"location":"2020/projectdocs/","title":"Projectdocs","text":"<p>I am a firm believer that documentation is an integral part of the project. A terse, twisted, incomplete or sometimes even missing documentation penalizes your projects success. At the same time clean, concise and comprehensive documentation is not only something worth being proud of, but an opening to a users' appreciation and fame.</p> <p>I am sharing the way I build, publish and host documentation sites for my projects via this live-example site - projectdocs.netdevops.me</p>","tags":["documentation","mkdocs-material"]},{"location":"2020/using-wireshark-remote-capture-with-eve-ng/","title":"Using Wireshark remote capture with EVE-NG","text":"<p>The power of a packet capture is boundless... Sometimes its indeed a pcap that can save you nights of troubleshooting, so being able to get one quickly and easily is an ace up a neteng sleeve. In this post I'll show you how I use Wireshark's remote capture ability to sniff on packets running in EVE-NG without being need to install any custom plugins or packages from EVE.</p> <p>EVE-NG provides some integration packs with wrappers around Wireshark's remote capture feature to make capturing a one-click task. The integration pack has all the needed software and some Duct tape to make it all work:</p> <pre><code>plink 0.73 (for wireshark)\nall necessary wrappers\nIt will modify windows registry files for proper work\n</code></pre> <p>I would rather want to keep my registry untouched for a simple task like sniffing the packets from a remote location, therefore I always use Wireshark remote capture without installing any client packs from Eve. It feels more \"appropriate\", though I wouldn't mind to install the pack in a VM that I don't care about much.</p> <p>So, you are perfectly capable of sniffing on packets running in EVE by having Wireshark alone. Thats the procedure:</p> <ol> <li>Install wireshark</li> <li>In the EVE lab view grep the link name of an interface you want to capture from 2.1 right click on the device you want to capture from 2.2 select \"Capture\" menu 2.3 move mouse over the interface you want to capture from 2.4 get the interface name (<code>vunl0_1_0</code> in my example)</li> <li>Open Wireshark and choose remote capture in the list of the capture interfaces </li> <li>Enter the address of your EVE hypervisor (can use names of your systems from ssh_config)     </li> <li>Type down the interface name you got in step 2 (the <code>capture filter</code> statement generates automatically)     </li> <li>Start capturing!</li> </ol> <p>It might look like a lot of manual steps from the first sight, but it takes actually 10 seconds, since you only need to memorize the link name and type it once in the wireshark interface.</p>","tags":["wireshark","eve-ng"]},{"location":"2020/netconf-subtree-filtering-by-example/","title":"NETCONF subtree filtering by example","text":"<p>If you pick a random NetEng and ask them if they love NETCONF they would likely say \"Nah\". The hate-hate love-hate kind of relationship with NETCONF mostly roots in its XML layer that one can't swap out. But if we set the XML-related challenges aside, it will become clear that NETCONF is a very well designed management interface with lots of capabilities.  </p> <p>In this topic we will touch on the NETCONF's subtree filtering capabilities.</p> <p>NETCONF's RFC 6241 defines two methods for filtering contents on the server (router) side:</p> <ul> <li>Subtree filtering - mandatory for a NETCONF-enabled device to support</li> <li>XPATH filtering - an optional capability</li> </ul> <p>Subtree filtering is powered by the following components:</p> <ul> <li>Namespace Selection</li> <li>Attribute Match Expressions</li> <li>Containment Nodes</li> <li>Selection Nodes</li> <li>Content Match Nodes</li> </ul> <p>They are very well explained in the RFC, so I won't bother with copy-pasting the definition and the rules these filtering components follow. Instead we will focus on the practical examples and put Selection and Content Match nodes to work in different scenarios.</p>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#1-selection-nodes","title":"1 Selection nodes","text":"<p>Selection node allow us to get a node and all its nested elements. Our simple examples will revolve around interactions with local users configuration on a Nokia SR OS which is modelled with the following YANG model:</p> <pre><code>module: nokia-conf\n  +--rw configure\n     +--rw system\n     |  +--rw security\n     |  |  +--rw user-params\n     |  |     +--rw local-user\n     |  |        +--rw user* [user-name]\n     |  |           +--rw user-name     types-sros:named-item\n     |  |           +--rw password      types-sros:hashed-leaf\n     |  |           +--rw access\n     |  |           |  +--rw console?   boolean\n     |  |           |  +--rw ftp?       boolean\n     |  |           |  +--rw snmp?      boolean\n     |  |           |  +--rw netconf?   boolean\n     |  |           |  +--rw grpc?      boolean\n     |  |           |  +--rw li?        boolean\n     |  |           +--rw console\n                       +--rw member*    -&gt;../aaa/local-profiles\u2026\n</code></pre> <p>If we want to filter all the configuration information related to the local users we could use Selection node <code>&lt;local-user/&gt;</code> in our get-config RPC:</p> <pre><code>&lt;get-config&gt;\n    &lt;source&gt;\n    &lt;running /&gt;\n    &lt;/source&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user/&gt;  &lt;!-- selection node --&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/filter&gt;\n&lt;/get-config&gt;\n</code></pre> <p>Hint #1: Nokia-yangtree is a beautiful way to explore Nokia YANG models. Hint #2: I recommend netconf-console to talk NETCONF to your routers.</p> <p>If we translate this get-operation command to plain English it would sound like: Dear router, can you please return everything you have under <code>local-user</code> node in the running configuration datastore? And that is what router replies back:</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:f00ec433-17b3-4bcb-9d83-c3557794e56e\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;password&gt;$2y$10$Ro5MzyBZ18eVve/aTIYt..fSBbyJar11QGcQbixrVPfxLcpXeZ4eu&lt;/password&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                                &lt;console&gt;\n                                    &lt;member&gt;administrative&lt;/member&gt;\n                                &lt;/console&gt;\n                            &lt;/user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;roman&lt;/user-name&gt;\n                                &lt;password&gt;$2y$10$xkqn46jNHBUJWit446j2o.Yu3E9zWOg44yRGjRK2YjRZE4p5xFjmG&lt;/password&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                &lt;/access&gt;\n                                &lt;console&gt;\n                                    &lt;member&gt;default&lt;/member&gt;\n                                &lt;/console&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre> <p>The answer satisfies the request we specified. Router dumps everything it has under <code>local-users</code>.</p>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#11-multiple-selection-nodes","title":"1.1 Multiple selection nodes","text":"<p>But what is we don't want to get back all that information about the local users and just interested in the account names and their access methods? That is as well the work for Selection nodes. But instead of referencing a container or a list with the Selection node, we will pinpoint the nodes of interest - <code>user-name</code> and <code>access</code>:</p> <pre><code>&lt;get-config&gt;\n    &lt;source&gt;\n    &lt;running /&gt;\n    &lt;/source&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name /&gt;\n                                &lt;access /&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/filter&gt;\n&lt;/get-config&gt;\n</code></pre> <p>Pay attention that it doesn't matter what type of node we are referencing with a Selection node. It can be a container, a list, a leaf. If a selected node happens to have nested elements they will be returned as well.</p> <p>In the example above we reference the <code>user-name</code>  leaf and the <code>access</code> container, as a result we receive back a concrete data stored as the <code>user-name</code> node and everything that exists under the <code>access</code> container:</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:4c646ef4-601b-465e-ba35-f90953527a73\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;roman&lt;/user-name&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#12-selection-nodes-in-different-containment-nodes","title":"1.2 Selection nodes in different containment nodes","text":"<p>It is totally fine to have the Selection nodes under different containment nodes. That allows you to filter the information from different nodes in a single request.</p> <p>What if we wanted not only to see which users are configured on a box, but also to see how many login attempts each of them made? Thats a perfect example how Selection nodes from different containment nodes play well together.</p> <pre><code>&lt;get&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name /&gt;  &lt;!-- selection node in context A --&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n        &lt;state xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;attempted-logins /&gt;  &lt;!-- selection node in context B --&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/state&gt;\n    &lt;/filter&gt;\n&lt;/get&gt;\n</code></pre> <p>Here we used Selection nodes even in two different YANG datastores and getting both configuration and state data in a single reply:</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:c622d500-b780-4765-a71e-8f6b354beff4\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                            &lt;/user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;roman&lt;/user-name&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n        &lt;state xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;attempted-logins&gt;74&lt;/attempted-logins&gt;\n                            &lt;/user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;roman&lt;/user-name&gt;\n                                &lt;attempted-logins&gt;0&lt;/attempted-logins&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/state&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#2-content-match-nodes","title":"2 Content Match nodes","text":"<p>In many cases it is needed to filter not only on the node itself (what Selection node does), but also on the value of the referenced leaf. That is a work for Content Match nodes.</p> <p>Using our local users examples that translates to a need to filter the information of a single user only. Let's get the configuration of the <code>admin</code> user only by using the Content Match node semantics:</p> <pre><code>&lt;get-config&gt;\n    &lt;source&gt;\n    &lt;running /&gt;\n    &lt;/source&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;  &lt;!-- Content Match node --&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/filter&gt;\n&lt;/get-config&gt;\n</code></pre> <p>Content Match nodes filtering is only applicable to the leafs, in our example that was <code>user-name</code> which we set to <code>admin</code>. As a result, we got back the configuration related to the <code>admin</code> user only:</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:5c1da160-69e1-466a-97c0-541f3add8f2d\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;password&gt;$2y$10$Ro5MzyBZ18eVve/aTIYt..fSBbyJar11QGcQbixrVPfxLcpXeZ4eu&lt;/password&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                                &lt;console&gt;\n                                    &lt;member&gt;administrative&lt;/member&gt;\n                                &lt;/console&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#21-multiple-content-match-nodes","title":"2.1 Multiple Content Match nodes","text":"<p>By adding multiple Content Match nodes in your filter request you add an implicit <code>AND</code> operand between them. Lets say we want to list the configured users who both have access to netconf and grpc. We can craft such a filter request by using two Content Match nodes expressions:</p> <pre><code>&lt;get-config&gt;\n    &lt;source&gt;\n    &lt;running /&gt;\n    &lt;/source&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;access&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/filter&gt;\n&lt;/get-config&gt;\n</code></pre> <p>In the end we get our single user - <code>admin</code> - who has access to the subsystems we put in a filter, cool!</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:33d1666e-de58-4414-8c4d-374bd73d8ef2\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;access&gt;\n                                    &lt;console&gt;true&lt;/console&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                    &lt;grpc&gt;true&lt;/grpc&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#3-content-match-and-selection-nodes","title":"3 Content Match and Selection nodes","text":"<p>Another interesting filtering technique is combining Selection and Content Match nodes. Quite often you want to filter on the content, but at the same time limit the amount of data that router replies back. That might be very expensive for a router to return every sibling when only Content Match node is used, therefore its a good practice to craft a filter that will contain only the needed information.</p> <p>Talking our local users database me might want to know if <code>admin</code> user has access to <code>netconf</code> subsystem and we don't care at all about any other configuration that user has. Thats a perfect candidate for a combination of Content Match and Selection nodes:</p> <pre><code>&lt;get-config&gt;\n    &lt;source&gt;\n    &lt;running /&gt;\n    &lt;/source&gt;\n    &lt;filter&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;  &lt;!-- content match --&gt;\n                                &lt;access&gt;\n                                    &lt;netconf/&gt;                 &lt;!-- selection --&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/filter&gt;\n&lt;/get-config&gt;\n</code></pre> <p>And look at what a concise and clear response we got back. It has only the information we cared about.</p> <pre><code>&lt;rpc-reply xmlns:nc=\"urn:ietf:params:xml:ns:netconf:base:1.0\" xmlns=\"urn:ietf:params:xml:ns:netconf:base:1.0\" message-id=\"urn:uuid:02b36787-e805-4370-ac7b-b569a14d2e64\"&gt;\n    &lt;data&gt;\n        &lt;configure xmlns=\"urn:nokia.com:sros:ns:yang:sr:conf\"&gt;\n            &lt;system&gt;\n                &lt;security&gt;\n                    &lt;user-params&gt;\n                        &lt;local-user&gt;\n                            &lt;user&gt;\n                                &lt;user-name&gt;admin&lt;/user-name&gt;\n                                &lt;access&gt;\n                                    &lt;netconf&gt;true&lt;/netconf&gt;\n                                &lt;/access&gt;\n                            &lt;/user&gt;\n                        &lt;/local-user&gt;\n                    &lt;/user-params&gt;\n                &lt;/security&gt;\n            &lt;/system&gt;\n        &lt;/configure&gt;\n    &lt;/data&gt;\n&lt;/rpc-reply&gt;\n</code></pre> <p>In the simplified local users database example that might not seem critical, but on a real network element you might filter through hundreds of configuration elements while only cared about a single one. Then it makes all the sense to combine Content Match nodes with Selection nodes to minimize the payload sizes and computation times.</p>","tags":["netconf"]},{"location":"2020/netconf-subtree-filtering-by-example/#summary","title":"Summary","text":"<p>NETCONF Subtree filtering is a powerful mechanism that is both easy to use and reason about. By using Contaiment, Selection and Content Match nodes one can easily filter anything, while maintaining efficiency and cleanliness of the filter construct.</p> <p>Remember that using Selection nodes with Content Match nodes allow you to follow the beast practices and request only the information that you need, without clutter.</p>","tags":["netconf"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/","title":"Containerlab - your network-centric labs with a Docker UX","text":"<p>With the growing number of containerized Network Operating Systems (NOS) grows the demand to easily run them in the user-defined, versatile lab topologies. Unfortunately, container runtimes alone and tools like docker-compose are not a particularly good fit for that purpose, as they do not allow a user to easily create p2p connections between the containers.</p> <p>Containerlab provides a framework for orchestrating networking labs with containers. It starts the containers, builds a virtual wiring between them to create a topology of users choice and then manages a lab lifecycle.</p> <p></p> <p>Containerlab focuses on containerized Network Operating Systems such as:</p> <ul> <li>Nokia SR-Linux</li> <li>Arista cEOS</li> <li>Azure SONiC</li> <li>Juniper cRPD</li> <li>FRR</li> </ul> <p>In addition to native containerized NOSes, containerlab can launch traditional virtual-machine based routers using vrnetlab integration:</p> <ul> <li>Nokia virtual SR OS (vSim/VSR)</li> <li>Juniper vMX</li> <li>Cisco IOS XRv</li> <li>Arista vEOS</li> </ul> <p>And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span contain all the needed variants of nodes:</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#the-why","title":"The WHY","text":"<p>As it often happens, https://containerlab.srlinux.dev was created by engineers to address their needs.</p> <p>In containerlab's case the need was simple - to be able to create networking topologies with containerized Network Operating Systems</p> <p>As you might know, the off-the-shelf tools like docker-compose are not really fit-for-purpose of defining a multi-interfaced containers, therefore many of us created the bespoke bash scripts ruling a web of veth pairs between containers.</p> <p>Containerlab solves this, and helps many other pain points you might have seen while running your labs.</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#the-what","title":"The WHAT","text":"<p>Containerlab is what docker-compose would be if it was created with networking topologies in mind.</p> <p>We use so-called <code>clab files</code> to define a topology that is then deployed by containerlab anywhere, where docker runs without any 3<sup>rd</sup> party dependencies.</p> <p></p> <p>The <code>clab file</code> is a YAML in disguise, it offers a way to define your topology top-to-bottom.</p> <p>Balancing between the simplicity, conventionality and expressiveness it allows users to define topologies that are both easy to read/write and yet are not limited in features)</p> <pre><code>name: srlceos01\n\ntopology:\n  nodes:\n    srl:\n      kind: srl\n      image: srlinux:20.6.3-145\n      license: license.key\n    ceos:\n      kind: ceos\n      image: ceos:4.25.0F\n\n  links:\n    - endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#the-how","title":"The HOW","text":"<p>This <code>clab file</code> is all that is needed to spin up a lab of the two interconnected nodes - Nokia SR Linux and Arista cEOS.</p> <p>Yes, that is all that's needed. No bulky emulators, no bespoke datapaths. A pure container-based lab powered by linux networking primitives.</p> <p>That is what you get:</p> <p></p> <p>All the heavy lifting of launching the containerized NOS is abstracted by containerlab kinds. It knows how to start SR Linux and cEOS. Just tell it which kind you need and what image to use</p> <p>No need to keep handy those endless ENV vars or lengthy  commands.</p> <p>Interconnecting the nodes is as easy as writing a string of text. Tell containerlab which interfaces you want to be interconnected, and it will create the veth pairs blazingly fast.</p> <p></p> <p>And surely enough, that is just the tip of an iceberg, containerlab packs a ton of features which I won't repeat here, as they are all mentioned in the docs site we carefully maintain.</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#multivendor-capabilities","title":"Multivendor capabilities","text":"","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#arista-ceos","title":"Arista cEOS","text":"<p>Although containerlab was born in Nokia, it is now truly multivendor.</p> <p>Arista folks reading this? Here is a full blown support for cEOS</p> <p>Run cEOS as a first class citizen, it even makes cEOS to respect the docker assigned IP address.</p> <p><p>Although containerlab was born in Nokia, it is now truly multivendor.Arista folks, you there? @burneeed @flat_planet @TiTom73 @loopback1 https://t.co/N9OJQByszRContainerlab can run cEOS as a first class citizen, it even makes cEOS to respect the docker assigned IP address pic.twitter.com/HWFpMSyiAE</p>\u2014 Roman Dodin (@ntdvps) April 1, 2021 </p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#juniper-crpd","title":"Juniper cRPD","text":"<p>We don't pick sides in our choice of containerized NOS support, so Juniper cRPD is as welcome as any other NOS.</p> <p></p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#sonic","title":"SONiC","text":"<p>Yes, SONiC is there as well and we spent some time to make it beautifully integrated and start up just as any other NOS.</p> <p>A perfect candidate to be paired with Nokia SR Linux and see a modern DC interop use cases through and through.</p> <p></p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#frr","title":"FRR","text":"<p>Coming from the free OSS NOS camp? FRR is also under containerlab umbrella.</p> <p>Basically, any Linux based NOS that you can image will be able to be run by containerlab, as it is agnostic to the packages inside the linux container.</p> <p>Containerlab is extensible, and if anything that is dear to your heart is missing it definitely can be added.</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#network-node-regular-containers","title":"Network node + regular containers","text":"<p>Also remember that the same clab file can really be like docker-compose file.</p> <ul> <li>\u2705 Need to bind mount files/dirs to your network node</li> <li>\u2705 Want to expose a port to a container host</li> <li>\u2705 Maybe set ENV vars</li> <li>\u2705 Or change/augment the CMD the node runs</li> </ul> <p>I am repeating myself, but can't stress this enough, containerlab clab files are a mix of a docker-compose and some networking stardust. That means that you can define a topology that will have both linux containers and network nodes.</p> <p>A perfect example - a telemetry lab.</p> <p></p> <p>The above topology is defined in a single clab file that has your networking nodes and regular linux container defined.</p> <p>A single gittable, versionable lightweight text file defines a ready-made topology that is spinnable in 15 seconds.</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#what-about-my-vm-based-routers","title":"What about my VM-based routers?","text":"<p>I can imagine how @ioshints says that this container-ish thingy can't stand a chance vs real-deal qcow2 packaged VMs.</p> <p>Yes, a valid concern, but we are lucky that guys like @plajjan did some splendid work that we leveraged in containerlab.</p> <p>Watch my hands.</p> <p>Containerlab can run classic VMs like Nokia SR OS, Cisco IOS-XR, Juniper vMX, Arista vEOS in the container packaging. Yes, defined in the same clab file.</p> <p>This is possible by using our adapted version of vrnetlab project - https://github.com/hellt/vrnetlab</p> <p></p> <pre><code>name: vr04\n\ntopology:\n  nodes:\n    srl:\n      kind: srl\n      image: srlinux:20.6.3-145\n      license: license.key\n    xrv9k:\n      kind: vr-xrv9k\n      image: vr-xrv:7.2.1\n\n  links:\n    - endpoints: [\"srl:e1-1\", \"xrv9k:eth1\"]\n</code></pre> <p>With this you can turn any EVE-NG or GNS lab that you have into a clab file. By packaging your routers into container images you can push them into a registry and enjoy your labs with a docker UX.</p> <p>Total control about reproducibility.</p> <p></p> <p>In fact, in Nokia many engineers already transitioned from virsh/EVE/GNS to containerlab and they helped us refine containerlab to make it play nicely with classic VM-based products.</p> <p>The benefits of treating a router VM as a container are quite compelling.</p> <p><p>In fact, in Nokia many engineers already transitioned from virsh/EVE/GNS to containerlab and they helped us refine containerlab to make it play nicely with classic VM-based products.The benefits of treating a router VM as a container are quite compelling. pic.twitter.com/B5LnDpxDtX</p>\u2014 Roman Dodin (@ntdvps) April 1, 2021 </p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#sharing-lab-access","title":"Sharing lab access","text":"<p>Ok, so in the same containerlab file we can</p> <ul> <li>run any linux container</li> <li>run most popular containerized NOSes</li> <li>run VM-based routers</li> </ul> <p>what else?</p> <p>Globe with meridians here goes the story about our collab with @atoonk and his @mysocketio service</p> <p>There is a thing about 'em labs. They usually run in a closed, isolated environments, with a handful of ppl having access to it.</p> <p>But quite often you find yourself in need to share access to this lab. And then it becomes a battle of a hundred SSH tunnels and exposed credentials.</p> <p>By integrating mysocketio service into containerlab we achieved an on-demand, stable &amp; secure and lab access sharing.</p> <p>Check out this short video that explains the concepts:</p> <p></p> <p>Just like this, adding a single line to your node definition, you make it available via Internet over the anycast network with optional strict OAuth rules for a fine grained access control.</p> <p></p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#what-are-the-use-cases","title":"What are the use cases?","text":"<p>The usecases where containerlab can shine are not only limited to labs.</p> <p>It is a perfect demo tool, you have a guarantee that your lab will run just like it always was thanks to strict versioning and immutable container images. With a very small footprint, requiring only Docker</p> <p>Another domain where we see containerlab be of a great help is CI/CD. Github Actions and Gitlab CI both have docker installed on their runners, so you can launch topologies and test them in your CI easily.</p> <p></p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#any-examples","title":"Any examples?","text":"<p>Definitely, we also launched a satellite repo for containerlab based labs - https://clabs.netdevops.me</p> <p>This catalog is meant to be an open collection of labs built with containerlab. Anything you build with containerlab I will gladly feature there with full attribution to an author.</p> <p>You will likely find more use cases that fit your need, so give https://containerlab.srlinux.dev a nice spin and let us know how it goes.</p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/containerlab---your-network-centric-labs-with-a-docker-ux/#special-thanks","title":"Special thanks","text":"<p>I want to thank @WHenderickx and @Karimtw_ who started this thing and created the core architecture.</p> <p>Then our internal users and contributors for always providing feedback and thus making containerlab better. It was a truly team work.</p> <p>A special kudos goes to @networkop1 who is always ahead of time and had a similar tool (docker-topo) created years ago. We took inspiration from it when were creating the containerlab topo file schema.</p> <p>Found this awesome, do not hesitate to star our repo - https://github.com/srl-labs/containerlab as a way of saying thanks.</p> <p>Want to contribute? That is awesome and appreciated!</p> <p>PS. The original announcement was made via this tweet-series.</p> <p><p>\ud83d\udea8 I've been sitting on my hands for 3 months, but now the time has finally come...\ud83e\udd7cWe are releasing containerlab - the open source CLI tool that may redefine the way you run networking labs.https://t.co/WZQGFWEttBIt will be a long \ud83e\uddf5but I guarantee, you will dig it.</p>\u2014 Roman Dodin (@ntdvps) April 1, 2021 </p>","tags":["containerlab","srlinux","ceos","crpd","sonic","frr","nokia","juniper","cisco","arista"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/","title":"Remove binaries and big files from Git repo","text":"<p>You slice and dice your files in a Git repo like a pro and accidentally commit a binary file. It happened to you as well, don't pretend it didn't. Sooner or later you recognizes this file shouldn't be there, it is clogging your Git repo for no reason. OK, you delete the file and commit. But the repo size doesn't get any smaller. Hm...</p> <p>Indeed, next time you do <code>git clone</code> you are wondering why your repo is still megabytes in size, while it has just some source code files?</p> <p>The thing is, by just deleting the file from your working tree and committing this action you don't make things any better. This large file still sits somewhere in <code>.git</code> directory waiting for you to rewind the history back and get it. The problem though is that you want this file gone for good.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#0-tldr","title":"0 TLDR","text":"<p>All the tags, branches are preserved with this procedure, although I do not guarantee that the workflow will work in your case. Do a backup always.</p> <pre><code># clone a repo with --mirror flag and change into it\ngit clone --mirror &lt;repo-url&gt;\n\n# launch cleanup process\ngit filter-repo --strip-blobs-bigger-than 3M\n\n# run GC\ngit reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive\n\n# update the more\ngit push\n</code></pre>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#1-show-me-big-files","title":"1 Show me big files","text":"<p>To see if your repo holds those monster files you can leverage some Git commands, but I found this self-contained python script quite a good fit for the purpose:</p> <pre><code>\u276f ./lf.py -c 20\nFinding the 20 largest objects\u2026\nFinding object paths\u2026\n\nAll sizes in kB. The pack column is the compressed size of the object inside the pack file.\n\nsize   pack  hash                                      path\n6769   6761  82d233ab6ff841f16bd17c2b5a6906ccdd8af8e5  rpm/tool-1.0.0.x86_64.rpm\n13439  6723  dbd32fc21381cf1e4cb0ba964f53aff1ebcc8547  bin/tool\n12437  6223  967237f169780b8660a771c6f478de1d93822157  bin/tool\n12413  6211  dfd93506fa17401cc996223337b8372bf921887e  bin/tool\n11776  5917  f35577a72a2493b00c6e0520d1454d9fdaedb886  bin/tool\n5646   5638  66cc7eb29577bb84aaa682dd1eb694fde1d9e399  rpm/tool-1.0.0.x86_64.rpm\n5944   4073  360106b01776e4e7419ab414878d582747d7c945  bin/tool-test\n5333   3899  53ef404d20a09db9040696eeb5df5bebf10ecf52  bin/tool\n4985   3569  1d81eafd70736f568526b7e5221478b5b3e67c6d  bin/tool\n4111   3224  acfd2077e642272c2ab09cbfaf435b4fc91ac012  bin/tool\n4018   3205  f331cec6b0e599dfbef7361c947a14beea7ce4c2  bin/tool\n3849   3111  8bb4fdaeccecf1ef0a91fc780c243cb89109597a  bin/tool\n655    456   393dadfa6f5957f60a42287ed2c6e7ddcd5688cc  bin/tool\n</code></pre> <p>Nice and easy we get 20 largest files which I have no intention to keep and they make the size of the repo to be in 70MB range with compression. No good.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#2-removing-large-files","title":"2 Removing large files","text":"","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#21-beware-of-consequences","title":"2.1 Beware of consequences","text":"<p>Now to the fun part, lets remove those files, making our repo fit again!</p> <p>One problem, though, it's not that easy. First of all, this operation is very intrusive. As Git stores the commits in a graph fashion, you can't change some commit in the middle, without rewriting the commits after it.</p> <p>So be prepared, that all the commits will eventually have new hashes. Evaluate the consequences and implications of it.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#22-procedure","title":"2.2 Procedure","text":"<p>If you start searching, you will find many workflows, dating back to early 2000s and Git 1.x. I tried them and I regret. Eventually I found the working combination that I tested on two of my repos and they worked flawlessly.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#221-make-a-backup","title":"2.2.1 Make a backup","text":"<p>Do a backup of your original repo</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#222-clone-a-mirror","title":"2.2.2 Clone a mirror","text":"<p>Now clone the repo with a <code>--mirror</code> option. That step is very important. You will have your repo cloned under <code>&lt;repo-name&gt;.git</code> directory, but you won't see actual files, instead you will have the Git database of this repo.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#223-install-git-filter-repo","title":"2.2.3 Install Git-filter-repo","text":"<p>The actual tool that does the job is called Git-filter-repo. It is a successor to Git-filter-branch and BFG Repo Cleaner.</p> <p>There is the Install document, but it is written somehow complex. The easy way to install for me was copying the raw script and copying it under the directory that <code>git --exec-path</code> command outputs.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#224-run-the-cleanup","title":"2.2.4 Run the cleanup","text":"<p>Then you can read about the options this script supports, for me I chose the easiest path possible -- delete every file that is bigger than X Megabytes. So I entered the directory that appeared after I did <code>git clone --mirror</code> and executed the following command:</p> <pre><code>git filter-repo --strip-blobs-bigger-than 3M\n</code></pre> <p>For my no-so-big repo with 500 commits, it finished in under a second. It removed all the files bigger than 3Megs and re-wrote all the commits that were affected by that change.</p>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#225-garbage-collect","title":"2.2.5 Garbage collect","text":"<p>We are not done yet. Although the files were removed for good, we still need to tell Git to run a garbage collection procedure to forget about those missing files:</p> <pre><code>git reflog expire --expire=now --all &amp;&amp; git gc --prune=now --aggressive\n</code></pre>","tags":["git"]},{"location":"2021/remove-binaries-and-big-files-from-git-repo/#226-update-the-remote","title":"2.2.6 Update the remote","text":"<p>Now the final part. We are ready to update the remote with our new Git history. Interesting enough it is done with a simple</p> <pre><code>git push\n</code></pre> <p>no <code>force</code> is needed \u00af_(\u30c4)_/\u00af</p>","tags":["git"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/","title":"Building and publishing deb/rpm packages with goreleaser and gemfury","text":"<p>I am a huge fan of a goreleaser tool that enables users to build Go projects and package/publish build artifacts in a fully automated and highly customizable way. We've have been using goreleaser with all our recent projects and we couldn't be any happier since then.</p> <p>But once the artifacts are built and published, the next important step is to make them easily installable. Especially if you provide deb/rpm packages which are built with NFPM integration.</p> <p>The \"challenge\" with deb/rpm packages comes to light when project owners want to add those packages to Apt/Yum repositories. Goreleaser doesn't provide any integrations with 3<sup>rd</sup> party repositories nor there are Apt/Yum repositories which are free and provide an API to upload artifacts. Or are there?</p>","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/#gemfury-aka-furyio","title":"Gemfury aka Fury.io","text":"<p>Actually there is at least one - the gemfury.io project that does just that (and even more).</p> <p></p> <p>Gemfury is a private package repository to help you easily reuse code without worrying about its hosting or deployment. It integrates directly with existing package management tools that you already use.</p> <p>Among other repositories, Fury provides a Yum/Apt repo for pre-built deb/rpm packages. It is free for public packages, which makes it a good choice for OSS projects. It also sports a hefty number of options to upload artifacts, from a simple <code>curl</code> to a push via its own CLI tool.</p> <p></p> <p>Just register within the service and generate a push token, and you are good to go leveraging Goreleaser to push your artifacts to Fury.</p>","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/#using-goreleaser-with-fury","title":"Using Goreleaser with Fury","text":"","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/#step-1-adding-fury-token","title":"Step 1: Adding Fury' token","text":"<p>Once you have a Fury' push token, it is a matter of a few lines of code on the Goreleaser side.</p> <p>I am using Goreleaser' Github action to build and publish artifacts, therefore I added push token to repo's secrets and added it as another environment variable of a goreleaser action:</p> <pre><code># github action workflow file\n---\nname: Release\non:\n  push:\n    tags:\n      - v*\njobs:\n  goreleaser:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v2\n        with:\n          fetch-depth: 0\n      - name: Set up Go\n        uses: actions/setup-go@v2\n        with:\n          go-version: 1.15\n      - name: Run GoReleaser\n        uses: goreleaser/goreleaser-action@v2\n        with:\n          version: v0.155.0\n          args: release --rm-dist\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          FURY_TOKEN: ${{ secrets.FURYPUSHTOKEN }}\n</code></pre> <p>This will make our <code>FURYPUSHTOKEN</code> secret value to be available inside the Goreleaser' Env vars under the <code>FURY_TOKEN</code> name.</p>","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/#step-2-add-id-for-nfpm-builds","title":"Step 2: Add ID for NFPM builds","text":"<p>In the <code>nfpm</code> section of your <code>.goreleaser.yml</code> file add <code>id</code> field. This identification string will be used in Step 3 to scope which artifacts will be pushed to Fury. Since Fury will be used exclusively for dep/rpm artifacts, by using the <code>id</code> related to them we will skip artifacts which are generated in the <code>build</code> section of goreleaser (aka archives).</p> <pre><code># .goreleaser.yml file\n&lt;SNIP&gt;\nnfpms:\n  - id: packages # here we say that artifacts built with nfpm will be identified with `packages` string.\n    file_name_template: \"{{ .ProjectName }}_{{ .Version }}_{{ .Os }}_{{ .Arch }}\"\n&lt;SNIP&gt;\n</code></pre>","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/building-and-publishing-debrpm-packages-with-goreleaser-and-gemfury/#step-3-add-custom-publisher","title":"Step 3: Add custom publisher","text":"<p>Now we need to tell Goreleaser to actually push those deb/rpm files it produced to a Fury repo. This is easily done with the custom publishers feature.</p> <pre><code>publishers:\n  - name: fury.io\n    # by specifying `packages` id here goreleaser will only use this publisher\n    # with artifacts identified by this id\n    ids:\n      - packages\n    dir: \"{{ dir .ArtifactPath }}\"\n    cmd: curl -F package=@{{ .ArtifactName }} https://{{ .Env.FURY_TOKEN }}@push.fury.io/&lt;your_username&gt;/ #(1)!\n</code></pre> <ol> <li>Do not forget to fill in your user name.</li> </ol> <p>Look how easy it is. Now on every goreleaser' build, artifacts from nfpm will be concurrently uploaded to Fury and immediately available to the users of those Apt/Yum repositories. Do note, that by default pushed artifacts have a private scope, so don't forget to visit Fury' account dashboard and make them public.</p> <p>Did I say that Goreleaser is a great tool? I bet I did, so consider supporting it if you have a chance.</p>","tags":["gemfury","apt","yum","goreleaser"]},{"location":"2021/transparently-redirecting-packetsframes-between-interfaces/","title":"Transparently redirecting packets/frames between interfaces","text":"<p>Lately I have been consumed by an idea of running container-based labs that span containerized NOSes, classical VM-based routers and regular containers with a single and uniform UX.</p> <p>Luckily the foundation was already there. With plajjan/vrnetlab you get a toolchain that cleverly packages qemu-based VMs inside the container packaging, and with networkop/docker-topo you can run, deploy and wire containers in meshed topologies.</p> <p>One particular thing though we needed to address, and it was the way we interconnect containers which host vrnetlab-created routers inside.</p> <p>Vrnetlab uses its own \"overlay datapath\" to wire up containers by means of an additional \"vr-xcon\" container that stitches the exposed sockets. Although this approach allows to re-wire containers in different topologies after the start, this was not something that we could use if we wanted use non-vrnetlab containers in our topology. Ideally I wanted to emulate p2p links between the routers (running inside containers) by veth pairs stretched between them, pretty much like docker does when it launches containers. And that is also the way docker-topo works.</p>","tags":["tc","ovs","lacp","vrnetlab"]},{"location":"2021/transparently-redirecting-packetsframes-between-interfaces/#1-linux-bridge-and-you-shall-not-pass","title":"1 Linux bridge and \"you shall not pass\"","text":"<p>Michael Kashin in his docker-topo project wanted to do the same, and he proposed to add a new connection type to vrnetlab which used linux bridges inside vrnetlab containers, thus allowing to interconnected vrnetlab containers in a docker-way:</p> <pre><code>docker create --name vmx --privileged vrnetlab/vr-vmx:17.2R1.13 --meshnet\ndocker network connect net1 vmx\ndocker network connect net2 vmx\ndocker network connect net3 vmx\n</code></pre> <p>In a nutshell, this is what was proposed by Michael:</p> <p>The router VM sitting inside the container connects to container' data interfaces <code>eth1+</code> by the means of Linux bridges. This approach is the most straightforward one, it doesn't require any additional kernel modules, it is well-known and battle-tested and it has a native support in qemu/libvirt.</p> <p>But the elephant in the room is a Linux bridge' inability to pass certain Ethernet frames - specifically LACP and STP BPDUs. And apparently LACP support is something that is badly needed in nowadays labs, as people want to test/demo EVPN multihoming. So as easy as it gets, classical bridges can't satisfy the requirement of emulating a p2p link between data interfaces.</p> <p>Off we go looking for alternatives.</p> <p>ADD: Apparently, there is a simple way to make LACP to pass over the linux bridge, another great person Vincent Bernard read the mailing list archives and found out that you can only restrict the MAC_PAUSE frames and leave LACP be. though tc solution is cleaner for the purpose of a point-to-point link.</p>","tags":["tc","ovs","lacp","vrnetlab"]},{"location":"2021/transparently-redirecting-packetsframes-between-interfaces/#2-macvtap","title":"2 Macvtap","text":"<p>Another approach that Michael tried when he was working on docker-topo was macvtap interface that looked promising on paper.</p> <p></p> <p>And although</p> <ol> <li>this approach required to mount the whole <code>/dev</code> to a container namespace,</li> <li>it had no qemu native support so we had to play with opening file descriptors</li> </ol> <p>we still tried...</p> <p>...and we failed.</p> <p>Macvtaps in bridge mode worked, but they were not passing LACP still. No matter what we tried it became evident that path is a no go.</p>","tags":["tc","ovs","lacp","vrnetlab"]},{"location":"2021/transparently-redirecting-packetsframes-between-interfaces/#3-openvswitch","title":"3 Openvswitch","text":"<p>Most of my colleagues use openvswitch bridges to interconnect classical libvirt/qemu VMs when they need to have support for LACP. With OvS all it takes is a single configuration command:</p> <pre><code>ovs-vsctl set bridge $brname other-config:forward-bpdu=true\n</code></pre> <p>The reason I didn't want to start with OvS in the first place is that it is like using a sledge-hammer when all you need is to drive through a tiny nail. OvS is heavy in dependencies, it requires a kernel module and sometimes you simply can't install anything on the host where you want to run containers.</p> <p>But with all other options exhausted, I decided to add this datapath option to my fork of vrnetlab to finally land LACP. And it worked as it should, until I started to hear complaints from users that sometimes they can't install OvS for multiple reasons.</p> <p>But there was nothing else to try, or was there? We even wanted to explore eBPF path to see if it can help here...</p>","tags":["tc","ovs","lacp","vrnetlab"]},{"location":"2021/transparently-redirecting-packetsframes-between-interfaces/#4-tc-to-the-rescue","title":"4 tc to the rescue","text":"<p>Then all of a sudden Michael pinged me with the following message:</p> <p>@hellt have you seen this? \"Using tc redirect to connect a virtual machine to a container network \u00b7 GitHub\" https://gist.github.com/mcastelino/7d85f4164ffdaf48242f9281bb1d0f9b</p> <p>This gist demonstrated how <code>tc mirred</code> function can be used to solve a task of port mirroring. Isn't this brilliant? That was exactly what we needed, to transparently redirect all layer 2 frames between a pair of interfaces. Pretty much like veth works.</p> <p>And <code>tc</code> delivered!</p> <p>With a couple of lines and no external dependencies (tc is part if iproute2 which nowadays ubiquitous) <code>tc</code> made a perfect datapath pipe between VM and container interfaces:</p> <pre><code># create tc eth0&lt;-&gt;tap0 redirect rules\ntc qdisc add dev eth0 ingress\ntc filter add dev eth0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev tap1\n\ntc qdisc add dev tap0 ingress\ntc filter add dev tap0 parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev eth1\n</code></pre> <p>Back in 2010 some RedHat engineer was looking for a way to do port-mirroring on linux host and he explained how <code>tc mirred</code> works, maybe that inspired mcastelino to write that gist that Michael found, but whichever it was, that helped to solve my case of transparently wiring container interface to a tap interface of a VM.</p> <p>And it was super easy to make it integrated with qemu, since all you need is to create an ifup script for a tap interface:</p> <pre><code>#!/bin/bash\n\nTAP_IF=$1\n# get interface index number up to 3 digits (everything after first three chars)\n# tap0 -&gt; 0\n# tap123 -&gt; 123\nINDEX=${TAP_IF:3:3}\n\nip link set $TAP_IF up\n\n# create tc eth&lt;-&gt;tap redirect rules\ntc qdisc add dev eth$INDEX ingress\ntc filter add dev eth$INDEX parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev $TAP_IF\n\ntc qdisc add dev $TAP_IF ingress\ntc filter add dev $TAP_IF parent ffff: protocol all u32 match u8 0 0 action mirred egress redirect dev eth$INDEX\n</code></pre> <p>and then use this script in qemu:</p> <pre><code>-netdev tap,id=XX,ifname=tap1,script=/etc/tc-tap-ifup,downscript=no\n</code></pre>","tags":["tc","ovs","lacp","vrnetlab"]},{"location":"2021/nokia-sr-linux-goes-public/","title":"Nokia SR Linux goes public","text":"<p>It's been almost two years since Nokia announced its Data Center Fabric solution. The three-layered solution ranged from hardware platforms all the way up in the stack to the DC fabric lifecycle management suite - Fabric Services System (FSS).</p> <p></p> <p>At the very heart of the DC Fabric solution lies a purpose-built, modern Network OS - SR Linux.</p> <p></p> <p>SR Linux comes with quite some interesting and innovative ideas. By being able to design the NOS from the ground up, the product team was freed from the legacy burdens which will be there have they decided to built the NOS on top of the existing one. Features like:</p> <ul> <li>YANG-first APIs</li> <li>Protobuf based SDK</li> <li>Disaggregated application stack</li> <li>Programmable CLI</li> </ul> <p>are the result of taking a fresh look at the modern data center networks and building the NOS for the Netdevops era.</p> <p>No wonders engineers around the world wanted to play with SR Linux and take those features for a spin first hand. And today it is finally possible!</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#public-sr-linux-container","title":"Public SR Linux container","text":"<p>I am a firm believer that Network Operating Systems should be available for testing to everybody. The reality, unfortunately, is quite different, with vendors either not allowing you to download virtual NOS at all, or requiring you to have an account, a registration with their system or a license file to run it.</p> <p>With SR Linux, we are making a big step into the openness by pushing SR Linux container to the public container registry so everyone can it pull without any registration, payments, or active service accounts. Absolutely free and open.</p> <pre><code>docker pull ghcr.io/nokia/srlinux\n</code></pre>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#running-light","title":"Running light","text":"<p>Containerized NOSes have a lot of benefits that come from the container packaging. One of them being lightweight compared to the VM-based counterparts.</p> <p>On average, a single SR Linux container will consume about 0.5vCPU and ~1GB RAM<sup>2</sup>. That allows you to spin up labs of decent size having only an entry-level VM at your disposal.</p> <p>For example, one of the most typical labs is a Clos fabric with a few leafs and spines. The lab like that will fit into 2vCPU and 6GB RAM VM.</p> <p>You can even run this lab on a free Github Actions runner, which has 8GB RAM. Imagine the sheer possibilities in writing CI pipelines for testing your DC features which can run in the public cloud for free.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#full-feature-parity","title":"Full feature parity","text":"<p>When working with virtual networking products one needs to be aware of any limitations the virtual appliance imposes. Quite often the virtual images we work with in labs are crippled both in dataplane and control plane functions.</p> <p>These limitations of the virtual images make it hard to create a reliable and \"real\" automated testing pipeline.</p> <p><p>When you say vNOS, do you mean as a separate standalone product or as a virtual version of an image that will be sold to run on hardware only?</p>\u2014 Joe Neville \ud83c\udf3b (@joeneville_) July 21, 2021 </p> <p>SR Linux container has the same code inside that actually runs on our hardware platforms. There is no control or data plane deviations<sup>1</sup>, so by using this image in your CI pipelines you can be sure that when deployed to production, it will behave the same.</p> <p>And all that with a small resource footprint. Imagine running a fully functional 3-stage mini-Clos fabric with 6 nodes on a machine with 2vCPU and 6GB RAM? That will fit into a free GitHub runner!</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#no-license-strings-attached","title":"No license strings attached","text":"<p>Being able to pull SR Linux NOS as any other container image is big on its own, but we also wanted to make sure that you can use right away. To do that, we made licensing optional, so once you pulled an image you can use it to its full extent!</p> <p>Info</p> <p>When running without a license users can enjoy all the features of SR Linux, but dataplane interfaces will have a 1000pps throughput limitation and running time is limited by 2 weeks.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#versioning","title":"Versioning","text":"<p>We plan to push every public release of SR Linux to the container registry so that you can pull a specific version or the <code>:latest</code> one. The SR Linux version is a tag on the container image, so there is an easy way to match a release to its image.</p> <p>We will keep all versions available for you to pull.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#github-container-registry","title":"GitHub container registry","text":"<p>The decision use GitHub container registry was made specifically to allow you to get the image without facing the pull restrictions that Docker has in place.</p> <p>We do that because we think that one of the very promising applications for public SR Linux container is to use it in CI pipelines. And in CI your jobs can pull the images quite frequently, so having limitations for pulling, is an important improvement.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#learn-sr-linux","title":"Learn SR Linux","text":"<p>SR Linux was designed to answer the needs of today and tomorrow, with a strong focus on automation teams. With that forward-looking design, it is clear that many things will look new to the users who worked with traditional Networks OSes in the past.</p> <p></p> <p>To help you navigate the SR Linux world, we are launching a community-oriented documentation portal - learn.srlinux.dev</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#learn-by-doing","title":"Learn by doing","text":"<p>The main goal of the portal is to introduce you to the SR Linux by means of the interactive tutorials. All the tutorials are based on a certain lab scenario that we go through explaining the technology and how SR Linux implements it.</p> <p>The lab scenarios are deployed with containerlab so both the tutorial authors and the readers follow along the same path and can reproduce the whole tutorial.</p> <p>We start with explaining how to actually run SR Linux container and build arbitrary topologies, and then offer you to follow one of the use-cases centric tutorials.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#not-your-usual-tutorials","title":"Not your usual tutorials","text":"<p>One of the objectives for the tutorials that we put on this portal was to make them stand out, and this is achieved with the following:</p> <ol> <li> <p>Backed by runnable labs     As have already been mentioned, every tutorial is actually created using a lab deployed with SR Linux containers. And we share those labs for you to follow along the configuration journey.     That is very important to us, because completing a hands-on tutorial beats reading experience every day.</p> </li> <li> <p>Complete, top-to-bottom explanations     Every tutorial is built from the ground-up, with every step explained and demonstrated. If there are any pre-requisites which needs to be met, we explain how to do that.     For example, if there is a routing underlay that we use to deploy overlay services on top, we always explain how this is configured and provide you with config snippets to achieve the same required state.</p> </li> <li> <p>Control &amp; Data planes verification     A large part of the tutorials are dedicated to control plane and data plane verifications. It is not enough to just configure a feature, we want to show you also how to verify that the applied configuration results in a proper control/data plane function.</p> </li> <li> <p>PCAPs     Yes, where applicable we will also share the PCAP files with control and data plane traffic captured. The truth is always in PCAPs, so by analyzing them we can see how control plane protocols operate and what encapsulations are used in the data plane.</p> </li> </ol>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#always-on-sr-linux","title":"Always-ON SR Linux","text":"<p>Although it is extremely easy to run SR Linux on your own system, it is always nice to have a system running on the Internet which you can access.</p> <p>Please welcome, an Always ON SR Linux instance.</p> <p>As the diagram shows, we are exposing every management interface the NOS has for you to explore them.</p> <ul> <li>The modern-looking CLI is accessible via SSH</li> <li>The fully potent gNMI interface is open for everyone to try out getting information with gNMI Get and stream it with gNMI Subscribe</li> <li>The third interface - JSON-RPC over HTTP - is a REST API like interface for teams who prefer to deliver automation via it, or those who find gNMI specification limiting.</li> </ul> <p>Be a good citizen</p> <p>Please, act in good faith and do not try to oversubscribe the instance by streaming massive amounts of data. This is a shared instance, and we want everyone to have a good experience with it.</p>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#pre-configured-services","title":"Pre-configured services","text":"<p>The Always-ON SR Linux instance has a few pre-configured services that you can explore either via CLI, or with other interfaces such as gNMI.</p> <p>The pre-configured services are:</p> <ol> <li>Layer 2 EVPN with VXLAN dataplane with <code>mac-vrf-100</code> network instance</li> <li>Layer 3 EVPN with VXLAN dataplane with <code>ip-vrf-200</code> network instance</li> </ol>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/nokia-sr-linux-goes-public/#sr-linux-community","title":"SR Linux community","text":"<p>SR Linux has lots to offer to various groups of engineers...</p> <p>Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS.</p> <p>Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI.</p> <p>Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support.</p> <p>We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community is the SR Linux Discord Server which everyone can join!</p> <ol> <li> <p>since dataplane for the SR Linux container is simulated, there are some edge cases where the real dataplane would behave differently, but this is only true for 5% of the overall cases.\u00a0\u21a9</p> </li> <li> <p>still, it is required to have at least 4GB RAM VM and preferably &gt;1 vCPU\u00a0\u21a9</p> </li> </ol>","tags":["srlinux","containerlab","nokia"]},{"location":"2021/using-scrapligo-with-kubectl-exec/","title":"Using scrapligo with kubectl exec","text":"<p>As the networking industry is (slowly) moving towards forklifting networking functions to the cloud-native space we often become the witnesses of mixing decade old tools with cloud-native approaches and architectures.</p> <p>This post is about one such crazy mixture of using screen scraping library scrapligo with <code>kubectl exec</code> and <code>docker exec</code> commands.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#what-and-why","title":"What and Why?","text":"<p>I can imagine that for some readers the previous sentence makes no sense, why do you need a screen scraping library when working with standalone containers or kubernetes workloads? Shouldn't it be all covered with various APIs already?</p> <p>It should, yes, but when networking workloads are being moved into the cloud it is more often than not results in a compromised architecture which is not fully aligned with the behavior of the containerized workloads.</p> <ul> <li> <p>For example, when deploying a network function on kubernetes you might realize that the container image doesn't use the IP address that container runtime provisions for its <code>eth0</code> interface.</p> </li> <li> <p>Or you might want to add some basic configuration to a Network OS running as a k8s pod without creating a service for its SSH/NETCONF/etc server.</p> </li> <li> <p>Or you need to generate self-signed certificates on the NOS side to enable programmable access via gNMI or HTTPS.</p> </li> </ul> <p>In all these cases you often resort to <code>kubectl/docker exec</code> commands to connect to a shell/CLI and do some CLI based configuration over that terminal interface. This makes <code>kubectl exec</code> pretty much a modern day Telnet.</p> <p>Since these operations over a terminal interface allocated by <code>exec</code> commands are almost inevitable, it makes a lot of sense to be able to automate interactions over it.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#scrapligo-and-kubectl-exec","title":"scrapligo and <code>kubectl exec</code>","text":"<p>Scrapligo, which is a Go version of the famous Scrapli library, is now able to handle CLI based interactions over the terminal interface offered by <code>docker exec</code> or <code>kubectl exec</code> (or any other command that exposes a PTY actually).</p> <p>This is especially cool in conjunction with <code>kubectl exec</code>, since you might have your networking workloads live in a remote cluster and without any k8s services you could get programmatic access to the shell/CLI of the Network OS to perform some bootstrap or validation.</p> <p>An approach like that can hugely simplify the operations of networking workloads in the remote clusters, and believe me, the tasks that you can't carry out otherwise still exist...</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#lab-deployment","title":"Lab deployment","text":"<p>To demonstrate this new scrapligo capability I will use the following user story.</p> <p>A user deployed a networking lab with Nokia SR Linux nodes on a remote k8s cluster. They intend to manage the nodes with gNMI interface, but in order to do that, a TLS certificate must be provisioned on a device which gNMI will use to secure the transport layer.</p> <p>Info</p> <p>I am using SR Linux containers here because they are available for pulling for everyone, but you can swap it with Arista cEOS or any other NOS easily.</p> <p>For various reasons, it is not possible to configure an Ingress service to enable external access for SR Linux workload, but cluster management is possible via <code>kubectl</code>. So we could configure TLS certificates over <code>kubectl exec</code>, and that is what we will do, but programmatically.</p> <p>To replicate this scenario we will deploy two docker containers on a host</p> <ul> <li>the container named <code>gnmi</code> that hosts the <code>gnmic</code> tool to test gNMI access</li> <li>and <code>srlinux</code> container that is our Network OS of choice.</li> </ul> <p>On the docker host side we will run a <code>scrapligo</code> program that will provision TLS certificates and gNMI service over <code>docker exec</code> command (step 1).</p> <p>Then container with <code>gnmic</code> inside will be able to use gNMI service on the srlinux container (step 2).</p> <p>Info</p> <p>For simplicity we use <code>docker exec</code> and plain containers, but the same will work with <code>kubectl exec</code> without any deviations.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#deploy-containers","title":"Deploy containers","text":"<p>First, start an srlinux container named <code>srlinux</code> in daemon mode, which is as easy as:</p> <pre><code>docker run -t -d --rm --privileged \\\n  -u $(id -u):$(id -g) \\\n  --name srlinux ghcr.io/nokia/srlinux \\\n  sudo bash /opt/srlinux/bin/sr_linux\n</code></pre> <p>Then add gnmi container:</p> <pre><code>docker run --rm -it --privileged \\\n  --name gnmic ghcr.io/hellt/network-multitool \\\n  bash\n</code></pre>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#check-gnmi","title":"Check gNMI","text":"<p>To ensure we are not cheating, let's verify that gNMI service is not configured on SR Linux, and we can't use it.</p> <p>Since SR Linux cli is a process inside the container, we can provide a CLI command to that process and get its output:</p> <pre><code>docker exec srlinux sr_cli \"info system gnmi-server\"\n    system {\n        gnmi-server {\n        }\n    }\n</code></pre> <p>The gNMI config is indeed empty, as well as the TLS server profiles:</p> <pre><code># provides empty output\ndocker exec srlinux sr_cli \"info system tls\"\n    system {\n        tls {\n        }\n    }\n</code></pre>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#code-walkthrough","title":"Code walkthrough","text":"<p>Now it is time to run the Go program that will leverage scrapligo's abilities to execute commands over a terminal offered by <code>docker exec</code>.</p> <p>On your host, clone hellt/scrapligo-pty-demo and explore its <code>main.go</code> file.</p> <pre><code>git clone https://github.com/hellt/scrapligo-pty-demo.git\n</code></pre> <p>The whole program is contained within the <code>main.go</code> file, let's cover the main pieces of this short program.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#network-driver-creation","title":"Network driver creation","text":"<p>We first start by creating an SR Linux driver, using srlinux-scrapli package providing scrapligo support for SR Linux.</p> <pre><code>func main() {\n contName := \"srlinux\"\n tlsProfileName := \"demo\"\n\n d, err := srlinux.NewSRLinuxDriver(\n  contName,\n  base.WithAuthStrictKey(false),\n  base.WithAuthBypass(true),\n )\n</code></pre> <p>We also set the container name our srlinux container has - <code>srlinux</code> - as well as set the name for the TLS profile we want to configure along the way.</p> <p>The important part is <code>base.WithAuthBypass(true)</code>, this option disables the authentication that would normally happen should you try to SSH into the device. With <code>exec</code> command the authentication is not needed, as we execute the process directly inside the container.</p> <p>Info</p> <p>Note, that no IP/DNS address is present, that is because we are not using SSH to access the node, instead we provide a container name that we will refer later within <code>docker exec</code> command.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#setting-open-command","title":"Setting Open command","text":"<p>When we initialized our Network Driver we implicitly said to scrapligo that we would like to use the <code>System</code> transport.</p> <p>The way <code>System</code> transport works is that it calls <code>ssh</code> program on your host and then works with the pseudo terminal the openssh offers.</p> <p>In <code>docker exec</code> case what we need to do is to actually overwrite the default command that is used to call the <code>ssh</code> program and tell scrapligo to use <code>docker exec</code> instead. This is how its done:</p> <pre><code>transport, _ := d.Transport.(*transport.System)\ntransport.ExecCmd = \"docker\"\ntransport.OpenCmd = []string{\"exec\", \"-u\", \"root\", \"-it\", contName, \"sr_cli\"}\n</code></pre> <p>Notice, that we simply state the CLI command that you would normally use in your terminal. That is exactly how <code>System</code> transport works, it calls the specified command an expects to be able to connect a pseudo terminal to the called process.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#configuring-certificates-and-gnmi","title":"Configuring certificates and gNMI","text":"<p>The rest of the code is not interesting at all, as all that happens next is just some tinkering with CLI commands to tell SR Linux to generate TLS certificates and use them for gNMI.</p> <pre><code>err = srlinux.AddSelfSignedServerTLSProfile(d, tlsProfileName, false)\nif err != nil {\n fmt.Println(err)\n os.Exit(1)\n}\n\nd.AcquirePriv(\"configuration\")\n\nfmt.Println(\"Enabling gNMI...\")\ngnmiCfg := []string{\n \"set / system gnmi-server admin-state enable\",\n \"set / system gnmi-server timeout 7200\",\n \"set / system gnmi-server rate-limit 60\",\n \"set / system gnmi-server session-limit 20\",\n \"set / system gnmi-server network-instance mgmt admin-state enable\",\n \"set / system gnmi-server network-instance mgmt use-authentication true\",\n \"set / system gnmi-server network-instance mgmt port 57400\",\n fmt.Sprintf(\"set / system gnmi-server network-instance mgmt tls-profile %s\", tlsProfileName),\n \"commit save\",\n}\n\n_, err = d.SendConfigs(gnmiCfg)\n</code></pre>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#running-the-program","title":"Running the program","text":"<p>Now all is ready to run that program that should result in configuring the TLS certs and gNMI service over <code>docker exec</code> on srlinux container.</p> <pre><code>\u276f go run main.go\nConfiguring TLS certificate...\nEnabling gNMI...\n</code></pre> <p>Let's see if TLS certs are now in place:</p> <pre><code>\u276f docker exec srlinux sr_cli \"info system tls\"\n    system {\n        tls {\n            server-profile demo {\n                key $aes$7PkycPYrUtfg=$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\n                certificate \"-----BEGIN CERTIFICATE-----\nMIIE+zCCAuOgAwIBAgIUPydqw851/QfI/nSER+MYsD457egwDQYJKoZIhvcNAQEL\nBQAwDTELMAkGA1UEBhMCVVMwHhcNMjEwODE3MTQ1MjQzWhcNMjIwODE3MTQ1MjQz\nWjANMQswCQYDVQQGEwJVUzCCAiIwDQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIB\nAM1wPwXpvoakWZFhtzrBVJ1qsO0qmsdEzZJBeZ0PQjL9xkM5JsS/rrybPXu5WNqa\ndWMLB3OtWLuaJ+uchdt0rF1os6ALsNlHl4cZyrU2b+0tZkF+E7q/nz0E0NSjoT9O\nU9Si10eWwRj1jI/DsUwJr+d9lDtjHGWG7SquF7b8423pDlhSrdtSTXb2Eh8g2Yrg\nfKwvoqDsMrnLN4dtYb192NpjKo6SJFJozzrweICVqdNKcx5BaUredxA5FpdIZOmQ\nEc5K9eEBxpxAmmXoW1U1GEAUiqf9diJ6HPw5sXltjj87Fxher8NnT5R0bj0HlctY\nKM+RWMxJzKZqs/FWGdpnti71v1bynT/O8LUiULk/6Ms1bSS1pe9sd3Co9HiZLnp1\nLWTGFAmtjpFYq3lXYql/rwx4v9xqSVqMLyjDtVRXDxjaVLFNyQZsR4U+nlJ6tq6v\nDIYj6h05MBAejXIo7YeEUkTZJDL7+cHgJU6YjPQKDw3p9keLJsPYA62AKikgnAfX\nPAYll7kZxNT/rGM5emj0Kkr3cgJY22bowqvo36/GcJrte+It/27IwE5Lb53PD5BT\nDiBo21BCYPFe8qX2dN10vDAm08naD59vA68FfmBXhM6voRGGcovK6itRuR6xSJWX\nTRI4kP+E+9sGiGX5rMlWX1PjATWx+ygqLIfBxLMtFyhbAgMBAAGjUzBRMB0GA1Ud\nDgQWBBTcUql90C5AYyryIncoCdOwvGPcxzAfBgNVHSMEGDAWgBTcUql90C5AYyry\nIncoCdOwvGPcxzAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEBCwUAA4ICAQBo\n4Lr2gJQBQukNwEr8fSZNNhvuveo69nOvQRbQbAaMbWtyZlL4LKIfOg5vh6aD7Lcr\nWbQK+4xNwvFoyKwa3DTqYUu2xO8DKPbP+Fnfv3WP4geYc3hexsjveE+64oAM/Te3\npzSW3bx1gLgS3LCRF8NT1ZBP+xwfs57tAXypoSbLr8y35cynFQkgd3Phwb6koUxg\nj5OrsTrf8hOMvigJ6dSiwGwYZ77kyDsoHdQTaoQQoxhdz8RZWbvCr1JuCMXfqCSg\nJ1biwqTF8mpJ0EGUSlz2D1RKnxjsKZ5EgZ3X0s2v23A4GiMqCJDEyAA+rFzCq/ov\nPmuNAXsioMdPk6C2VMNzyxq2GCSTweJnSMa+jpk/WkQok39jnx4kW7LoW555cT3+\nweuT1pK1Kg1z1S1ytkn7q+AjNfaYZgRtKQ/FWaww97LCXrEoZQw0VMiuhTxFajyJ\niv5a6771qvnDbWSkZFqL08PU/24m6OrpNOfXzp9KMYBi7O83flCQbuvnpdAlDOEA\nWyNOXHiIQ+jrfETSCWwD2ncKeucGlZqj3uZk1n+yEdMLuoFzOKfLg+OG5Lx+zU2D\nlZ5hmj9IiQ7NSDXbSnDTUJ56XbKx92kkVOdKAKhML8mgtLFJMK2fy5K3ahinoLOK\n8k4fIE+zVn7ld/LLf7MXdh1SvFLOF4/kxKUw9pRoEQ==\n-----END CERTIFICATE-----\"\n                authenticate-client false\n            }\n        }\n    }\n</code></pre> <p>And gNMI server has been configured to use the <code>tls-profile</code> that the program created:</p> <pre><code>\u276f docker exec srlinux sr_cli \"info system gnmi-server\"\n    system {\n        gnmi-server {\n            admin-state enable\n            timeout 7200\n            rate-limit 60\n            session-limit 20\n            network-instance mgmt {\n                admin-state enable\n                use-authentication true\n                port 57400\n                tls-profile demo # &lt;- references the created TLS profile which has the generated cert/key\n            }\n        }\n    }\n</code></pre> <p>All points out that we can now use gNMI service with a secured transport. To check that, let's go back to our gnmic container that we launched and try and get some data via gNMI.</p> <p>Info</p> <p>Before doing that, we need to see which IP address the srlinux has on its management interface. It can be queried with the following show command:</p> <pre><code>\u276f docker exec srlinux sr_cli \"show interface mgmt0.0\"\n===============================================================\n  mgmt0.0 is up\n    Network-instance: mgmt\n    Encapsulation   : null\n    Type            : None\n    IPv4 addr    : 172.17.0.2/16 (dhcp, preferred)\n    IPv6 addr    : fe80::42:acff:fe11:2/64 (link-layer, preferred)\n===============================================================\n</code></pre> <p>Now in the bash shell of the gnmic container, craft the <code>gnmic</code> command to get some data over gNMI:</p> <pre><code>gnmic -a 172.17.0.2 -u admin -p admin --skip-verify -e json_ietf \\\n      get --path /system/information/version\n</code></pre> <p>and that returns the version of the SR Linux NOS, yay!</p> <pre><code>[\n  {\n    \"timestamp\": 1629268834502587033,\n    \"time\": \"2021-08-18T06:40:34.502587033Z\",\n    \"updates\": [\n      {\n        \"Path\": \"srl_nokia-system:system/srl_nokia-system-info:information/version\",\n        \"values\": {\n          \"srl_nokia-system:system/srl_nokia-system-info:information/version\": \"v21.6.1-250-g433be28615\"\n        }\n      }\n    ]\n  }\n]\n</code></pre>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/using-scrapligo-with-kubectl-exec/#summary","title":"Summary","text":"<p><code>kubectl exec</code> is the modern day's Telnet, it allows to get terminal access to applications running in k8s cluster, without having network access exposed on the workloads themselves.</p> <p>Scrapligo (and later scrapli) allows us to programmatically use this terminal interfaces exposed by commands like <code>kubectl exec/docker exec/etc</code> to configure/interrogate the workloads over this OOB character based interface.</p> <p>The approach demonstrated above is especially useful in the current days when networking workloads may still require some bootstrapping steps in order to come up ready to be managed.</p>","tags":["scrapli","docker","kubernetes","go"]},{"location":"2021/network-automation-options-in-go-with-scrapligo/","title":"Network automation options in Go with scrapligo","text":"<p>Just recently the network automation folks witnessed a great library to be ported from Python to Go - scrapligo.</p> <p><p>Been working on learning go a bit and have published scrapligo https://t.co/NDXQ6khxCr -- still a work in progress, but has been a fun learning experience! Check it out and let me know what ya think! \ud83e\udd20</p>\u2014 Carl Montanari (@carlrmontanari) May 19, 2021 </p> <p>For me personally this was a pivotal point because with scrapligo the Go-minded netengs can now automate their networks with a solid and performant library.</p> <p>One of the things that scrapligo packs is, of course, the ability to reliably talk to the network devices using the same command line interface as a human would normally do. That means that scrapligo would send and receive the pieces of data that an operator would send/receive if they were connected with a terminal over SSH.</p> <p>As you may very well be aware, the typical output that a network device produces for a given command is unstructured, meaning that it is not presented in a way that can be effortlessly parsed by a machine.</p> <pre><code># output of a `show system information` command from Nokia SR OS\n\n===============================================================================\nSystem Information\n===============================================================================\nSystem Name            : sros\nSystem Type            : 7750 SR-1\nChassis Topology       : Standalone\nSystem Version         : B-20.10.R3\nCrypto Module Version  : SRCM 3.1\nSystem Contact         :\nSystem Location        :\nSystem Coordinates     :\nSystem Up Time         : 8 days, 00:24:27.04 (hr:min:sec)\n</code></pre> <p>If we were to send <code>show system information</code> command with scrapligo towards a Nokia SR OS device, we would have not be able to get, say, the device version right away, since the response is basically the unstructured blob of text as the program sees it.</p> <p>What can we do about it?</p>","tags":["scrapli","textfsm","netconf"]},{"location":"2021/network-automation-options-in-go-with-scrapligo/#use-netconfgnmiapi","title":"use NETCONF/gNMI/API","text":"<p>In an ideal world, you would have stopped reading this post, because ALL your devices were equipped with some kind of programmatic interface that returns structured data. Like in the example above we connect to the SR OS node with scrapligo netconf subsystem and retrieve back a result of a NETCONF Get operation.</p> <p>We then can run a query on this XML document we received and get the data out just nicely.</p> <pre><code>package main\n\nimport (\n \"fmt\"\n \"strings\"\n\n \"github.com/antchfx/xmlquery\"\n \"github.com/scrapli/scrapligo/driver/base\"\n \"github.com/scrapli/scrapligo/netconf\"\n \"github.com/scrapli/scrapligo/transport\"\n)\n\nfunc main() {\n d, _ := netconf.NewNetconfDriver(\n  \"clab-scrapli-sros\",\n  base.WithAuthStrictKey(false),\n  base.WithAuthUsername(\"admin\"),\n  base.WithAuthPassword(\"admin\"),\n  base.WithTransportType(transport.StandardTransportName),\n )\n\n d.Open()\n\n r, _ := d.Get(netconf.WithNetconfFilter(`\n &lt;state xmlns=\"urn:nokia.com:sros:ns:yang:sr:state\"&gt;\n &lt;system&gt;&lt;version&gt;&lt;version-number/&gt;&lt;/version&gt;&lt;/system&gt;\n &lt;/state&gt;`))\n\n doc, _ := xmlquery.Parse(strings.NewReader(r.Result))\n\n ver := xmlquery.Find(doc, \"//version-number\")\n\n fmt.Println(ver[0].InnerText())\n\n d.Close()\n}\n</code></pre> <p>Output:</p> <pre><code>\u276f go run netconf.go\nB-20.10.R3\n</code></pre> <p>Unfortunately we are not yet there, we have thousands of access devices in Service Providers network which do not have any of the fancy interface. We have Enterprise networks running decade old gear. And we also live in a harsh world where even if the Network OS has one of those fancy interface, the level of information you can query via them is not on par with what you can do over CLI.</p>","tags":["scrapli","textfsm","netconf"]},{"location":"2021/network-automation-options-in-go-with-scrapligo/#on-box-json-output","title":"on-box JSON output","text":"<p>The next best thing is to leverage the device's ability to present the output as JSON. Then you can capture this output over SSH and let your JSON parser to do it's thing.</p> <p>For example, on EOS every show command can be represented as a JSON blob:</p> <pre><code>ceos&gt;show inventory | json\n{\n    \"fpgas\": {},\n    \"storageDevices\": {},\n    \"xcvrSlots\": {},\n    \"subcompSerNums\": {},\n    \"portCount\": 3,\n    \"switchedBootstrapPortCount\": 2,\n    \"managementPortCount\": 1,\n    \"dataLinkPortCount\": 0,\n    \"emmcFlashDevices\": {},\n    \"cardSlots\": {},\n    \"internalPortCount\": 0,\n    \"powerSupplySlots\": {},\n    \"fanTraySlots\": {},\n    \"systemInformation\": {\n        \"name\": \"cEOSLab\",\n        \"description\": \"cEOSLab\",\n        \"mfgDate\": \"\",\n        \"hardwareRev\": \"\",\n        \"hwEpoch\": \"\",\n        \"serialNum\": \"\"\n    },\n    \"unconnectedPortCount\": 0,\n    \"switchedPortCount\": 0,\n    \"switchedFortyGOnlyPortCount\": 0\n}\n</code></pre> <p>and with this tiny <code>scrapligo</code> program you can easily retrieve all the data from this output:</p> <pre><code>package main\n\nimport (\n \"encoding/json\"\n \"fmt\"\n\n \"github.com/scrapli/scrapligo/driver/base\"\n \"github.com/scrapli/scrapligo/driver/core\"\n \"github.com/scrapli/scrapligo/transport\"\n)\n\nfunc main() {\n d, _ := core.NewCoreDriver(\n  \"clab-scrapli-ceos\",\n  \"arista_eos\",\n  base.WithAuthStrictKey(false),\n  base.WithAuthUsername(\"admin\"),\n  base.WithAuthPassword(\"admin\"),\n  base.WithTransportType(transport.StandardTransportName),\n )\n\n d.Open()\n // send show command and ask to output it as JSON\n r, _ := d.SendCommand(\"show inventory | json\")\n\n // imagine, that the structure that this output can be parsed into is unknown to us\n // thus we will use a map of empty interfaces to dynamically query data after\n var jOut map[string]interface{}\n json.Unmarshal(r.RawResult, &amp;jOut)\n\n fmt.Println(\"number of management ports:\", jOut[\"managementPortCount\"])\n\n d.Close()\n}\n</code></pre> <p>that produces the following output:</p> <pre><code>\u276f go run main_arista.go\nnumber of management ports: 1\n</code></pre> <p>That approach is a decent alternative to a missing programmatic interface and sometimes is the best option. But, as it usually happens, it is not universal. Many Network OS'es can not emit JSON for any given command, if at all. That means we need to resort to the parsing of the unstructured data ourselves.</p>","tags":["scrapli","textfsm","netconf"]},{"location":"2021/network-automation-options-in-go-with-scrapligo/#good-old-parsing","title":"good old parsing","text":"<p>And we back to square 1, where we usually get after some reality check. That is where we need to parse the unstructured output ourselves and get the blob of text we receive from a device to be transformed to some data structure which we can use in a program.</p> <p>Of course, we can simply use Regular Expressions or even brute characters matching a loop, but when dealing with lengthy outputs (usually a product of a <code>show</code> command), we often resort to a framework that can simplify the parsing.</p> <p>For quite a long time the TextFSM python library was the answer to that particular task and since then a huge amount of textfsm templates were written to parse all sorts of outputs from various devices.</p> <p>Being a Go person myself I was wondering if TextFSM exists in Go, since once we have scrapli in Go, having a parsing library in Go was the key piece missing.</p> <p>Fortunately, some bright mind already ported TextFSM to Go - go-textfsm - and Carl integrated it into scrapligo the same day I notified him that go-textfsm exists.</p> <p>Let's have a look how it is used within scrapligo. For that exercise we will take a <code>show system information</code> output from Nokia SR OS and use a textfsm template to create a structured data out of it.</p> <p>the original textFSM templates might need to be touched by you, since Go regexp differs from Python RE in some parts.</p> <p>To parse the output we will get from the Nokia SR OS device I will create a file with the textfsm template for this output under <code>sysinfo.textfsm</code> file name. Here is the template body:</p> <pre><code># System\nValue SysName (\\S+)\nValue SysType (.*)\nValue Version (\\S+)\nValue SysContact (.*)\nValue SysLocation (.*)\nValue SysCoordinates (.*)\nValue SysAtv (\\S+)\nValue SysUpTime (.*)\nValue ConfigurationModeCfg (.*)\nValue ConfigurationModeOper (.*)\n\n\nStart\n  ^System Information -&gt; ReadData\n\nReadData\n  ^System Name\\s*:\\s*${SysName}\n  ^System Type\\s*:\\s*${SysType}\n  ^System Version\\s*:\\s*${Version}\n  ^System Contacts+:\\s+${SysContact}\n  ^System Location\\s*:\\s*${SysLocation}\n  ^System Coordinates\\s*:\\s*${SysCoordinates}\n  ^System Active Slot\\s*:\\s*${SysAtv}\n  ^System Up Time\\s*:\\s*${SysUpTime}\n</code></pre> <p>Now when the template is there, we can write the following scrapligo + gotextfsm program:</p> <pre><code>package main\n\nimport (\n \"fmt\"\n\n \"github.com/scrapli/scrapligo/driver/base\"\n \"github.com/scrapli/scrapligo/driver/core\"\n \"github.com/scrapli/scrapligo/transport\"\n)\n\nfunc main() {\n d, _ := core.NewCoreDriver(\n  \"clab-scrapli-sros\",\n  \"arista_eos\",\n  base.WithAuthStrictKey(false),\n  base.WithAuthUsername(\"admin\"),\n  base.WithAuthPassword(\"admin\"),\n  base.WithTransportType(transport.StandardTransportName),\n )\n\n d.Open()\n\n r, _ := d.SendCommand(\"show system information\")\n\n parsedOut, _ := r.TextFsmParse(\"private/sysinfo.textfsm\")\n\n fmt.Printf(\"Version: %s\\nUptime: %s\",\n  parsedOut[0][\"Version\"], parsedOut[0][\"SysUpTime\"])\n\n d.Close()\n}\n</code></pre> <p>Output:</p> <pre><code>\u276f go run main_fsm.go\nVersion: B-20.10.R3\nUptime: 8 days, 03:07:43.25 (hr:min:sec)\n</code></pre> <p>Easy-peasy! All thanks to the textFSM integration that scrapligo recently added!</p>","tags":["scrapli","textfsm","netconf"]},{"location":"2021/network-automation-options-in-go-with-scrapligo/#ps","title":"PS","text":"<p>Regardless which network/vendor/consulancy firm you employed with, you won't be able to avoid CLI parsing activities at all times. The legacy gear is out there, with no other management interface but SSH/SNMP.</p> <p>Before scrapligo it was quite tedious (I'd claim not worth it even) to automate network activities over SSH. Now the module packs almost everything you need to efficiently get going and write some nice automation programs or CLI tools.</p>","tags":["scrapli","textfsm","netconf"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/","title":"How to patch Ubuntu 20.04 Focal Fossa with UKSM?","text":"<p>Running multiple VMs out of the same disk image is something we, network engineers, do quite often. A virtualized network usually consists of a few identical virtualized network elements that we interconnected with links making a topology.</p> <p></p> <p>In the example above we have 7 virtualized routers in total, although we used only two VM images to create this topology (virtualized Nokia router and it's Juniper vMX counterpart). Each of this VMs require some memory to run, for the simplicity, lets say each VM requires 5GB of RAM.</p> <p>So roughly, the above topology will claim 30-35GB of RAM in order to operate. Enriching the topology by adding more VMs of the same type will continue to push for more memory, thus running big topologies often becomes an exercise of hunting for RAM.</p> <p>Luckily, there are technologies like Kernel Same Merging (KSM) and it's enhanced version Ultra-KSM (UKSM) that are able to lift the memory requirement for use cases like above. In a nutshell, they allow to merge mem pages of the same content, effectively reusing the same memory pages between virtual machines.</p> <p>from UKSM usenix paper</p> <p>Memory deduplication can reduce memory footprint by eliminating redundant pages. This is particularly true when similar OSes/applications/data are used across different VMs. Essentially, memory deduplication detects those redundant pages, and merges them by enabling transparent page sharing.</p> <p>Although UKSM is not a silver bullet for every application and use case, it tends to be a very good fit for hypervisors used to run virtualized networking topologies. For that reason the EVE-NG network emulation platform embeds UKSM in their product.</p> <p>So I decided to bring UKSM to my Ubuntu 20.04 VM that I use to launch virtualized routers and containers to witness the benefits/issues of having it.</p> <p></p> <p>The results look promising. Running 6 VMs with a system memory footprint of one is a solid memory optimization, especially considering that performance penalty is something we can bare in a lab where we mostly play with control plane features.</p> <p></p> <p>Now if you want to bring UKSM to your hypervisor you will need to jump through some hoops, as UKSM is a kernel feature that is not available as a module. This means that you need to build a kernel with UKSM enabled, and that might be a barrier too high for some of you. It was for me, until I spent a night trying multiple things until it worked, so let me share with you the process and the outcomes so that you can rip the benefits without having all the trouble of trial-and-error routine.</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#0-tldr","title":"0 TL;DR","text":"<ul> <li> <p>Download UKSM patches</p> </li> <li> <p>Download kernel source</p> </li> <li>Apply UKSM patch</li> <li>Build kernel</li> <li>Install kernel</li> </ul>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#1-get-uksm-patches","title":"1 Get UKSM patches","text":"<p>As mentioned above, UKSM is a kernel feature and the way it is distributed nowadays is via <code>patch</code> files that are available in this Github repo. So our first step is cloning this repo to get the patches for recent (4.x and 5.x) kernels. Easy start.</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#2-get-the-kernel-source-code","title":"2 Get the kernel source code","text":"<p>As the UKSM patches need to be applied to a kernel source code, we need to get one. Here things can get a tad complicated.</p> <p>There are many different kernels out there:</p> <ul> <li>vanilla Linux kernels blessed by Linus himself</li> <li>distribution kernels (Debian, Ubuntu, Fedora, etc)</li> <li>third party kernels with the best hacks</li> </ul> <p>The UKSM patches were created against the vanilla Linux kernel, but my Ubuntu VM runs a kernel that was produced by Ubuntu team.</p> <pre><code># on Ubuntu 20.04\nuname -a\nLinux kernel-build 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux\n</code></pre> <p>Vanilla linux kernel uses X.Y.Z versioning. If anything is appended after X.Y.Z (like <code>-48-generic</code>) in my case, it indicates that the kernel comes from a distributor (Ubuntu in my case).</p> <p>Things that didn't work: 1 At first I tried to download the original Linux kernel, but the build process failed without giving me a good explanation. 2 Download latest 5.4 kernel from Ubuntu - UKSM patch didn't apply, as the code has changed apparently</p> <p>After multiple rinse-repeat iterations I found out that I can take the Ubuntu kernel <code>5.4.0-48.52</code> as UKSM patch applies to it no problem and the build succeeds.</p> <p>How did I get one? Oh, that is also something worth documenting, as the path to knowing it is paved with broken links and articles dated early 2000s. First, go here and check what tags/branches are available for Focal release of Ubuntu. Once the tag/branch name is found, pull this one only, to save on data transfer:</p> <pre><code># fetch single branch/tag only\ngit clone --depth 1 --single-branch --branch Ubuntu-5.4.0-48.52 https://git.launchpad.net/\\~ubuntu-kernel/ubuntu/+source/linux/+git/focal\n</code></pre> <p>Fast forward 180MB of kernel source code and you have it in <code>focal</code> directory. Next is patching.</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#3-patch-the-source","title":"3 Patch the source","text":"<p>To embed the UKSM code into the kernel code we need to use the patch utility.</p> <p>In the UKSM repo we cloned in step 1 we have patch files per kernel MAJOR.MINOR version. As we downloaded the Ubuntu kernel 5.4.0-something, let's try and apply the patch from <code>uksm-5.4.patch</code> patch file.</p> <pre><code># assuming we cloned UKSM repo in ~\ncd focal\npatch -p1 &lt; ~/uksm/v5.x/uksm-5.4.patch\n</code></pre> <p>patch command must not return any failures. If it does, do not proceed!</p> <p>This is the most important step, the patch must apply cleanly, meaning that if you see any <code>FAILURE</code> strings in its output (or <code>echo $?</code> doesn't return <code>0</code>) it means the patch is not compatible with the kernel.</p> <p>The tricky part was to find the <code>Ubuntu kernel+patch file</code> combination that didn't result in an error. For me the merry pair was <code>Ubuntu-5.4.0-48.52 + uksm-5.4.patch</code>.</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#4-build-the-patched-kernel","title":"4 Build the patched kernel","text":"<p>Once the patch is cleanly applied we build the kernel.</p> <p>Here I feel obliged to say that it was my first kernel build, so the explanations are surely not technically correct, but it works, so why not sharing my view on it.</p> <p>To build the kernel we fist need to create the build configuration file. As we use the kernel that we actually run (5.4.0-48) we can reuse the existing kernel configuration:</p> <pre><code># being in focal directory\nmake oldconfig\n</code></pre> <p>This command will run the config generation script and it prompted me that there is a UKSM config option added (as a result of a UKSM patch) and if I want to use it instead of the default KSM option. I typed <code>1</code> in the prompt confirming that I need UKSM to be an acting KSM feature. That is the only input that was needed.</p> <p>After the config is made, start the build process:</p> <pre><code># j8 is the number of cores I had on my machine\nmake -j8 deb-pkg LOCALVERSION=-uksm\n</code></pre> <p>40 minutes later I had four debian packages created:</p> <pre><code>~/focal #Ubuntu-5.4.0-48.52 !15 ?16                                                              root@devbox-u20 08:36:17\n\u276f ls -la ../*deb\n-rw-r--r-- 1 root root  11441428 Feb 17 20:50 ../linux-headers-5.4.60-uksm_5.4.60-uksm-1_amd64.deb\n-rw-r--r-- 1 root root 910558572 Feb 17 20:58 ../linux-image-5.4.60-uksm-dbg_5.4.60-uksm-1_amd64.deb\n-rw-r--r-- 1 root root  61261664 Feb 17 20:50 ../linux-image-5.4.60-uksm_5.4.60-uksm-1_amd64.deb\n-rw-r--r-- 1 root root   1071476 Feb 17 20:50 ../linux-libc-dev_5.4.60-uksm-1_amd64.deb\n</code></pre>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#5-install-the-kernel","title":"5 Install the kernel","text":"<p>Out of these four files I needed to install all but <code>*dbg*</code> files:</p> <pre><code>sudo dpkg -i ../linux-headers-5.4.60-uksm_5.4.60-uksm-1_amd64.deb\nsudo dpkg -i ../linux-image-5.4.60-uksm_5.4.60-uksm-1_amd64.deb\nsudo dpkg -i ../linux-libc-dev_5.4.60-uksm-1_amd64.deb\n</code></pre> <p>Once this is done, update your grub config to have the new kernel load by default:</p> <pre><code>sudo update-grub\n</code></pre> <p>And <code>reboot</code>. Done!</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#6-verify-uksm-is-working","title":"6 Verify UKSM is working","text":"<p>After the reboot, ensure that your new kernel is running by examining <code>uname -r</code> output. It should match the new version.</p> <p>Launch some VMs, and check the memory consumption as well as the number of sharing pages with <code>cat /sys/kernel/mm/uksm/pages_sharing</code>.</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#7-get-the-built-kernel","title":"7 Get the built kernel","text":"<p>If you don't want to build a kernel yourself (and one night lost I can see why), I packaged the deb files into a bare container which you can pull and copy the files from to install the kernel on your Ubuntu machine:</p> <pre><code># pull the container and copy the deb files out of it\ndocker pull ghcr.io/hellt/ubuntu-5.4.60-uksm:0.1\nid=$(docker create ghcr.io/hellt/ubuntu-5.4.60-uksm:0.1 foo)\ndocker cp $id:/uksm-kernel .\n</code></pre> <p>All you need to do is to start with step 5 and you should be all good. Thanks for tuning in!</p>","tags":["uksm","ubuntu"]},{"location":"2021/how-to-patch-ubuntu-2004-focal-fossa-with-uksm/#ps-ksm-vs-uksm","title":"PS. KSM vs UKSM","text":"<p>There is a KSM kernel feature that allows you to achieve some memory sharing via a similar mechanisms. It can be that KSM will deliver a similar performance on your setup, and being included in your kernel by default it might be worth checking out.</p> <p>The following resources will help you start with KSM:</p> <ul> <li>https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_tuning_and_optimization_guide/chap-ksm#sect-KSM-The_KSM_tuning_service</li> <li>https://gist.github.com/mapuo/17e3b253222172c1659782eb14150c3a</li> <li>https://www.linux-kvm.org/page/KSM#Enabling_KSM</li> <li>https://openterprise.it/2019/03/enable-ksm-kernel-same-page-merging-on-fedora/</li> <li>https://www.kernel.org/doc/Documentation/vm/ksm.txt</li> </ul>","tags":["uksm","ubuntu"]},{"location":"2022/gnmic-joins-openconfig-/","title":"gNMIc joins Openconfig \ud83d\ude80","text":"<p>Two years ago, a dozen contributors less, 400 Pull Requests, and 2000 commits behind, another pet project appeared on a vast GitHub landscape. It was a learning exercise by Karim Radhouani to sharpen his skills in gNMI - a niche network management protocol promoted by the Openconfig group.</p> <p>Initially named <code>gnmi_client</code>, it had a noble but narrow scope of providing a feature-rich, complete, yet intuitive CLI for gNMI-enabled routers. Fast forward two years, and we have the gNMIc software suite that is much more than just a CLI for gNMI.</p> <p>Today, Nokia donates the gNMIc project to Openconfig, and with that move, we expect to see gNMIc adopted by even more companies and organizations </p> <ul> <li> <p> gNMIc new address</p> <p> openconfig/gnmic</p> <p> https://gnmic.openconfig.net</p> </li> </ul> <p>In this post I'd like to give you a brief overview of gNMIc's core features and share my thoughts on what we expect to happen with gNMIc moving under the Openconfig's wing.</p> <p>Over the past two years, gNMIc became a feature-rich gNMI-focused software suite. Both its CLI and collection capabilities matured with lots of new integrations added. Moreover, gNMIc had quite some air time in production networks, not to mention lab deployments and dev testbeds.</p> <p>I would like to briefly highlight gNMIc's current feature set for those who aren't familiar with it yet. Broadly, we split gNMIc capabilities into the following three domains:</p> <ul> <li>CLI</li> <li>Collector</li> <li>API</li> </ul> <p>Note</p> <p>Each of those domain areas packs a hefty number of neat features, and I will only highlight some of them to keep the introduction short and sweet.</p> <p>gNMIc cherishes documentation; readers are encouraged to follow the links provided in this post to get more information on particular topics.</p> <p> </p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#cli","title":"CLI","text":"<p>gNMIc provides an intuitive yet full-featured CLI for interacting with gNMI-capable targets. It fully implements gNMI v0.7+ specification with extensions<sup>1</sup>, and with the move under Openconfig, it becomes a standard gNMI CLI tool.</p> <p>In 2020 I wrote about gNMIc highlighting its CLI capabilities. Since then CLI side of gNMIc has only become better with the following changes:</p> <ul> <li>Added template-based payloads to simplify complex and data-driven configuration use cases.</li> <li>GetSet command introduced to allow conditional execution of a Set RPC based on a received Get response.</li> <li>Implemented <code>diff</code> command to compare configurations between two different targets and identify configuration drift.</li> <li>Added <code>prompt</code> mode for a guided CLI experience.</li> <li>Added generation of paths out of the YANG modules.</li> <li>Support configuration via any of the following methods: CLI flags, environment variables, or file.</li> <li>Integrated prototext and protojson output options to display raw requests and responses.</li> </ul> <p>gNMIc CLI</p> <p>Easy one-click installation, multi-arch/multi-OS hermetic binary, full feature parity with gNMI spec and intuitive commands layout make <code>gnmic</code> tool a perfect choice for the task.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#collector","title":"Collector","text":"<p>Having a great CLI was just the beginning; the lion's share of changes happened in the collector area of gNMIc.</p> <p> gNMIc as a collector in a typical open-source streaming telemetry deployment</p> <p>With the growing interest in Streaming Telemetry, we saw an opportunity to create an open-source telemetry collector to meet the demand. Not just a collector, but the open-source collector that can survive a production deployment with all its requirements. I'd like to believe that gNMIc succeeded in delivering on that promise.</p> <p>Given the central piece that collection capabilities take in gNMIc, it makes sense to spend some additional time on collector's core components.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#clustering","title":"Clustering","text":"<p>Streaming Telemetry is often perceived as glorified monitoring that can tolerate outages. Not really, no.</p> <p>Modern telemetry systems are essential for observability, diagnostics and starting to play a vital role in network automation; Because of that importance, an outage is undesired and should be avoided. For that reason, a Streaming Telemetry collector needs to support High Availability and resiliency.</p> <p></p> <p>gNMIc comes with automatic cluster formation routines that enable high availability, scaling, and target redistribution.</p> <p>A cluster of gNMIc instances distributes the load by locking the targets to certain instances of the cluster. When the cluster is healthy and operational, targets are assigned to specific gNMIc nodes; in the case of a gNMIc node failure, its targets are going to be moved to a healthy gNMIc node.</p> <p> A view on Consul service key/value store with targets distributed across gNMIc instances</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#target-loaders","title":"Target loaders","text":"<p>Adding gNMI targets to the config file works as long as the number of targets stays reasonable and constant. More often than not, in production deployments, the number of targets is quite large. And what is even more critical, targets are being added/removed over time. Consequently, it is desirable for a controller to be able to discover and load targets automatically.</p> <p>gNMIc supports the dynamic loading of gNMI targets from external systems and services. This feature allows adding and deleting gNMI targets without the need to restart gnmic.</p> <p></p> <p>Targets can be loaded from the following sources:</p> <ul> <li>file: watch for the changes done to a local file containing gNMI targets definitions.</li> <li>consul: read targets registered in the Consul service registry.</li> <li>docker: retrieve available targets using Docker API.</li> <li>http: read target definition from an HTTP server.</li> </ul>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#processors","title":"Processors","text":"<p>No collection service can live without a data processing pipeline. Data that the users typically collect from the network elements often requires some processing before it can be stored in a database.</p> <p>Let's have a look at a few examples where processing is mandatory:</p> <ul> <li>Units normalization/conversion: in a multivendor network it may be needed to normalize units for, say, utilization rate, from various vendors to a common unit. MB, KB to Bytes, or various time formats to a common epoch time or datetime format.</li> <li>Filtering metrics: when dealing with a wildcard-based subscription, a collector may receive more data than needed. To optimize for space in the database, processors can be used to allow/drop the needed metrics and give you control over what is going to be written to it.</li> <li>Data conversion: depending on the telemetry encoding or vendor's implementation, a collector might receive data in a string format, while it needs to be an integer. A conversion processor can automatically convert such metrics so that users can run operations on them in the database.</li> <li>Tag extraction: For certain metrics collected via gNMI, users may need to extract specific values and promote them to metric's tags to enable a nice layout in the database.</li> </ul> <p>gNMIc employs a large set of processors that can help you transform and shape your data the way you need it. Processors can form processing pipelines and be associated with any configured output to enable flexible data pipelines.</p> Processors Add Tag Allow Convert Data Convert Date String Delete Drop Duration Convert Extract Tags Group By Jq Merge Override TS Strings To Tag Trigger Value Tag Write <p>Processors become invaluable when Streaming Telemetry leaves the lab's sandbox and gets applied in production where dragons lie.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#outputs","title":"Outputs","text":"<p>A Streaming Telemetry typically doesn't store collected data; instead, it pushes the collected and processed data to an output, such as a Time Series Database (TSDB), a message bus, or a file.</p> <p>Various supported outputs make a streaming telemetry collector versatile because it can be deployed in different data pipelines. gNMIc supports a solid number of outputs categorized as databases, message queues, and files.</p> <p></p> <p>Most popular and used outputs are already supported by gNMIc:</p> <ul> <li>Time Series Databases<ul> <li>InfluxDB</li> <li>Prometheus (pull and remote write models)</li> <li>and others working with Prometheus or Influx wire protocols.</li> </ul> </li> <li>Message queues<ul> <li>NATS</li> <li>STAN</li> <li>Kafka</li> </ul> </li> <li>Raw outputs<ul> <li>TCP, UDP (for example in conjunction with ElasticSearch database)</li> </ul> </li> </ul> <p>With a powerful concept of multiple outputs, users can write their metrics to different data stores and create advanced data pipelines.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#inputs","title":"Inputs","text":"<p>Complex telemetry pipelines might be built using gNMIc's concept of inputs. With inputs, gNMIc is able to receive gNMI data not from an end-device such as a router but from another gNMIc instance.</p> <p>This powerful technique enables users to build a distributed cluster of gNMIc collectors that export the data to a single collector upstream.</p> <p></p> <p>Or create a so-called data-reuse pipeline where multiple outputs receive the same telemetry data.</p> <p></p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#tunnel-server-gnmi-dial-out","title":"Tunnel server (gNMI dial-out)","text":"<p>Dial-out Streaming Telemetry has been a custom thing for quite some time. gNMI specification only specifies the dial-in model where a collector initiates the session towards the gNMI-enabled targets, and not the other way around. But being able to initiate a connection from the router towards a collector is sometimes desirable or even mandatory.</p> <p>To accommodate for that deployment scenario, vendors implemented custom gRPC services that catered to this use case. And recently, the Openconfig group proposed a standard approach to enable dial-out gNMI telemetry using  openconfig/grpc-tunnel project.</p> <p></p> <p>gNMIc is the first open-source collector that implements grpc-tunnel specification and thus can support deployment scenarios where dial-out is needed using a proposed standard approach.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#deployment-examples","title":"Deployment examples","text":"<p>All those features make gNMIc quite versatile and powerful, but at the same time, it might be overwhelming for newcomers. With that thought in mind, gNMIc packs many deployment examples that should help users get going quickly and smooth.</p> <p> A deployment topology from one of the examples</p> <p>gNMIc deployment examples provide users with a complete use case explanation. Moreover, every example comes with a ready-made virtual testbed<sup>2</sup> so that you can try the scenario for yourself. The examples typically include a gNMIc instance with its configuration, the rest of the Telemetry stack (TSDB of choice plus Grafana), and a virtual network to extract the data.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#what-else","title":"What else?","text":"<p>Many other things and improvements were made to gNMIc, making it even more powerful.</p> <ul> <li>gNMI Server that makes gNMIc act as a gNMI target itself to build hierarchical collector deployments.</li> <li>Actions allow gNMIc to invoke reactions based on the received telemetry data and, to some extent, help users build reactive systems.</li> <li>REST API to automate target provisioning and lifecycle.</li> </ul>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#why-not-telegraf","title":"Why Not Telegraf?","text":"<p>This question may very well still be on your mind when you reach this chapter. And now, when we walked over the collector's features, it is evident that most of those features are simply not available in Telegraf. Clustering, high availability, target discovery, hierarchical deployments are all unique to gNMIc.</p> <p></p> <p>Telegraf is an excellent product, don't get me wrong, but when it comes to gNMI it is a swiss knife vs. a surgical scalpel.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#go-api","title":"Go API","text":"<p>And finally, gNMIc provides a human-friendly Go API for gNMI. In contrast with the auto-generated gNMI API supplied by the <code>github.com/openconfig/gnmi</code> package, the API exposed by the gNMIc in the <code>github.com/openconfig/gnmic/api</code> package has abstractions in place that make interactions with gNMI targets less cumbersome and more intuitive.</p> Create gNMI targetCreate Get RequestRunning request <pre><code>router, err := api.NewTarget(\n    api.Name(\"router1\"),\n    api.Address(\"10.0.0.1:57400\"),\n    api.Username(\"admin\"),\n    api.Password(\"S3cret!\"),\n    api.SkipVerify(true),\n)\n</code></pre> <pre><code>getRequest, err := api.NewGetRequest(\n    api.Encoding(\"json_ietf\"),\n    api.DataType(\"config\"),\n    api.Path(\"interfaces/interface\"),\n    api.Path(\"network-instances/network-instance\"),\n)\n</code></pre> <pre><code>getResponse, err := router.Get(ctx, getRequest)\n</code></pre> <p>With a friendly API, we expect to see an uptake in gNMI as a network management protocol being used programmatically. Go get it!</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#move-to-openconfig","title":"Move to Openconfig","text":"<p>Now to the meat of it. Nokia donates gNMIc to the Openconfig group. But why moving?</p> <p>I believe there are several interdependent areas of improvement worth indicating.</p> <ol> <li>The move will help gNMIc to gain more visibility across the expanding field of Streaming Telemetry users.</li> <li>With the popularity gain, we might discover new use cases, new integration opportunities and get more feedback.</li> <li>Close collaboration with Openconfig/Google might bring new contributors to the project and help with sustainability. It is an open-source project, and it will stay open.</li> <li>Being under the wing of Openconfig should help users be less concerned about the project's health should they consider using it in production.</li> </ol> <p>There is a lot of wishful thinking, and we don't know if everything we wish to accomplish will materialize, but we would like to give it a go.</p> <p>Karim, as the sole developer, will still be at the helm of gNMIc development, but we expect more contributors appear in the future.</p>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/gnmic-joins-openconfig-/#disclaimer","title":"Disclaimer","text":"<ol> <li> <p>I contributed to gNMIc during the project's early days, but 99% of the effort came from Karim Radhouani<sup>3</sup>. The credit goes to him for making gNMIc as we know it today. I would also like to thank our contributors who helped shape and form gNMIc with their valuable comments, feedback, and contributions.</p> </li> <li> <p>The thoughts and statements I made in this post belong to me and do not necessarily match Nokia's.</p> </li> </ol> <ol> <li> <p>such as History \u21a9</p> </li> <li> <p>powered by containerlab or docker-compose.\u00a0\u21a9</p> </li> <li> <p>you can find him at linkedin and github.\u00a0\u21a9</p> </li> </ol>","tags":["gnmic","gnmi","openconfig"]},{"location":"2022/using-guestfish-container-image/","title":"Using guestfish container image","text":"<p>Once in a while, one still needs to get down to a VM-land and dust off some guestfish skills.</p> <p>Like today I got the IPInfusion OcNOS <code>qcow2</code> image whose devs decided it is best to use VNC console by default. VNC console for a text-based terminal...</p> <p>So along come guestfish commands.</p> <p>It is hugely satisfying to modify the VM images using containers, so here are my two commands to modify GRUB settings.</p> <p>I first check the initial grub content, then swap it with a modified one (with a serial console, right?). Clean, fast, \ud83e\uddd1\u200d\ud83c\udf73</p> <pre><code># show image's file contents\n\nDISK_IMG=/tmp/ocnos.qcow2\nDISK_DIR=$(dirname ${DISK_IMG})\nDISK_NAME=$(basename ${DISK_IMG})\n\ndocker run -i --rm \\\n  -v ${DISK_DIR}:/work/${DISK_DIR} \\\n  -w /work/${DISK_DIR} \\\n  bkahlert/libguestfs \\\n  guestfish \\\n  --ro \\\n  --add ${DISK_NAME} \\\n  --mount /dev/sda1:/ \\\n  cat /etc/default/grub\n</code></pre> <pre><code># copy-in a file\n\nLOCAL_FPATH=/tmp/ocnos-newgrub\nREMOTE_FPATH=/etc/default/grub\n\ndocker run -i --rm \\\n  -v ${DISK_DIR}:/work/${DISK_DIR} \\\n  -v ${LOCAL_FPATH}:/work${LOCAL_FPATH} \\\n  -w /work/${DISK_DIR} \\\n  bkahlert/libguestfs \\\n  guestfish \\\n  --rw \\\n  --add ${DISK_NAME} \\\n  --mount /dev/sda1:/ \\\n  upload /work${LOCAL_FPATH} ${REMOTE_FPATH}\n</code></pre>","tags":["guestfish"]},{"location":"2022/decoding-gnmi-with-wireshark/","title":"Decoding gNMI with Wireshark","text":"<p>Okay, here goes my first attempt fitting the shoes of a content creator.</p> <p>Please welcome the NetRel episode 001 - Decoding gNMI with Wireshark, it is a 35min journey of using Wireshark to parse the gNMI traffic (both non-secured and secured).</p> <p> <p></p> <p>I won't spend your time explaining the first episode; instead, let me tell you what I want the NetRel series to be about. I am interested in covering the aspects of network automation that are not widely covered.</p> <p>Luckily, my main line of work often offers me chances to work with the technologies relevant for webscalers. Hence, I will focus on the advanced content and not waste effort on the beginners' videos. A few broad topics that I wish to cover along the way:</p> <ul> <li>Go for network automation</li> <li>YANG based automation and programmable interfaces</li> <li>k8s as a network control plane</li> <li>gNxI interfaces</li> </ul> <p>Quite convinced a few more topics will stem in time, but these are the current candidates.</p> <p>Now a few words on why YT? Why now?</p> <p>1.5 years ago I was willing to die on the hill of the text-based blogs. I do still love blogs and think they offer the best signal/noise ratio for knowledge transfer. But over time, I noticed how video blogs become dominant. I realized that I land on YT more often than stumble upon a text blog. So, as a viewer, I see how video material is getting ahead in terms of audience coverage and views.</p> <p>The video genre also makes it easier to add interactivity to your content. Gifs &amp; screenshots can't compete with a hands-on demo you can record with a voice-over. This was the second plus on the blogs-v-videos scales that inclined me to try out going solo on YT.</p> <p>As far as content creation goes, in my case, the time required to create a video is &gt; time needed to craft a blog post. But I can see how this can improve over time. The more you get used to the editing process and grow your library of intros/callouts/etc, the easier it gets. What helped me overstep the barrier of spending too much time in editing is making a contract with myself. If I stutter, misspel a word or make a grammar mistake I roll with it. Minimum cuts, maximum time to work on the content itself and not its form.</p>","tags":["netrel","gnmi","gnmic"]},{"location":"2022/diy-yang-browser/","title":"DIY YANG Browser","text":"<p>Here comes the second episode of the NetRel show: NetRel episode 002 - DIY YANG Browser. Be ready to dive into the paths we took to create a YANG Browser for Nokia SR Linux platform.</p> <p> <p></p> <p>YANG data models are the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse, search through, and process these data models.</p> <p>To answer these demands, we created a web portal - https://yang.srlinux.dev - it offers:</p> <ul> <li>Fast Path Browser to effectively search through thousands of available YANG paths</li> <li>Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux</li> <li>Source <code>.yang</code> files neatly stored in nokia/srlinux-yang-models repository for programmatic access and code generation</li> </ul>","tags":["netrel","yang"]},{"location":"2022/sr-linux-json-rpc-and-ansible/","title":"SR Linux, JSON-RPC and Ansible","text":"<p>This week I have authored two tutorials for our https://learn.srlinux.dev portal.</p> <ul> <li>JSON-RPC Basics</li> <li>Using Ansible with SR Linux's JSON-RPC Interface</li> </ul> <p>The prime reason for this deep dive was to help our customers marry Ansible automation platform with network operations on SR Linux NOS. While I am personally not in the camp \"Ansible for network automation\" users, the reality is that it is still used by many small/mid teams who by now have large collections of playbooks and trained engineers.</p>","tags":["srlinux","json-rpc","ansible"]},{"location":"2023/test-coverage-for-go-integration-tests/","title":"Test coverage for Go integration tests","text":"<p>I have been working on containerlab for a while now; a project that once started as a simple idea of a tool that would create and wire up SR Linux containers grew into a full-blown network emulation tool loved by the community and used in labs by many.</p> <p>As it became evident that many more users started to rely on containerlab for their daily work, the looming feeling of responsibility for the quality of the tool started to creep in. At the same time, the growing user base exposed us to many more feature requests and integrations, making it harder to find time to address technical debt and improve testing.</p> <p>Given the nature of the project, it was clear that integration tests offer a quick way to validate the functionality, as we could replicate the user's workflow and verify the outcome. However, the integration tests are not without their own challenges, and one of them is the test coverage which is not as easy to get as with unit tests.</p> <p>In this post, I will share how coverage enhancements introduced in Go 1.20 helped us to get the coverage for our integration tests and jump from a miserable 20% to a (less sad) 50%.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#why-not-unit-tests","title":"Why not unit tests?","text":"<p>You might wonder if it would be easier, more correct, faster, and more proper to write unit tests and get the coverage out of the box. Well, yes, but also no.</p> <p>Like in the meme \"draw an owl\" writing unit tests for a <code>sum(a,b int) int</code> is easy, but crafting tests for interactions with container runtimes is not. Mocking, interface stubs generation, state management and other things that come with unit tests are not always easy to implement and maintain. Especially if you're not a pro Go developer with years of experience.</p> <p>Consequently, when I looked at what it'd take to write proper unit tests for containerlab, I decided to go with integration tests instead. The integration tests are easier to write, and they are more representative of the user's workflow. Yes, they run way slower, but that's a trade-off I was willing to make.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#containerlab-integration-tests","title":"Containerlab integration tests","text":"<p>People are debating which integration test runners are better. Some popular options are bats, ginkgo, Robot Framework, and many others. I've been working with Ginkgo and Robot, but given that I had more experience with Robot, I decided to go with it.</p> <p>Tip</p> <p>Robot Framework is a Python-based test automation framework that allows you to write tests in a human-readable format (business-driven testing). Tests are written in a keyword-driven style which makes it easy to create readable tests.</p> <p>An example of a Robot test that deploys a test topology and verifies that the nodes are up and running, and links are opertional<sup>1</sup>:</p> <pre><code>*** Test Cases ***\nDeploy ${lab-name} lab\n    ${rc}    ${output} =    Run And Return Rc And Output\n    ...    sudo -E ${CLAB_BIN} --runtime ${runtime} deploy -t ${CURDIR}/${lab-file}\n\n    Should Be Equal As Integers    ${rc}    0\n\nVerify links in node l1\n    ${rc}    ${output} =    Run And Return Rc And Output\n    ...    ${runtime-cli-exec-cmd} clab-${lab-name}-l1 ip link show eth1\n\n    Should Be Equal As Integers    ${rc}    0\n    Should Contain    ${output}    state UP\n\n    ${rc}    ${output} =    Run And Return Rc And Output\n    ...    ${runtime-cli-exec-cmd} clab-${lab-name}-l1 ip link show eth2\n\n    Should Be Equal As Integers    ${rc}    0\n    Should Contain    ${output}    state UP\n</code></pre> <p>For the sake of this post, I will not go into the details of how RF test suites are built; it is sufficient to say that the tests are started by invoking a shell script that runs the <code>robot</code> command that starts the test execution.</p> snippet from rf-run.sh script<pre><code>GOCOVERDIR=${COV_DIR} robot --consolecolors on -r none \\\n  --variable CLAB_BIN:${CLAB_BIN} --variable runtime:$1 \\ \n  -l ./tests/out/$(basename $2)-$1-log \\\n  --output ./tests/out/$(basename $2)-$1-out.xml $2\n</code></pre> <p>The integration tests use the <code>containerlab</code> binary that is built using the tip of the branch that is being tested. Unit tests and integration tests run in parallel allowing to identify error in both test suites. The following diagram shows the relationship between the unit and integration tests:</p> <pre><code>stateDiagram-v2\n    build : Build containerlab binary\n    unit : Run unit tests\n    integration1 : Run integration tests suite #1\n    integration2 : Run integration tests suite #2\n    integrationN : Run integration tests suite #N\n\n    build --&gt; integration1\n    build --&gt; integration2\n    build --&gt; integrationN</code></pre> <p>In reality, this translates into the following GitHub actions pipeline:</p> containerlab CI pipeline <p>These integration tests themselves run in parallel and provide good coverage of the containerlab functionality. However, it was hard to quantify the coverage of the tests because test coverage from the integration tests is not reported by default. Thankfully, starting with Go 1.20 it is now possible to build a binary that will report the test coverage when running it, but the whole process has some quirks, so let's dive into it.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#test-coverage-for-go-binaries","title":"Test coverage for Go binaries","text":"<p>When I first read the Go 1.20 release notes, I got excited about the new test coverage feature. It seemed like a perfect fit for containerlab integration tests; we could finally bump our test coverage by simply running the tests with a new binary. However, it turned out that it was not just a matter of building a binary with a certain flag; there is more to it.</p> <p>To my surprise, Dustin was quick enough to write a blog post about the new feature and how to use it with the unit test coverage. Here I will mainly refer to his blog post and provide references to containerlab code to put things into the context of a real project.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#building-a-binary-with-test-coverage","title":"Building a binary with test coverage","text":"<p>As I referred to above, the new feature that allows to get test coverage assumes that users build a binary with the test coverage enabled. This is done by passing the <code>-cover</code> flag to the <code>go build</code> command.</p> <p>In containerlab's case this is done in the Makefile where we build a binary used in tests only; it has race detection and debugging extenstions enabled, so we just added <code>-cover</code> flag to the mix.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#running-tests-with-coverage","title":"Running tests with coverage","text":"<p>The new binary is ready, now we can run the same integration tests as before, expecting to get the coverage report. Besides the binary, we have to set the <code>GOCOVERDIR</code> env var that will tell the Go runtime where to store the coverage report. In our case we set the env var in the test runner script. Without having this env var set the coverage report won't be generated at all.</p> <p>The new binary format of the test coverage report is not human-readable, so one would have to convert as Dustin shows. But we don't do this in our CI, because our goal is to merge the coverage from all the integration tests and unit tests into a single report, and this requires some tinkering.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#merging-coverage-reports","title":"Merging coverage reports","text":"<p>The test coverage report that you typically get when executing <code>go test</code> with coverage profile enabled is a single text file that coverage reporting tools like Codecov and Coveralls parse. Easy, but this coverage profile format is incompatible with the coverage data generated after you run the binary with coverage support.</p> <p>So what do we do? We run the <code>go test</code> and ask it to produce coverage data that is in the same binary format so that later we could merge the two.</p> <p>In the CI we use the <code>test</code> make target which runs <code>go test</code> and leverages a dirty workaround of setting the <code>goverdir</code> arg to output the coverage dir so that we get the coverage data in the binary format we need.</p> <pre><code>test:\n    rm -rf $$PWD/tests/coverage\n    mkdir -p $$PWD/tests/coverage\n    CGO_ENABLED=1 go test -cover -race ./... -v -covermode atomic -args -test.gocoverdir=\"$$PWD/tests/coverage\"\n</code></pre> <p>The coverage data appears in the <code>./tests/coverage</code> dir that we set as the <code>GOCOVERDIR</code> env var when running the integration tests. As a result, we will get the coverage data from both unit and integration tests in the same format. Now we need to merge them and produce a coverage profile.</p> <p>And here again the batteries are included in the <code>go</code> tool. All we have to do is:</p> <pre><code>go tool covdata textfmt -i=./tests/coverage -o coverage.out\n</code></pre> <p>And voila, we get the <code>coverage.out</code> in the text format that we can pass to Codecov or Coveralls.</p>","tags":["containerlab","go","testing"]},{"location":"2023/test-coverage-for-go-integration-tests/#codecov-integration","title":"Codecov integration","text":"<p>The last piece of the puzzle is how to do all these steps in a CI environment where different test jobs run separately. In our case, we have a separate job for unit tests and each integration test suite. Since we need to get coverage reports from each of them, we have to use GitHub Actions artifacts to pass the coverage data between the jobs.</p> <p>To achieve this goal, each coverage-producing job uploads the coverage data to the GitHub artifacts, and the last job downloads all the coverage data and merges it into a single report. Here is how it looks like in the CI pipeline:</p> Coverage pipeline <p>The <code>coverage</code> job is a simple sequence of steps that downloads the coverage data from the artifacts, merges it, and uploads the merged coverage report to the artifacts again. The <code>coverage</code> job is triggered only when all the test coverage-producing jobs are finished.</p> <pre><code>  coverage:\n    runs-on: ubuntu-22.04\n    needs:\n      - unit-test\n      - smoke-tests\n      - ext-container-tests\n      - ceos-basic-tests\n      - srlinux-basic-tests\n      - ixiac-one-basic-tests\n    steps:\n      - name: Checkout\n        uses: actions/checkout@v3\n        with:\n          fetch-depth: 0\n      - uses: WillAbides/setup-go-faster@v1.10.1\n        with:\n          go-version: ${{ env.GOVER }}\n      - uses: actions/download-artifact@v3\n        with:\n          name: coverage\n          path: tests/coverage\n      - name: convert Go's binary coverage to text coverage\n        run: make convert-coverage\n      - name: Upload coverage to codecov\n        uses: codecov/codecov-action@v3\n</code></pre> <p>And that's it; now we can enjoy the coverage report that combines the coverage from unit and integration tests.</p> <ol> <li> <p>Snippet from the full test.\u00a0\u21a9</p> </li> </ol>","tags":["containerlab","go","testing"]},{"location":"2023/gnmic-talks-at-dknog-and-nanog/","title":"gNMIc talks at DKNOG and NANOG","text":"<p>If you have never heard of gNMI and/or gNMIc project, you can start with my last week's talk DKNOG</p> <p> <p></p> <p>Thirty minutes introduction to gNMI and gNMIc to get you started.</p> <p>After this taster, you will likely want to know more, and Karim Radhouani has you covered. A 1-hour gNMIc tutorial has recently been published directly from NANOG 87 stage.</p> <p> <p></p> <p>After this one you'll never wanna see OIDs again.</p>","tags":["gnmi","gnmic","dknog","nanog"]},{"location":"2023/refreshing-go-package-index-for-your-package/","title":"Refreshing Go package index for your package","text":"<p>It is quite frustrating to wait for pkg.go.dev to refresh your index, and I always forget how give it a slight push:</p> <pre><code>GOPROXY=proxy.golang.org go list -m example.com/mymodule@v0.1.0\n</code></pre> <p>The new version won't appear immediately, but at least it seems it will be quicker to show up.</p>","tags":["go"]},{"location":"2023/sr-linux-logging-with-elk/","title":"SR Linux logging with ELK","text":"<p>Implementing centralized logging using modern log collectors is an interesting task even before you start solving scaling problems.</p> <p>My colleague and I opened up a series of posts dedicated to logging in the context of datacenter networks. We started with the basics of SR Linux logging and used the famous ELK stack as our log storage/processing solution.</p> <p>Integrating SR Linux logging with ELK via Syslog was fun, and we tried to capture every step of the way. Plus, we create a containerlab-based lab that anyone can use to test the solution themselves.</p> <p>Dig into \"SR Linux logging with ELK\" and open up the world of modern logging.</p>","tags":["srlinux","logging","elk"]},{"location":"2023/creating-a-syntax-highlighter-for-sr-linux-cli-snippets/","title":"Creating a syntax highlighter for SR Linux CLI snippets","text":"<p>How to write a custom syntax highligher for your favorite Network OS CLI and integrate it the doc engine?</p> Raw text CLI snippetWith <code>srl</code> syntax applied <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n            route 2001:1::192:168:18:0/64 {\n                admin-state enable\n                metric 1\n                preference 6\n                next-hop-group static-ipv6-grp\n            }\n        }\n</code></pre> <pre><code>--{ * candidate shared default }--[ network-instance black ]--\nA:leaf1# info static-routes\n        static-routes {\n            route 192.168.18.0/24 {\n                admin-state enable\n                metric 1\n                preference 5\n                next-hop-group static-ipv4-grp\n            }\n            route 2001:1::192:168:18:0/64 {\n                admin-state enable\n                metric 1\n                preference 6\n                next-hop-group static-ipv6-grp\n            }\n        }\n</code></pre> <p>Read in my post \"SR Linux Syntax Highlighting with Pygments\" at learn.srlinux.dev portal.</p>","tags":["srlinux","pygments"]},{"location":"2024/adding-border-to-the-logos/","title":"Adding border to the logos","text":"<p>It is super easy nowadays to generate a decent logo for your OSS project using any of genAI tool (dall-e, bing, etc). But one of the side-effects of the image generation might be the noise around the edges.</p> <p>If that's your case you can apply a border (offset) to your logo to mask the potential rough edges. I can recommend borderize service that makes it a matter of a few clicks.</p>","tags":["blogging"]},{"location":"2024/openstack-client-container-image/","title":"OpenStack Client Container Image","text":"<p>I like the portability, managability and package manager agnostic nature of container images. Especially for the tools I use couple of times a month. And even more so for Python tools that don't have native wheels for all their dependencies. Like OpenStack Client.</p> <p>So I built a small multi-stage Dockerfile to build a container image with OpenStack Client and all its dependencies. It's based on the official Python image and has a slim footprint:</p> <pre><code>FROM python:3.10-alpine as builder\n\nARG VERSION=6.4.0\nARG OCTAVIA_VERSION=3.6.0\n\nRUN apk add gcc python3-dev libc-dev linux-headers &amp;&amp; \\\n    pip install wheel \\\n    python-openstackclient==${VERSION} \\\n    python-octaviaclient==${OCTAVIA_VERSION}\n\n\nRUN pip freeze &gt; requirements.txt &amp;&amp; pip wheel -r requirements.txt -w /wheels\n\n# Final image\nFROM python:3.10-alpine\n\nCOPY --from=builder /wheels /wheels\n\nRUN pip install --no-index --find-links=/wheels python-openstackclient python-octaviaclient\n\nCMD [\"openstack\"]\n</code></pre> <p>You can pull the image from ghcr:</p> <pre><code>docker pull ghcr.io/hellt/openstack-client:6.4.0\n</code></pre> <p>To use this image you first need to source the env vars from your openrc file:</p> <pre><code>source myopenrc.sh\n</code></pre> <p>Then I prefer to install the alias <code>openstack</code> to my shell so that it feels like I have the client installed locally:</p> <pre><code>alias openstack=\"docker run --rm -it \\\n    -e OS_AUTH_URL=${OS_AUTH_URL} -e OS_PROJECT_ID=${OS_PROJECT_ID} \\\n    -e OS_USER_DOMAIN_NAME=${OS_USER_DOMAIN_NAME} \\\n    -e OS_PROJECT_NAME=${OS_PROJECT_NAME} \\\n    -e OS_USERNAME=${OS_USERNAME} -e OS_PASSWORD=${OS_PASSWORD} \\\n    ghcr.io/hellt/openstack-client:6.4.0 openstack $@\"\n</code></pre> <p>Then you can use the client as usual:</p> <pre><code>\u276f openstack server list\n+-----------------------------+----------------+--------+-----------------------------+------------------------------+---------------------+\n| ID                          | Name           | Status | Networks                    | Image                        | Flavor              |\n+-----------------------------+----------------+--------+-----------------------------+------------------------------+---------------------+\n| 0fa75185-0f76-482f-8cc3-    | k8s-w3-411e6d7 | ACTIVE | k8s-net-304e6df=10.10.0.11  | nesc-baseimages-             | ea.008-0024         |\n| 38e4d60212c8                |                |        |                             | debian-11-latest             |                     |\n-- snip --\n</code></pre>","tags":["openstack","docker"]},{"location":"2024/formatting-bash/","title":"Formatting bash","text":"<p>Whenever I need to format <code>bash</code> scripts I use the mvdan's shfmt - https://github.com/mvdan/sh/blob/master/cmd/shfmt/shfmt.1.scd as a docker container:</p> <pre><code>sudo docker run --rm -u \"$(id -u):$(id -g)\" -v $(pwd):/mnt -w /mnt mvdan/shfmt:v3 -w utils/if-wait.sh\n</code></pre>","tags":["bash"]},{"location":"2024/upscaling-images/","title":"Upscaling images","text":"<p>You know how in TV shows with IMDB rating ranging from 5 to 7 cops use that magic image recognition software that transforms the cctv image of a bad guy from the \"mashed potato\" quality to the Vogue cover? Yeah...</p> <p>But recently I needed to upscale the AI generated logo I made for Clabernetes project, and the quality was \"meh\", since it was a free service from Bing and whatnot. So whilst it was printable, the DPI was not good enough. So I decided to try my luck and google for these AI-powered image upscale services; frankly, my hopes were quite low.</p> <p>The first hit with a fishy DNS name - https://www.upscale.media/ - did not reinforce my beliefs that this is all a gimmick. But I tried, and it was legit good. It upscaled my logo while removed the noise and blurriness from the original image.</p> <p>Here is the comparison:</p> <p></p> <p>So yeah, something I wanted to save here because I will likely use it next time as well.</p>","tags":["blogging"]},{"location":"2015/ldp-ordered-label-distribution-control-explained/","title":"LDP. Ordered Label Distribution Control explained","text":"<p>Major network vendors (except Cisco) default to the following modes of Label Distribution Protocol (LDP) operation (as per RFC 5036 LDP Specification):</p> <ul> <li>Label Distribution (Advertisement): Downstream Unsolicited (section 2.6.3)</li> <li>Label Control: Ordered (section 2.6.1)</li> <li>Label Retention: Liberal (section 2.6.2)</li> </ul> <p>This topic focuses on Ordered Label Distribution Control procedure to help you better understand when LSR actually assigns labels and initiates transmission of a label mapping.</p> <p>Both RFC 3031 Multiprotocol Label Switching Architecture and RFC 5036 LDP Specification give definition for_Ordered Label Distribution Control_ mode:</p> <p> RFC 3031: In Ordered LSP Control, an LSR only binds a label to a particular FEC if it is the egress LSR for that FEC, or if it has already received a label binding for that FEC from its next hop for that FEC. </p> <p> RFC 5036: When using LSP Ordered Control, an LSR may initiate the transmission of a label mapping only for a FEC for which it has a label mapping for the FEC next hop, or for which the LSR is the egress. For each FEC for which the LSR is not the egress and no mapping exists, the LSR MUST wait until a label from a downstream LSR is received before mapping the FEC and passing corresponding labels to upstream LSRs. </p> <p>Lets break this definition into distinct sentences:</p> <ol> <li>In case LSR is the egress router for a FEC X then LSR maps label to FEC X and transmits this label mapping to its LDP peers;</li> <li>In case LSR is not the egress router for a FEC X then in order to map a label for this FEC and to propagate this label mapping to its peers, it has to wait until it receives a label mapping for this particular FEC X from its downstream LDP peer.</li> </ol> <p>In this manner the entire LSP is established before MPLS begins to map data onto the LSP, preventing early data mapping from occurring on the first LSR in the path.</p> <p>Let's take a look at this simple topology to illustrate these steps.</p> <p></p> <p>Routers R1-R2-R3 have OSPF enabled on their interfaces and announce their loopback (or system, as long as we\u2019re using with Alcatel-Lucent routers in this example) IP addresses. So every router has a route to other router\u2019s loopback address.</p> <pre><code>A:R1# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.1.2.0/24                                   Local   Local     00h08m36s  0\n       toR2                                                         0\n10.2.3.0/24                                   Remote  OSPF      00h04m20s  10\n       10.1.2.2                                                     200\n10.10.10.1/32                                 Local   Local     00h08m36s  0\n       system                                                       0\n10.10.10.2/32                                 Remote  OSPF      00h08m05s  10\n       10.1.2.2                                                     100\n10.10.10.3/32                                 Remote  OSPF      00h01m22s  10\n       10.1.2.2                                                     200\n-------------------------------------------------------------------------------\nNo. of Routes: 5\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n</code></pre> <p>To investigate label mappings creation and propagation processes lets dive into step-by-step LDP operation for the particular Forward Equivalence Class (FEC) <code>10.10.10.1/32</code> \u2013 which is the loopback address of the router R1.</p> <p></p> <p> </p> <p>Everything starts with R1 which is an Egress router for the FEC 10.10.10.1/32. According to abovementioned Ordered Distribution Control mode definition R1 (again, as an Egress router) has a right to create a binding for the FEC 10.10.10.1/32. Such bindings, created for the FECs which are local to a router often called \u201cLocal bindings\u201d.</p> <p>Let\u2019s check that R1 actually has this local label mapping in its Label Information Base:</p> <pre><code>A:R1# show router ldp bindings\n\n===============================================================================\nLDP Bindings (IPv4 LSR ID 10.10.10.1:0)\n             (IPv6 LSR ID ::[0])\n===============================================================================\nLDP IPv4 Prefix Bindings\n===============================================================================\nPrefix                                      IngLbl                    EgrLbl\nPeer                                        EgrIntf/LspId\nEgrNextHop\n-------------------------------------------------------------------------------\n10.10.10.1/32                               131071U                     --\n10.10.10.2:0                                  --\n  --\n&lt;-- omitted --&gt;\n</code></pre> <p>Yep, there it is \u2013 R1 tells us that he assigned a local label 131071 for the FEC <code>10.10.10.1/32</code>. Local bindings appear in the \u201cIngLbl\u201d column. Ingress label 131071 means that R1 expects its LDP peers to send their MPLS packets destined to the FEC 10.10.10.1/32 equipped with the label 131071 on top of MPLS label stack.</p> <p>Another benefit for being an Egress router for the FEC 10.10.10.1/32 is that R1 could send his locally created binding to its peers right away. Therefore, R1 sends this mapping to its LDP peer 10.10.10.2:0 (R2).</p> <p></p> <p> </p> <p>When R2 receives this label mapping message from R1 regarding the FEC 10.10.10.1, it follows this logic: I have received the label mapping for the FEC 10.10.10.1 sent from R1. Thanks to Liberal Retention mode I place this label binding into my LIB under \u201cEgrLbl\u201d column without further investigation. Such label mapping is often called remote label mapping since it came from the remote LDP peer.</p> <pre><code>*A:R2# show router ldp bindings prefixes prefix 10.10.10.1/32\n\n===============================================================================\nLDP Bindings (IPv4 LSR ID 10.10.10.2:0)\n             (IPv6 LSR ID ::[0])\n===============================================================================\nLegend: U - Label In Use,  N - Label Not In Use, W - Label Withdrawn\n        WP - Label Withdraw Pending, BU - Alternate For Fast Re-Route\n===============================================================================\nLDP IPv4 Prefix Bindings\n===============================================================================\nPrefix                                      IngLbl                    EgrLbl\nPeer                                        EgrIntf/LspId\nEgrNextHop\n-------------------------------------------------------------------------------\n10.10.10.1/32                                 --                      131071\n10.10.10.1:0                                1/1/1\n10.1.2.1\n\n&lt; omitted &gt;\n</code></pre> <p>But in order to be able to make its own label mapping for the 10.10.10.1/32 Ordered Label Distribution Control procedure demands R2 to check if remote binding was received from a downstream router for this particular FEC. Yes, it is, R1 is downstream router regarding data flow destined to the FEC 10.10.10.1/32.</p> <p>Good, R2 recognized R1 as a downstream router and therefore allowed to make its own label mapping (recall ordered control mode definition) for this FEC and propagate its mapping update to R3.</p> <p>R2 did not send this update to R1, because R1 is the owner of 10.10.10.1/32 and will never be a transit label switching router for this FEC.</p> <p></p> <p> </p> <p>Lets see R2\u2019s LIB:</p> <pre><code>*A:R2# show router ldp bindings prefixes prefix 10.10.10.1/32\n\n===============================================================================\nLDP Bindings (IPv4 LSR ID 10.10.10.2:0)\n             (IPv6 LSR ID ::[0])\n===============================================================================\nLegend: U - Label In Use,  N - Label Not In Use, W - Label Withdrawn\n        WP - Label Withdraw Pending, BU - Alternate For Fast Re-Route\n===============================================================================\nLDP IPv4 Prefix Bindings\n===============================================================================\nPrefix                                      IngLbl                    EgrLbl\nPeer                                        EgrIntf/LspId\nEgrNextHop\n-------------------------------------------------------------------------------\n10.10.10.1/32                                 --                      131071\n10.10.10.1:0                                1/1/1\n10.1.2.1\n\n10.10.10.1/32                               131070U                   131070\n10.10.10.3:0                                  --\n  --\n\n-------------------------------------------------------------------------------\nNo. of IPv4 Prefix Bindings: 2\n===============================================================================\n</code></pre> <p>Take a look at the highlighted strings, R2 chose a label value of 131070 for the FEC and sent this binding to LDP peer with identifier 10.10.10.3:0 (which is R3). We will cover the reason behind existence of the label 131070 in \u201cEgrLbl\u201d for peer 10.10.10.3:0 a bit later.</p> <p>Ok, R2 sent his mapping for 10.10.10.1/32 to R3, what happens next?</p> <p></p> <p>R3 follows the same logic as R2 did when it receives label mapping message from R2. In the same manner R3 installs remote label binding from R2, checks that R2 positioned downstream regarding data flow to address 10.10.10.1/32 and assigns local label binding for this FEC. But one thing is different with R3 \u2013 since R2 is not an owner for the FEC 10.10.10.1 then R3 can send label mapping message backward to R2 \u2013 see timestamp E.</p> <p> </p> <p></p> <p>Lets take a look at R3\u2019s LIB:</p> <pre><code>A:R3# show router ldp bindings prefixes prefix 10.10.10.1/32\n\n===============================================================================\nLDP Bindings (IPv4 LSR ID 10.10.10.3:0)\n             (IPv6 LSR ID ::[0])\n===============================================================================\nLegend: U - Label In Use,  N - Label Not In Use, W - Label Withdrawn\n        WP - Label Withdraw Pending, BU - Alternate For Fast Re-Route\n===============================================================================\nLDP IPv4 Prefix Bindings\n===============================================================================\nPrefix                                      IngLbl                    EgrLbl\nPeer                                        EgrIntf/LspId\nEgrNextHop\n-------------------------------------------------------------------------------\n10.10.10.1/32                               131070N                   131070\n10.10.10.2:0                                1/1/2\n10.2.3.2\n\n-------------------------------------------------------------------------------\nNo. of IPv4 Prefix Bindings: 1\n===============================================================================\n</code></pre> <p>When R2 receives this label mapping message from R3 regarding FEC 10.10.10.1, it follows the same logic: I have received the label mapping for the FEC 10.10.10.1. I will place this label binding into my LIB no matter what. This explains the egress label value 131070 in R2\u2019s LIB (see Listing #4) from peer 10.10.10.3:0.</p> <p>Although R3 installed a label, it will not take any actions regarding this update since it came from R3, which is not a downstream router for the FEC 10.10.10.1/32.</p> <p>This completes label propagation for the FEC 10.10.10.1/32.</p>","tags":["ldp","mpls"]},{"location":"2015/ldp-ordered-label-distribution-control-explained/#downstream-or-not","title":"Downstream or not?","text":"<p>You may have already noticed that the key in making a decision to create a label mapping for a FEC is if the label mapping came from a downstream router? But how does a router decide if its peer is downstream router or upstream? Let\u2019s think about it\u2026</p> <p>Rewind to Timestamp B. R2 receives a label mapping message from R1 and needs to decide if R1 is a downstream router regarding FEC 10.10.10.1/32? It looks into Forwarding Information Base for this prefix and sees the next-hop address for it is 10.1.2.1:</p> <pre><code>*A:R2# show router fib 1 10.10.10.1/32\n\n===============================================================================\nFIB Display\n===============================================================================\nPrefix [Flags]                                              Protocol\n    NextHop\n-------------------------------------------------------------------------------\n10.10.10.1/32                                               OSPF\n    10.1.2.1 (toR1)\n-------------------------------------------------------------------------------\nTotal Entries : 1\n-------------------------------------------------------------------------------\n</code></pre> <p>So what? We have a next-hop, but how do we know if IP address 10.1.2.1 belongs to R1? To asnswer this question we need to open RFC 5036 LDP Specification once again. Navigate to section 2.7  LDP Identifiers and Next Hop Addresses:</p> <p>   To enable LSRs to map between a peer LDP Identifier and the peer\u2019s addresses, LSRs advertise their addresses using LDP Address and Withdraw Address messages. </p> <p>Well, it seems LDP peers share LDP Address messages where they communicate all of their configured IP addresses, let\u2019s see under the hood:</p> <p></p> <p>That is the answer. LDP speaker should tell its peers about the addresses it has configured. This info communicated via Address Messages and helps remote peers to map LDP indentifier to an IP address.</p> <p>With this information provided R2 now can tell for sure that next-hop address it has in its Forwarding Information Base belongs to R1. And that is how R2 can tell that R1 is a downstream router \u2013 by matching its next-hop address from FIB with an IP addresses provided in an Address Message from R1.</p> <p>And this is all for this time. If you have any questions regarding this topic \u2013 do not hesitate, I will gladly address them.</p>","tags":["ldp","mpls"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/","title":"OSPF. Neighbors on a \"point-to-broadcast\" network","text":"<p>When it comes to basic OSPF troubleshooting the first-aid kit is Neighbor states and things, that should match to form an adjacency. And on one early morning while refreshing my memory on OSPF neighbor states I accidentally ran into quite interesting problem.</p> <p>But before we start, answer the short question:</p> <p>   Will adjacency be formed between directly connected via Gig. Ethernet interfaces routers R1 and R2 if </p> <li>   R1\u2019s OSPF interface type configured as point-to-point </li> <li>   R2\u2019s OSPF interface type configured as broadcast </li> <p>    Time\u2019s up. The answer is \u2013 yes and no. Wanna know why? Jump in, I have to show you something. </p> <p>Take a look at this topology which consists of two directly connected Cisco routers.</p> <p></p> <p>At first, topology seems as simple as a first figure in a CCNA book. But take a closer look at this OSPF interfaces, their types are different. Hm, definitely not a case you would find covered in an everyday OSPF configuration guide.</p> <p>I will not go into details about what are all the differences between various OSPF interface types, you already know it, or can read about 1-click away. Though I should remind you the key difference between point-to-point and broadcast interfaces behavior which is the necessity to elect DR/BDR routers for the latter.</p> <p>Also it is worth mention that it is best-practice nowadays to set \u201cbroadcast-by-nature\u201d Ethernet interfaces to operate in a point-to-point fashion. Why would one do it? To reduce convergence time. Ethernet interface once configured as point-to-point won\u2019t go into DR/BDR election which effectively means that neighbor relationships will reach FULL state 40 seconds faster (40s is the default wait interval for interface to elect DR/BDR routers_)_.</p> <p>Therefore this particular case with different interface types isn\u2019t some artificial Lab exercise, you can easily meet this interface mix in the real world. Let\u2019s say you simply forgot to configure another interface with p2p type, or your neighbor is under administration of the 3<sup>rd</sup> party who knows nothing about best practices and leave you with default broadcast behavior.</p> <p>Ok, different link types mean different networks with different behavior, rules and so forth, there is every indication that adjacency should not form. But it is not that simple. Let\u2019s Lab!</p>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#lab-time","title":"Lab time","text":"<p>I booted up two Cisco routers running IOS 15.2(4)S simultaneously and first thing checked R1\u2019s OSPF neighbors, expecting to see zero active neighbors:</p> <pre><code>R1#show ip ospf neighbor\n\nNeighbor ID     Pri   State           Dead Time   Address         Interface\n2.2.2.2           0   FULL/  -        00:00:36    10.1.2.2        GigabitEthernet1/0\n</code></pre> <p>Look at this, R1 has a neighbor in FULL state, which means that it has recognized R2 as a valid neighbor and successfully accomplished exchange of Link State Updates! As a precaution lets check OSPF interfaces parameters on both routers to confirm that they indeed operate in different types:</p> <p>On R1:</p> <pre><code>R1# show ip ospf interface gi1/0\nGigabitEthernet1/0 is up, line protocol is up\n  Internet Address 10.1.2.1/24, Area 0, Attached via Interface Enable\n  Process ID 1, Router ID 1.1.1.1, Network Type POINT_TO_POINT, Cost: 1\n  Topology-MTID    Cost    Disabled    Shutdown      Topology Name\n        0           1         no          no            Base\n  Enabled by interface config, including secondary ip addresses\n  Transmit Delay is 1 sec, State POINT_TO_POINT\n  Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5\n    oob-resync timeout 40\n    Hello due in 00:00:00\n  Supports Link-local Signaling (LLS)\n  Cisco NSF helper support enabled\n  IETF NSF helper support enabled\n  Index 1/1, flood queue length 0\n  Next 0x0(0)/0x0(0)\n  Last flood scan length is 1, maximum is 1\n  Last flood scan time is 0 msec, maximum is 0 msec\n  Neighbor Count is 1, Adjacent neighbor count is 1\n    Adjacent with neighbor 2.2.2.2\n  Suppress hello for 0 neighbor(s)\n</code></pre> <p>On R2:</p> <pre><code>R2#show ip ospf interface gi1/0\nGigabitEthernet1/0 is up, line protocol is up\n  Internet Address 10.1.2.2/24, Area 0, Attached via Interface Enable\n  Process ID 1, Router ID 2.2.2.2, Network Type BROADCAST, Cost: 1\n  Topology-MTID    Cost    Disabled    Shutdown      Topology Name\n        0           1         no          no            Base\n  Enabled by interface config, including secondary ip addresses\n  Transmit Delay is 1 sec, State DR, Priority 1\n  Designated Router (ID) 2.2.2.2, Interface address 10.1.2.2\n  Backup Designated router (ID) 1.1.1.1, Interface address 10.1.2.1\n  Timer intervals configured, Hello 10, Dead 40, Wait 40, Retransmit 5\n    oob-resync timeout 40\n    Hello due in 00:00:09\n  Supports Link-local Signaling (LLS)\n  Cisco NSF helper support enabled\n  IETF NSF helper support enabled\n  Index 1/1, flood queue length 0\n  Next 0x0(0)/0x0(0)\n  Last flood scan length is 2, maximum is 2\n  Last flood scan time is 0 msec, maximum is 4 msec\n  Neighbor Count is 1, Adjacent neighbor count is 1\n    Adjacent with neighbor 1.1.1.1  (Backup Designated Router)\n  Suppress hello for 0 neighbor(s)\n</code></pre> <p>Yes, interfaces operate in different types as seen in highlighted strings above. And R2\u2019s output also says us that R2 elected DR (itself) and BDR (R1).  But what about R2, does it have a neighbor?</p> <pre><code>R2#show ip ospf neighbor\n\nNeighbor ID     Pri   State           Dead Time   Address         Interface\n1.1.1.1           1   FULL/BDR         00:00:33    10.1.2.1        GigabitEthernet1/0\n</code></pre> <p>Apparently, yes, its neighbor is R1 with RID 1.1.1.1 and the state of this adjacency is also FULL. Well, does it mean that despite routers R1 and R2 have different OSPF interface types configured adjacency will be successfully formed and OSPF routes will populate the routing table? To ensure this conclusion lets check routers OSPF databases:</p> <p>R1 OSPF DB:</p> <pre><code>R1# show ip ospf database\n\n            OSPF Router with ID (1.1.1.1) (Process ID 1)\n\n                Router Link States (Area 0)\n\nLink ID         ADV Router      Age         Seq#       Checksum Link count\n1.1.1.1         1.1.1.1         514         0x80000002 0x00FFF6 3\n2.2.2.2         2.2.2.2         515         0x80000002 0x00A545 3\n\n                Net Link States (Area 0)\n\nLink ID         ADV Router      Age         Seq#       Checksum\n10.1.2.2        2.2.2.2         515         0x80000001 0x0021F5\n</code></pre> <p>R2 OSPF DB:</p> <pre><code>R2# show ip ospf database\n\n            OSPF Router with ID (2.2.2.2) (Process ID 1)\n\n                Router Link States (Area 0)\n\nLink ID         ADV Router      Age         Seq#       Checksum Link count\n1.1.1.1         1.1.1.1         1160        0x80000002 0x00FFF6 3\n2.2.2.2         2.2.2.2         1159        0x80000002 0x00A545 2\n\n                Net Link States (Area 0)\n\nLink ID         ADV Router      Age         Seq#       Checksum\n10.1.2.2        2.2.2.2         1159        0x80000001 0x0021F5\n</code></pre> <p>Both routers (as they must) have identical databases: 2 Router LSA (one from each router) and one Network LSA produced by Designated Router R2. Healthy-looking OSPF database. Now lets see if we have OSPF routes in each router\u2019s routing table to wrap this case up:</p> <p>R1's route table:</p> <pre><code>R1#show ip route ospf\nCodes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP\n       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area\n       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2\n       E1 - OSPF external type 1, E2 - OSPF external type 2\n       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2\n       ia - IS-IS inter area, * - candidate default, U - per-user static route\n       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP\n       + - replicated route, % - next hop override\n\nGateway of last resort is not set\n</code></pre> <p>R2's route table:</p> <pre><code>R2#show ip route ospf\nCodes: L - local, C - connected, S - static, R - RIP, M - mobile, B - BGP\n       D - EIGRP, EX - EIGRP external, O - OSPF, IA - OSPF inter area\n       N1 - OSPF NSSA external type 1, N2 - OSPF NSSA external type 2\n       E1 - OSPF external type 1, E2 - OSPF external type 2\n       i - IS-IS, su - IS-IS summary, L1 - IS-IS level-1, L2 - IS-IS level-2\n       ia - IS-IS inter area, * - candidate default, U - per-user static route\n       o - ODR, P - periodic downloaded static route, H - NHRP, l - LISP\n       + - replicated route, % - next hop override\n\nGateway of last resort is not set&lt;/pre&gt;\n</code></pre> <p>Oops, no OSPF routes in routing tables for both routers, how this even possible if we have just seen OSPF databases? Lets examine them one more time, now with detailed look to neighbor\u2019s Router LSA:</p> <p>On R1:</p> <pre><code>R1#show ip ospf database router 2.2.2.2\n\n            OSPF Router with ID (1.1.1.1) (Process ID 1)\n\n  Router Link States (Area 0)\n\n  Adv Router is not-reachable in topology Base with MTID 0\n  LS age: 827\n  Options: (No TOS-capability, DC)\n  LS Type: Router Links\n  Link State ID: 2.2.2.2\n  Advertising Router: 2.2.2.2\n  LS Seq Number: 80000002\n  Checksum: 0xA545\n  Length: 48\n  Number of Links: 2\n\n    Link connected to: a Stub Network\n     (Link ID) Network/subnet number: 2.2.2.2\n     (Link Data) Network Mask: 255.255.255.255\n      Number of MTID metrics: 0\n       TOS 0 Metrics: 1\n\n    Link connected to: a Transit Network\n     (Link ID) Designated Router address: 10.1.2.2\n     (Link Data) Router Interface address: 10.1.2.2\n      Number of MTID metrics: 0\n       TOS 0 Metrics: 1\n</code></pre> <p>On R2:</p> <pre><code>R2#show ip ospf database router 1.1.1.1\n\n            OSPF Router with ID (2.2.2.2) (Process ID 1)\n\n  Router Link States (Area 0)\n\n  Adv Router is not-reachable in topology Base with MTID 0\n  LS age: 995\n  Options: (No TOS-capability, DC)\n  LS Type: Router Links\n  Link State ID: 1.1.1.1\n  Advertising Router: 1.1.1.1\n  LS Seq Number: 80000005\n  Checksum: 0x6777\n  Length: 60\n  Number of Links: 3\n\n    Link connected to: a Stub Network\n     (Link ID) Network/subnet number: 1.1.1.1\n     (Link Data) Network Mask: 255.255.255.255\n      Number of MTID metrics: 0\n       TOS 0 Metrics: 1\n\n    Link connected to: another Router (point-to-point)\n     (Link ID) Neighboring Router ID: 2.2.2.2\n     (Link Data) Router Interface address: 10.1.2.1\n      Number of MTID metrics: 0\n       TOS 0 Metrics: 1\n\n    Link connected to: a Stub Network\n     (Link ID) Network/subnet number: 10.1.2.0\n     (Link Data) Network Mask: 255.255.255.0\n      Number of MTID metrics: 0\n       TOS 0 Metrics: 1\n</code></pre>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#busted","title":"Busted","text":"<p>Gotcha, look at line #7, both routers tell us that they cant reach advertising router based on their calculated topology! And if we recall that every OSPF router builds its own network diagram based on LSA\u2019s it received we should understand what happened behind the scenes.</p> <p>R1 thinks that R2 is its directly connected on point-to-point network to R2, but R2 thinks the other way, that it is connected to a broadcast network. Given that, when Dijkstra algorithm comes in play to build a network topology it literally got lost, because **topology data does not match.**And this is the very reason behind the absence of OSPF routes in R1&amp;R2 tables, SPF algorithm have not produced anything meaningful.</p> <p>Now let me zip this all to a single sentence:</p> <p>Cisco routers successfully form neighbor relationships even if OSPF interfaces configured with different types, however no OSPF routes will be installed into routing tables since OSPF database information is inconsistent.</p> <p>This was a totally strange behavior for me to see, yet Cisco didn't brake any rules. RFC 2328 OSPFv2 does not explicitly restrict to form adjacency for different network types, I think that authors thought it would be obvious not to mix different types on a single network segment. Moreover, OSPF Hello message does not contain any field for interface type, so routers do not know what interface type is on the other side.</p>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#and-what-about-alcatel-lucent-and-juniper","title":"And what about Alcatel-Lucent and Juniper?","text":"<p>But there are other vendors who managed to distinguish between different network interfaces to prevent such a bad situation when adjacency seems to be formed yet no routes are present. And we start with Alcatel-Lucent\u2019s SR-OS v12.0.R8 (If you are new to ALU routers, check this OSPF configuration tutorial).</p> <p></p> <p>Network topology and routers configuration area the same. Booting up routers simultaneously and checking adjacency right after that several times:</p> <p>On R1:</p> <pre><code>*A:R1&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR2                             2.2.2.2         ExchStart  1    0       36\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n\n\n*A:R1&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR2                             2.2.2.2         ExchStart  1    0       33\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n\n\n\n*A:R1&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nNo. of Neighbors: 0\n===============================================================================\n</code></pre> <p>On R2:</p> <pre><code>*A:R2&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR1                             1.1.1.1         Two Way    1    0       30\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n\n\n*A:R2&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR1                             1.1.1.1         Two Way    1    0       38\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n\n\n*A:R2&gt;config&gt;router&gt;ospf# /show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR1                             1.1.1.1         Init       1    0       39\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n</code></pre> <p>Look at this, R1 goes into Exchange Start phase very quickly, this is because its interface is operating in p-t-p mode and do not need to elect DR/BDR. Therefore right after R1 sees itself in received Hello message from R2 it immediately starts to send to R2 its Database Description messages.</p> <p>But R2 cant answer to R1, it needs to elect DR/BDR first, since it is thinking that it is on a broadcast network segment. At the same time R2 sees itself in coming Hello messages \u2013 that is why R2 is in 2Way state for a period of time.</p> <p>And then we see that R1 drops off its neighbor R1 (line #41), effectively putting adjacency to DOWN state. Why would R1 do this? Because it noticed that R2 operates in a different mode and there is no point to peer with it.</p>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#how-point-to-point-interface-detects-broadcast-interface","title":"How point-to-point interface detects broadcast interface?","text":"<p>But how did R1 figured out that R2\u2019s interface is broadcast if Hello messages do not communicate that information?  Well, it took some intellectual analysis from R1. And to get down to the truth we have to dig into debug output of R1:</p> <pre><code>18 2015/01/01 00:27:39.85 UTC MINOR: DEBUG #2001 Base OSPFv2\n\"OSPFv2: PKT\n\n&gt;&gt; Incoming OSPF packet on I/F toR2 area 0.0.0.0\nOSPF Version      : 2\nRouter Id         : 2.2.2.2\nArea Id           : 0.0.0.0\nChecksum          : f694\nAuth Type         : Null\nAuth Key          : 00 00 00 00 00 00 00 00\nPacket Type       : HELLO\nPacket Length     : 48\n\nNetwork Mask      : 255.255.255.0\nHello Interval    : 10\nOptions           : 02 -------E--\nRtr Priority      : 1\nDead Interval     : 40\nDesignated Router : 0.0.0.0\nBackup Router     : 0.0.0.0\nNeighbor-1        : 1.1.1.1\n\"\n\n20 2015/01/01 00:27:44.85 UTC MINOR: DEBUG #2001 Base OSPFv2\n\"OSPFv2: PKT\n\n&gt;&gt; Incoming OSPF packet on I/F toR2 area 0.0.0.0\nOSPF Version      : 2\nRouter Id         : 2.2.2.2\nArea Id           : 0.0.0.0\nChecksum          : de8f\nAuth Type         : Null\nAuth Key          : 00 00 00 00 00 00 00 00\nPacket Type       : HELLO\nPacket Length     : 48\n\nNetwork Mask      : 255.255.255.0\nHello Interval    : 10\nOptions           : 02 -------E--\nRtr Priority      : 1\nDead Interval     : 40\nDesignated Router : 10.1.2.2\nBackup Router     : 10.1.2.1\nNeighbor-1        : 1.1.1.1\n\"\n\n22 2015/01/01 00:27:44.85 UTC MINOR: DEBUG #2001 Base OSPFv2\n\"OSPFv2: PKT DROPPED\ninterface type mismatch\"\n</code></pre> <p>Debug shows incoming OSPF packets, I left only two of them which show the difference. Look, the second OSPF Hello message has DR and BDR IP addresses filled in, and that is why this packet got dropped. R1 knows that it is operating on a point-to-point network, thus it should not receive any Hello messages with DR and BDR IP addresses other then 0.0.0.0. That is why R1 drops this packets and control plane never sees them. And if Hello packets are not coming for Dead interval, R1 thinks that neighbor is down and breaks the neighboring process.</p> <p>This underlying logic effectively stops useless adjacency to form and keeps network administrators from unnecessary troubleshooting.</p>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#junos","title":"JUNOS","text":"<p>Juniper routers (I am running Junos 14.1 on vMX) acts in the same as ALU manner. They reject \u201cforeign\u201d Hello packets on point-to-point interface if they contain DR/BDR addresses.</p> <pre><code>Jun 11 10:14:57.781975 Received OSPF packet of type and wire_length 1, 60\nJun 11 10:14:57.782018 OSPF rcvd Hello 10.1.2.2 -&gt; 224.0.0.5 (ge-0/0/0.0 IFL 329 area 0.0.0.0)\nJun 11 10:14:57.782026   Version 2, length 48, ID 2.2.2.2, area 0.0.0.0\nJun 11 10:14:57.782032   checksum 0x0, authtype 0\nJun 11 10:14:57.782039   mask 255.255.255.0, hello_ivl 10, opts 0x12, prio 128\nJun 11 10:14:57.782045   dead_ivl 40, DR 10.1.2.2, BDR 0.0.0.0\nJun 11 10:14:57.782065 OSPF restart signaling: Received hello with LLS data from nbr ip=10.1.2.2 id=2.2.2.2.\nJun 11 10:14:57.782075 OSPF packet ignored: configuration mismatch from 10.1.2.2 on intf ge-0/0/0.0 area 0.0.0.0\n</code></pre>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#summary","title":"Summary","text":"<ul> <li>Nowadays it is best-practice to configure Ethernet interfaces between directly connected OSPF routers in a point-to-point type to reduce convergence time.</li> <li>It is possible to configure different interface\u2019s type on a single network segment between OSPF routers. Especially interesting the case when one interface configured with point-to-point type and the other with broadcast.</li> <li>Cisco routers form neighbor relationships even if OSPF interfaces configured with different types, however no OSPF routes will be installed into routing tables since OSPF database information is inconsistent.</li> <li>This Cisco\u2019s behavior can lead to unnecessary troubleshooting since adjacency seems up yet no OSPF routes will be seen in routing table.</li> <li>Alcatel-Lucent and Juniper routers effectively prevent adjacency to form in such case. They drop incoming to point-to-point interface Hello packets if they contain DR/BDR IP addresses other then zero.</li> </ul>","tags":["ospf"]},{"location":"2015/ospf-neighbors-on-a-point-to-broadcast-network/#links","title":"Links","text":"<ul> <li>OSPF Version 2 (RFC)</li> </ul>","tags":["ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/","title":"Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial","text":"<p>The purpose of this post is to cover basic OSPFv2 configuration steps and commands for Nokia SROS routers. Intended readers are engineers with basic OSPF knowledge who want to know how to configure OSPF on Alcatel-Lucent Service Routers (7750-SR, 7705-SR, 7210-SR).</p> <p>All examples are valid for <code>TiMOS-B-12.0.R8</code> software.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#single-area-ospf","title":"Single-area OSPF","text":"<p>Basic OSPF protocol configuration in a single area consists of the following steps:</p> <ol> <li>Enable OSPF globally</li> <li>Configure router with OSPF Router ID</li> <li>Configure backbone OSPF Area 0</li> <li>Include interfaces in Area 0</li> </ol> <p>The following network topology will be used throughout this tutorial:</p> <p></p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#enabling-ospf","title":"Enabling OSPF","text":"<p>To enable OSPF on a router simply issue <code>configure router ospf</code> command. This will start OSPF process #0 on a router. If you would like to run another separate OSPF process on the same router, use  <code>configure router ospf &lt;N&gt;</code>, where N is a decimal number of the desired OSPF process.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#router-id","title":"Router ID","text":"<p>Each router running OSPF should have an unique 32-bit identifier, namely Router ID. This identifier will be equal to the first configured value in the following prioritized:</p> <ol> <li><code>router-id</code> value configured globally for a router</li> <li><code>system</code> interface IPv4 address value</li> <li>the last 32 bits of the chassis MAC address</li> </ol> <p>Configuring router-id explicitly:</p> <pre><code>*A:R1# configure router router-id\n  - no router-id\n  - router-id  &lt;ip-address&gt;\n\n  &lt;ip-address&gt;         : a.b.c.d\n\n*A:R1# configure router router-id 2.2.2.2\n</code></pre> <p>Use <code>show router ospf status</code> command to check Router ID current value and OSPF status:</p> <pre><code>*A:R1# show router ospf status\n\n===============================================================================\nOSPFv2 (0) Status\n===============================================================================\nOSPF Cfg Router Id           : 0.0.0.0\nOSPF Oper Router Id          : 1.1.1.1    # Router ID inherited from System IPv4 address\nOSPF Version                 : 2\nOSPF Admin Status            : Enabled    # OSPF administratively enabled\nOSPF Oper Status             : Enabled    # OSPF is operating\n\n&lt;output omitted&gt;\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#configuring-backbone-area","title":"Configuring Backbone Area","text":"<p>Are configuration is done in the OSPF configuration context with <code>area</code> command:</p> <pre><code>A:R1&gt;config&gt;router&gt;ospf# area\n  - area  &lt;area-id&gt;\n  - no area  &lt;area-id&gt;\n\n  &lt;area-id&gt;            :  &lt;ip-address&gt; |  [0..4294967295]\n</code></pre> <p>Area ID can be set either as decimal (<code>area 0</code>) or in dotted-decimal format (<code>area 0.0.0.0</code>).</p> <p>It is easier to enable OSPF and set the Area ID in a single sweep:</p> <pre><code># This command will enable OSPF process (it is disabled by default) and configure Area 0 on this router.\n\nA:R1# configure router ospf area 0\n</code></pre> <p>To check all the configured areas on a router use <code>show router ospf area</code> command:</p> <pre><code>A:R1# show router ospf area\n\n==================================================================\nOSPFv2 (0) all areas\n==================================================================\nArea Id         Type        SPF Runs    LSA Count   LSA Cksum Sum\n------------------------------------------------------------------\n0.0.0.0         Standard    2           1           0xcaf7\n------------------------------------------------------------------\nNo. of OSPF Areas: 1\n==================================================================\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#configuring-ospf-interfaces","title":"Configuring OSPF interfaces","text":"<p>Once the backbone area is configured its time to add some interfaces to it with <code>interface &lt;interface_name&gt;</code> command.</p> <pre><code># Entering to OSPF Area 0 configuration context\n*A:R1# configure router ospf area 0\n\n# Adding system interface to OSPF process\n*A:R1&gt;config&gt;router&gt;ospf&gt;area# interface \"system\"\n*A:R1&gt;config&gt;router&gt;ospf&gt;area&gt;if$ back\n\n# Adding interface toR2 and configuring it with point-to-point type\n# By default, ethernet interfaces use Broadcast interface type \n*A:R1&gt;config&gt;router&gt;ospf&gt;area# interface \"toR2\"\n*A:R1&gt;config&gt;router&gt;ospf&gt;area&gt;if$ interface-type point-to-point\n*A:R1&gt;config&gt;router&gt;ospf&gt;area&gt;if$ back\n*A:R1&gt;config&gt;router&gt;ospf&gt;area# back\n\n# This configuration steps effectively lead us to this OSPF configuration for router R1\n*A:R1&gt;config&gt;router&gt;ospf# info\n----------------------------------------------\n            area 0.0.0.0\n                interface \"system\"\n                    no shutdown\n                exit\n                interface \"toR2\"\n                    interface-type point-to-point\n                    no shutdown\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>To check that OSPF interfaces were configured properly by evaluating their status use:</p> <pre><code>*A:R1# show router ospf interface\n\n===============================================================================\nOSPFv2 (0) all interfaces\n===============================================================================\nIf Name               Area Id         Designated Rtr  Bkup Desig Rtr  Adm  Oper\n-------------------------------------------------------------------------------\nsystem                0.0.0.0         1.1.1.1         0.0.0.0         Up   DR\ntoR2                  0.0.0.0         0.0.0.0         0.0.0.0         Up   PToP\n-------------------------------------------------------------------------------\nNo. of OSPF Interfaces: 2\n===============================================================================\n</code></pre> <p>Repeat the same configuration steps to include all interfaces to OSPF Area 0 for the other backbone routers R2, R3, R4 and you will end up with a fully configured OSPF Backbone Area.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#verification","title":"Verification","text":"<p>Finally its time to check that our routers have established the neighboring relationships:</p> <pre><code>A:R1# show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR2                             2.2.2.2         Full       1    0       33\n   0.0.0.0\ntoR3                             3.3.3.3         Full       1    0       39\n   0.0.0.0\n-------------------------------------------------------------------------------\nNo. of Neighbors: 2\n===============================================================================\n</code></pre> <p>One of the most useful OSPF verification commands is <code>show router ospf database</code>. This command shows all the Links State Advertisements (LSA) and helps the engineer to troubleshoot OSPF-related issues.</p> <pre><code>A:R1# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.0         1.1.1.1         1.1.1.1         873  0x8000000b 0x4dd5\nRouter  0.0.0.0         2.2.2.2         2.2.2.2         561  0x80000006 0x1c2b\nRouter  0.0.0.0         3.3.3.3         3.3.3.3         879  0x80000008 0x6d94\nRouter  0.0.0.0         4.4.4.4         4.4.4.4         1588 0x80000005 0x94ec\n-------------------------------------------------------------------------------\nNo. of LSAs: 4\n===============================================================================\n</code></pre> <p>You can query OSPF database for a specific LSAs by augmenting the above mentioned command:</p> <pre><code>A:R1# show router ospf database\n  - database [type {router|network|summary|asbr-summary|external|nssa|all}]\n    [area  &lt;area-id&gt;] [adv-router  &lt;router-id&gt;] [ &lt;link-state-id&gt;] [detail]\n\n  &lt;router|network|su*&gt; : keywords - specify database type\n  &lt;area-id&gt;            : ip-address - a.b.c.d\n                        area - [0..4294967295]\n  &lt;router-id&gt;          : a.b.c.d\n  &lt;link-state-id&gt;      : a.b.c.d\n  &lt;detail&gt;             : keyword - displays detailed information\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#multi-area-ospf","title":"Multi-area OSPF","text":"<p>Basic Multi-area OSPF configuration is straightforward as well. I added two more routers to the topology and introduced two areas: Area 1 and Area 2.</p> <p></p> <p>I mistyped the port numbers for R5-R1 and R6-R2 pairs, it should be 1/1/4. Though this wont affect the course of this tutorial in anyway.</p> <p>We will start by configuring <code>Area 1</code> on routers R5 and R1 using the same commands we used for single-area OSPF configuration.</p> <p>R5 configuration:</p> <pre><code># Creating Area 1 on R5 and adding \"system\" and \"toR1\" interfaces\nA:R5# configure router ospf area 1\n*A:R5&gt;config&gt;router&gt;ospf&gt;area$ interface \"toR1\" interface-type point-to-point\n*A:R5&gt;config&gt;router&gt;ospf&gt;area$ interface \"system\"\n\n\n# Verifying created Area 1 and its interfaces\n*A:R5# show router ospf interface\n\n===============================================================================\nOSPFv2 (0) all interfaces\n===============================================================================\nIf Name               Area Id         Designated Rtr  Bkup Desig Rtr  Adm  Oper\n-------------------------------------------------------------------------------\nsystem                0.0.0.1         5.5.5.5         0.0.0.0         Up   DR\ntoR1                  0.0.0.1         0.0.0.0         0.0.0.0         Up   PToP\n-------------------------------------------------------------------------------\nNo. of OSPF Interfaces: 2\n===============================================================================\n</code></pre> <p>R1 configuration</p> <pre><code># Creating Area 1 on R1 (ABR) and adding \"toR5\" interface to it.\n\n*A:R1# configure router ospf area 1\n*A:R1&gt;config&gt;router&gt;ospf&gt;area$ interface \"toR5\" interface-type point-to-point\n\n\n# Verifying created Area 1 and its interfaces\n*A:R1# show router ospf interface\n\n===============================================================================\nOSPFv2 (0) all interfaces\n===============================================================================\nIf Name               Area Id         Designated Rtr  Bkup Desig Rtr  Adm  Oper\n-------------------------------------------------------------------------------\nsystem                0.0.0.0         1.1.1.1         0.0.0.0         Up   DR\ntoR2                  0.0.0.0         0.0.0.0         0.0.0.0         Up   PToP\ntoR3                  0.0.0.0         0.0.0.0         0.0.0.0         Up   PToP\ntoR5                  0.0.0.1         0.0.0.0         0.0.0.0         Up   PToP\n-------------------------------------------------------------------------------\nNo. of OSPF Interfaces: 4\n===============================================================================\n</code></pre> <p>Adding one more Area (besides backbone Area 0) on R1 makes it Area Border Router. So R1 will form and maintain another neighbor relationships with R5 in Area 1. We will check if its true:</p> <p>R5 verification:</p> <pre><code>*A:R5# show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR1                             1.1.1.1         Full       1    0       33\n   0.0.0.1\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n</code></pre> <p>R1 verification:</p> <pre><code>*A:R1# show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR2                             2.2.2.2         Full       1    0       32\n   0.0.0.0\ntoR3                             3.3.3.3         Full       1    0       36\n   0.0.0.0\ntoR5                             5.5.5.5         Full       1    0       31\n   0.0.0.1\n-------------------------------------------------------------------------------\nNo. of Neighbors: 3\n===============================================================================\n</code></pre> <p>I repeated same configuration steps on R6: added it to Area 2 and neighbored with R2.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#examining-multi-area-lsdb","title":"Examining Multi-area LSDB","text":"<p>Since we configured multi-area OSPF we should expect to see some new LSA in our Link State Database:</p> <p>On R1:</p> <pre><code>A:R1# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.0         1.1.1.1         1.1.1.1         550  0x80000007 0x58cd\nRouter  0.0.0.0         2.2.2.2         2.2.2.2         668  0x80000006 0xaf65\nRouter  0.0.0.0         3.3.3.3         3.3.3.3         193  0x80000006 0x7192\nRouter  0.0.0.0         4.4.4.4         4.4.4.4         732  0x80000007 0xa34d\nSummary 0.0.0.0         5.5.5.5         1.1.1.1         773  0x80000002 0x508f\nSummary 0.0.0.0         10.1.5.0        1.1.1.1         1091 0x80000002 0x7172\nSummary 0.0.0.0         6.6.6.6         2.2.2.2         519  0x80000002 0x4d3\nSummary 0.0.0.0         10.2.6.0        2.2.2.2         724  0x80000002 0x3ca1\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         830  0x80000004 0x50ea\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         171  0x80000005 0x47bb\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         660  0x80000002 0x1d37\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         183  0x80000002 0xda11\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         122  0x80000002 0xac3b\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         318  0x80000002 0x6a15\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         207  0x80000002 0xe69\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         377  0x80000003 0x9055\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         262  0x80000003 0x855f\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         1039 0x80000003 0x5a24\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         864  0x80000002 0x4637\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         1056 0x80000003 0x4e2f\n-------------------------------------------------------------------------------\nNo. of LSAs: 20\n===============================================================================\n</code></pre> <p>On R5:</p> <pre><code>A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         900  0x80000004 0x50ea\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         238  0x80000005 0x47bb\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         728  0x80000002 0x1d37\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         252  0x80000002 0xda11\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         190  0x80000002 0xac3b\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         387  0x80000002 0x6a15\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         276  0x80000002 0xe69\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         446  0x80000003 0x9055\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         329  0x80000003 0x855f\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         1106 0x80000003 0x5a24\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         932  0x80000002 0x4637\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         1124 0x80000003 0x4e2f\n-------------------------------------------------------------------------------\nNo. of LSAs: 12\n===============================================================================\n</code></pre> <p>Aha, R1 being an ABR lists all LSA's for both Area 0 and Area 1. Moreover, R1 lists Type 3 Summary LSA from Area 0 to Area 1 and vice versa, from Area 1 to Area 0.</p> <p>R5 has only Area 1 LSAs, since R5 \"lives\" exactly in a single area - Area 1.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#ospf-routes-propagation","title":"OSPF routes propagation","text":"<p>To ensure that OSPF routers exchanged OSPF routes lets check R5 and R1 routing tables:</p> <p>On R1 (ABR):</p> <pre><code># Checking routes that have been received via OSPF\n\n*A:R1# show router route-table protocol ospf\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n2.2.2.2/32                                    Remote  OSPF      03h01m17s  10\n       10.1.2.2                                                     100\n3.3.3.3/32                                    Remote  OSPF      03h01m13s  10\n       10.1.3.3                                                     100\n4.4.4.4/32                                    Remote  OSPF      03h01m10s  10\n       10.1.2.2                                                     200\n5.5.5.5/32                                    Remote  OSPF      00h39m07s  10\n       10.1.5.5                                                     100\n6.6.6.6/32                                    Remote  OSPF      00h19m38s  10\n       10.1.2.2                                                     200\n10.2.4.0/24                                   Remote  OSPF      04h15m26s  10\n       10.1.2.2                                                     200\n10.2.6.0/24                                   Remote  OSPF      00h19m48s  10\n       10.1.2.2                                                     200\n10.3.4.0/24                                   Remote  OSPF      04h15m26s  10\n       10.1.3.3                                                     200\n-------------------------------------------------------------------------------\nNo. of Routes: 8\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n\n\n# Checking R1's route table\n\nA:R1# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n1.1.1.1/32                                    Local   Local     00h20m56s  0\n       system                                                       0\n2.2.2.2/32                                    Remote  OSPF      00h15m29s  10\n       10.1.2.2                                                     100\n3.3.3.3/32                                    Remote  OSPF      00h15m11s  10\n       10.1.3.3                                                     100\n4.4.4.4/32                                    Remote  OSPF      00h14m40s  10\n       10.1.2.2                                                     200\n5.5.5.5/32                                    Remote  OSPF      00h18m59s  10\n       10.1.5.5                                                     100\n6.6.6.6/32                                    Remote  OSPF      00h16m28s  10\n       10.1.2.2                                                     200\n10.1.2.0/24                                   Local   Local     00h20m36s  0\n       toR2                                                         0\n10.1.3.0/24                                   Local   Local     00h20m36s  0\n       toR3                                                         0\n10.1.5.0/24                                   Local   Local     00h20m36s  0\n       toR5                                                         0\n10.2.4.0/24                                   Remote  OSPF      00h20m31s  10\n       10.1.2.2                                                     200\n10.2.6.0/24                                   Remote  OSPF      00h16m38s  10\n       10.1.2.2                                                     200\n10.3.4.0/24                                   Remote  OSPF      00h20m30s  10\n       10.1.3.3                                                     200\n-------------------------------------------------------------------------------\nNo. of Routes: 12\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n</code></pre> <p>On R5:</p> <pre><code>*A:R5# show router route-table protocol ospf\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n1.1.1.1/32                                    Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     100\n2.2.2.2/32                                    Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     200\n3.3.3.3/32                                    Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     200\n4.4.4.4/32                                    Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     300\n6.6.6.6/32                                    Remote  OSPF      00h20m14s  10\n       10.1.5.1                                                     300\n10.1.2.0/24                                   Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     200\n10.1.3.0/24                                   Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     200\n10.2.4.0/24                                   Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     300\n10.2.6.0/24                                   Remote  OSPF      00h20m25s  10\n       10.1.5.1                                                     300\n10.3.4.0/24                                   Remote  OSPF      02h57m09s  10\n       10.1.5.1                                                     300\n-------------------------------------------------------------------------------\nNo. of Routes: 10\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n\n\n# Checking R5's route table\n\nA:R5# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n1.1.1.1/32                                    Remote  OSPF      00h20m13s  10\n       10.1.5.1                                                     100\n2.2.2.2/32                                    Remote  OSPF      00h19m27s  10\n       10.1.5.1                                                     200\n3.3.3.3/32                                    Remote  OSPF      00h19m09s  10\n       10.1.5.1                                                     200\n4.4.4.4/32                                    Remote  OSPF      00h18m37s  10\n       10.1.5.1                                                     300\n5.5.5.5/32                                    Local   Local     00h24m57s  0\n       system                                                       0\n6.6.6.6/32                                    Remote  OSPF      00h20m25s  10\n       10.1.5.1                                                     300\n10.1.2.0/24                                   Remote  OSPF      00h23m46s  10\n       10.1.5.1                                                     200\n10.1.3.0/24                                   Remote  OSPF      00h23m46s  10\n       10.1.5.1                                                     200\n10.1.5.0/24                                   Local   Local     00h24m37s  0\n       toR1                                                         0\n10.2.4.0/24                                   Remote  OSPF      00h23m47s  10\n       10.1.5.1                                                     300\n10.2.6.0/24                                   Remote  OSPF      00h20m36s  10\n       10.1.5.1                                                     300\n10.3.4.0/24                                   Remote  OSPF      00h23m47s  10\n       10.1.5.1                                                     300\n-------------------------------------------------------------------------------\nNo. of Routes: 12\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n</code></pre> <p>Since we configured all the routers in our topology and verified that the routes have been propagated properly we can try to run <code>ping</code> between R6 and R5 system interfaces:</p> <pre><code>A:R6# ping 5.5.5.5\nPING 5.5.5.5 56 data bytes\n64 bytes from 5.5.5.5: icmp_seq=1 ttl=62 time=11.7ms.\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#ospf-route-summarization","title":"OSPF Route Summarization","text":"<p>So far we have configured multi-area OSPF topology with three Areas. One of the multi-area OSPF benefits is to perform manual route summarization. In this section we will configure OSPF route summarization between Area 0 and Area 1.</p> <p>The idea is depicted below where we will have a single Summary LSA (Type 3) in the Area 1 instead of three different LSAs.</p> <p></p> <p>To get a range of IP addresses which we will summarize later we will add 3 loopback interfaces to R3 router and include them in OSPF Area 0:</p> <pre><code>A:R3# show router interface\n\n===============================================================================\nInterface Table (Router: Base)\n===============================================================================\nInterface-Name                   Adm         Opr(v4/v6)  Mode    Port/SapId\n   IP-Address                                                    PfxState\n-------------------------------------------------------------------------------\nlo1                              Up          Up/--       Network loopback\n   192.168.3.1/32                                                n/a\nlo2                              Up          Up/--       Network loopback\n   192.168.3.2/32                                                n/a\nlo3                              Up          Up/--       Network loopback\n   192.168.3.3/32                                                n/a\n &lt;output omitted&gt;\n\n\n*A:R3# show router ospf interface\n\n===============================================================================\nOSPFv2 (0) all interfaces\n===============================================================================\nIf Name               Area Id         Designated Rtr  Bkup Desig Rtr  Adm  Oper\n-------------------------------------------------------------------------------\nsystem                0.0.0.0         3.3.3.3         0.0.0.0         Up   DR\ntoR1                  0.0.0.0         0.0.0.0         0.0.0.0         Up   PToP\ntoR4                  0.0.0.0         0.0.0.0         0.0.0.0         Up   PToP\nlo1                   0.0.0.0         3.3.3.3         0.0.0.0         Up   DR\nlo2                   0.0.0.0         3.3.3.3         0.0.0.0         Up   DR\nlo3                   0.0.0.0         3.3.3.3         0.0.0.0         Up   DR\n-------------------------------------------------------------------------------\nNo. of OSPF Interfaces: 6\n===============================================================================\n</code></pre> <p>Since we added these interfaces to OSPF Area 0 we see them coming to R5 router as Type 3 Network Summary LSA. And these routes make their way to the routing table of R5.</p> <pre><code>A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         531  0x80000005 0x4eeb\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         43   0x80000006 0x45bc\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         496  0x80000003 0x1b38\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         1898 0x80000002 0xda11\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         225  0x80000003 0xaa3c\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         463  0x80000003 0x6816\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         373  0x80000003 0xc6a\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         323  0x80000004 0x8e56\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         32   0x80000004 0x8360\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         513  0x80000004 0x5825\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         1089 0x80000003 0x4438\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         1188 0x80000004 0x4c30\nSummary 0.0.0.1         192.168.3.1     1.1.1.1         566  0x80000001 0x5c2b\nSummary 0.0.0.1         192.168.3.2     1.1.1.1         560  0x80000001 0x5234\nSummary 0.0.0.1         192.168.3.3     1.1.1.1         554  0x80000001 0x483d\n-------------------------------------------------------------------------------\nNo. of LSAs: 15\n===============================================================================\n\n\nA:R5# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n &lt; &lt;ouput omitted&gt;&gt;\n\n192.168.3.1/32                                Remote  OSPF      00h10m07s  10\n       10.1.5.1                                                     200\n192.168.3.2/32                                Remote  OSPF      00h10m02s  10\n       10.1.5.1                                                     200\n192.168.3.3/32                                Remote  OSPF      00h09m56s  10\n       10.1.5.1                                                     200\n-------------------------------------------------------------------------------\nNo. of Routes: 15\nFlags: n = Number of times nexthop is repeated\n       B = BGP backup route available\n       L = LFA nexthop available\n       S = Sticky ECMP requested\n===============================================================================\n</code></pre> <p>We will configure route summarization on Area Border Router (R1), so it will advertise only one summary route <code>192.168.3.0/30</code> instead of three specific routes.</p> <p>Configuration steps:</p> <pre><code>A:R1# configure router ospf area 0\nA:R1&gt;config&gt;router&gt;ospf&gt;area# area-range 192.168.3.0/30\n</code></pre> <p>Pay attention, that summarization command <code>area-range</code> must be applied in the context of the area being summarized. Since we are summarizing routes from Area 0 to Area 1 we use this command in the Area 0 configuration context.</p> <p>By default, the command <code>area-range &lt;prefix&gt;/&lt;length&gt;</code> will actually be expanded to <code>area-range &lt;prefix&gt;/&lt;length&gt; advertise</code>, meaning that ABR will advertise this prefix. Counterpart statement <code>not-advertise</code> can be supplied to enable route suppression and will be discussed in a detail in next section.</p> <p>Pay attention: route summarization procedure automatically adds a black-hole route in R1's route table:</p> <pre><code>*A:R1# show router route-table 192.168.3.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.3.0/30                                Blackh* OSPF      00h32m52s  255\n       Black Hole                                                   100\n192.168.3.1/32                                Remote  OSPF      00h53m58s  10\n       10.1.3.3                                                     100\n192.168.3.2/32                                Remote  OSPF      00h53m52s  10\n       10.1.3.3                                                     100\n192.168.3.3/32                                Remote  OSPF      00h53m47s  10\n       10.1.3.3                                                     100\n-------------------------------------------------------------------------------\nNo. of Routes: 4\n</code></pre> <p>Lets see what has changed at R5. R5's LSDB now has just one Type 3 Summary LSA for the aggregated <code>192.168.3.0/30</code> network and has no LSA's for the specific prefixes:</p> <pre><code>A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1234 0x80000005 0x4eeb\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         746  0x80000006 0x45bc\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         1199 0x80000003 0x1b38\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         658  0x80000003 0xd812\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         928  0x80000003 0xaa3c\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         1166 0x80000003 0x6816\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         1076 0x80000003 0xc6a\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         1026 0x80000004 0x8e56\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         735  0x80000004 0x8360\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         1216 0x80000004 0x5825\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         420  0x80000004 0x4239\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         340  0x80000005 0x4a31\nSummary 0.0.0.1         192.168.3.0     1.1.1.1         4    0x80000001 0x5437\n-------------------------------------------------------------------------------\nNo. of LSAs: 13\n===============================================================================\n\n\n\nA:R5# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n &lt;ouptut omitted&gt;    \n\n192.168.3.0/30                                Remote  OSPF      00h30m58s  10\n       10.1.5.1                                                     200\n-------------------------------------------------------------------------------\nNo. of Routes: 13\n===============================================================================\n</code></pre> <p>Verifying that IP connectivity works for summarized route:</p> <pre><code>A:R5# ping 192.168.3.3\nPING 192.168.3.3 56 data bytes\n64 bytes from 192.168.3.3: icmp_seq=1 ttl=63 time=10.2ms.\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#ospf-route-filtering-on-abr","title":"OSPF Route filtering on ABR","text":"<p>You can filter unwanted routes on the ABR with the <code>area-range</code> command adding the key <code>not-advertise</code>.</p> <p></p> <p>For example lets take a look at R6's route table which contains specific routes to R3's loopback addresses. These addresses is advertising by R2 since it is acting as ABR and advertises all the routes it has in its routing table.</p> <pre><code>A:R6# show router route-table 192.168.3.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.3.1/32                                Remote  OSPF      01h21m48s  10\n       10.2.6.2                                                     300\n192.168.3.2/32                                Remote  OSPF      01h21m43s  10\n       10.2.6.2                                                     300\n192.168.3.3/32                                Remote  OSPF      01h21m37s  10\n       10.2.6.2                                                     300\n-------------------------------------------------------------------------------\nNo. of Routes: 3\n</code></pre> <p>If we want to prevent R2 from advertising some routes to its neighbor in Area 1 then we have to add configuration commands in Area 0's context of R2:</p> <pre><code>A:R2# configure router ospf area 0\nA:R2&gt;config&gt;router&gt;ospf&gt;area# area-range 192.168.3.1/32 not-advertise\n</code></pre> <p>By doing this, we tell R2 to stop advertising Type 3 Summary LSA for prefix 192.168.3.1/32. And R6's route table immediately reflects this change by not having this specific route in its route table:</p> <pre><code>A:R6# show router route-table 192.168.3.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.3.2/32                                Remote  OSPF      01h28m38s  10\n       10.2.6.2                                                     300\n192.168.3.3/32                                Remote  OSPF      01h28m32s  10\n       10.2.6.2                                                     300\n-------------------------------------------------------------------------------\nNo. of Routes: 2\n</code></pre> <p>You can also use \"summary\" prefix to filter a range of routes:</p> <pre><code># filter all prefixes that falls under 192.168.3.0/30 aggregate\nA:R2&gt;config&gt;router&gt;ospf&gt;area# area-range 192.168.3.0/30 not-advertise\n\n# All loopback routes are filtered by R2\nA:R6# show router route-table 192.168.3.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\nNo. of Routes: 0\n</code></pre> <p>Note: Route filtering does not install any black-hole routes.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#asbr-and-ospf-route-redistribution","title":"ASBR and OSPF Route Redistribution","text":"<p>Routes between different routing domains can me mutually exchanged. External routers can be exported into OSPF and vice versa. The process of routes exchange is often called route redistribution.</p> <p></p> <p>In this section we will redistribute the routes that are in R5's route table but are not advertised yet into OSPF process. These routes are created by means of additional loopback interfaces that are not included into OSPF process.</p> <pre><code>*A:R5# show router route-table 192.168.5.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.5.1/32                                Local   Local     00h00m14s  0\n       lo1                                                          0\n192.168.5.2/32                                Local   Local     00h00m09s  0\n       lo2                                                          0\n192.168.5.3/32                                Local   Local     00h00m05s  0\n       lo3                                                          0\n-------------------------------------------------------------------------------\nNo. of Routes: 3\n\n\n# ensure that loopbacks are not OSPF-enabled interfaces\n*A:R5# show router ospf interface\n\n===============================================================================\nOSPFv2 (0) all interfaces\n===============================================================================\nIf Name               Area Id         Designated Rtr  Bkup Desig Rtr  Adm  Oper\n-------------------------------------------------------------------------------\nsystem                0.0.0.1         5.5.5.5         0.0.0.0         Up   DR\ntoR1                  0.0.0.1         0.0.0.0         0.0.0.0         Up   PToP\n-------------------------------------------------------------------------------\nNo. of OSPF Interfaces: 2\n===============================================================================\n\n\n# and R1 has no routes towards these prefixes, since they are only local to R5\nA:R1# show router route-table 192.168.5.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nNo. of Routes: 0\n</code></pre> <p>To redistribute these local routes into OSPF process a policy should be configured. Lets give our policy a <code>EXPORT Local Loopbacks</code> name:</p> <pre><code>*A:R5# configure router policy-options\n\n\n*A:R5&gt;config&gt;router&gt;policy-options# begin\n\n# create prefix list to match desired routes\n*A:R5&gt;config&gt;router&gt;policy-options# prefix-list \"Loopbacks_to_export\"\n*A:R5&gt;config&gt;router&gt;policy-options&gt;prefix-list$ prefix 192.168.5.0/24 longer\n*A:R5&gt;config&gt;router&gt;policy-options&gt;prefix-list$ back\n\n# create policy statement to export routes\n*A:R5&gt;config&gt;router&gt;policy-options# policy-statement \"EXPORT Local Loopbacks\"\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement$ entry 10\n# refer to the prefix list created earlier\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ from prefix-list \"Loopbacks_to_export\"\n# narrow the scope of this policy \"to protocol ospf\" only\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ to protocol ospf\n\n# set accept action (default action is deny all)\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry# action accept\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry&gt;action# back\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ back\n*A:R5&gt;config&gt;router&gt;policy-options&gt;policy-statement$ back\n\n# commit changes\n*A:R5&gt;config&gt;router&gt;policy-options# commit\n\n\n# verify the created policy\n*A:R5&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"Loopbacks_to_export\"\n                prefix 192.168.5.0/24 longer\n            exit\n            policy-statement \"EXPORT Local Loopbacks\"\n                entry 10\n                    from\n                        prefix-list \"Loopbacks_to_export\"\n                    exit\n                    to\n                        protocol ospf\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n</code></pre> <p>The next step is to configure R5 router as an ASBR and to apply the created policy to OSPF process:</p> <pre><code># \"asbr\" keyword makes a router an ASBR\n*A:R5# configure router ospf\n*A:R5&gt;config&gt;router&gt;ospf# asbr\n# apply the export policy\n*A:R5&gt;config&gt;router&gt;ospf# export \"EXPORT Local Loopbacks\"\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#verifying-redistributed-routes-propagation","title":"Verifying redistributed routes propagation","text":"<p>Now, our R5 router is now configured as an ASBR and the local routes should get exported into OSPF process. These changes allow other routers in OSPF domain to receive these routes by means of Type 4 ASBR Summary LSA and Type 5 AS External LSA:</p> <p>R5:</p> <pre><code>*A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1033 0x80000004 0x50ea\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         1072 0x80000006 0x4bb4\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         706  0x80000004 0x1939\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         2226 0x80000002 0xda11\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         107  0x80000003 0xaa3c\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         929  0x80000003 0x6816\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         1879 0x80000002 0xe69\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         366  0x80000003 0x9055\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         472  0x80000003 0x855f\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         737  0x80000003 0x5a24\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         939  0x80000003 0x4438\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         442  0x80000003 0x4e2f\nSummary 0.0.0.1         192.168.3.0     1.1.1.1         355  0x80000003 0x5039\nAS Ext  n/a             192.168.5.1     5.5.5.5         506  0x80000001 0x63ea\nAS Ext  n/a             192.168.5.2     5.5.5.5         506  0x80000001 0x59f3\nAS Ext  n/a             192.168.5.3     5.5.5.5         506  0x80000001 0x4ffc\n-------------------------------------------------------------------------------\nNo. of LSAs: 16\n===============================================================================\n</code></pre> <p>R1:</p> <pre><code>A:R1# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.0         1.1.1.1         1.1.1.1         655  0x80000006 0x5acc\nRouter  0.0.0.0         2.2.2.2         2.2.2.2         988  0x80000006 0xaf65\nRouter  0.0.0.0         3.3.3.3         3.3.3.3         1122 0x80000009 0x1a6b\nRouter  0.0.0.0         4.4.4.4         4.4.4.4         1133 0x80000006 0xa54c\nSummary 0.0.0.0         5.5.5.5         1.1.1.1         562  0x80000003 0x4e90\nSummary 0.0.0.0         10.1.5.0        1.1.1.1         472  0x80000003 0x6f73\nSummary 0.0.0.0         6.6.6.6         2.2.2.2         189  0x80000003 0x2d4\nSummary 0.0.0.0         10.2.6.0        2.2.2.2         166  0x80000003 0x3aa2\nAS Summ 0.0.0.0         5.5.5.5         1.1.1.1         1407 0x80000001 0x449b\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1368 0x80000004 0x50ea\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         1409 0x80000006 0x4bb4\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         1042 0x80000004 0x1939\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         321  0x80000003 0xd812\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         443  0x80000003 0xaa3c\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         1265 0x80000003 0x6816\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         266  0x80000003 0xc6a\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         703  0x80000003 0x9055\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         809  0x80000003 0x855f\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         1074 0x80000003 0x5a24\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         1275 0x80000003 0x4438\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         779  0x80000003 0x4e2f\nSummary 0.0.0.1         192.168.3.0     1.1.1.1         691  0x80000003 0x5039\nAS Ext  n/a             192.168.5.1     5.5.5.5         843  0x80000001 0x63ea\nAS Ext  n/a             192.168.5.2     5.5.5.5         843  0x80000001 0x59f3\nAS Ext  n/a             192.168.5.3     5.5.5.5         843  0x80000001 0x4ffc\n-------------------------------------------------------------------------------\nNo. of LSAs: 25\n===============================================================================\n</code></pre> <p>R3:</p> <pre><code>A:R3# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.0         1.1.1.1         1.1.1.1         683  0x80000006 0x5acc\nRouter  0.0.0.0         2.2.2.2         2.2.2.2         1016 0x80000006 0xaf65\nRouter  0.0.0.0         3.3.3.3         3.3.3.3         1148 0x80000009 0x1a6b\nRouter  0.0.0.0         4.4.4.4         4.4.4.4         1159 0x80000006 0xa54c\nSummary 0.0.0.0         5.5.5.5         1.1.1.1         591  0x80000003 0x4e90\nSummary 0.0.0.0         10.1.5.0        1.1.1.1         499  0x80000003 0x6f73\nSummary 0.0.0.0         6.6.6.6         2.2.2.2         217  0x80000003 0x2d4\nSummary 0.0.0.0         10.2.6.0        2.2.2.2         193  0x80000003 0x3aa2\nAS Summ 0.0.0.0         5.5.5.5         1.1.1.1         1434 0x80000001 0x449b\nAS Ext  n/a             192.168.5.1     5.5.5.5         870  0x80000001 0x63ea\nAS Ext  n/a             192.168.5.2     5.5.5.5         870  0x80000001 0x59f3\nAS Ext  n/a             192.168.5.3     5.5.5.5         870  0x80000001 0x4ffc\n-------------------------------------------------------------------------------\nNo. of LSAs: 12\n===============================================================================\n</code></pre> <p>R6:</p> <pre><code>A:R6# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.2         2.2.2.2         2.2.2.2         432  0x80000004 0x76b3\nRouter  0.0.0.2         6.6.6.6         6.6.6.6         1309 0x80000006 0xf9f2\nSummary 0.0.0.2         1.1.1.1         2.2.2.2         535  0x80000003 0xe802\nSummary 0.0.0.2         2.2.2.2         2.2.2.2         1098 0x80000004 0xcc7d\nSummary 0.0.0.2         3.3.3.3         2.2.2.2         524  0x80000003 0x7806\nSummary 0.0.0.2         4.4.4.4         2.2.2.2         1427 0x80000003 0x5e80\nSummary 0.0.0.2         5.5.5.5         2.2.2.2         471  0x80000003 0x1c5a\nSummary 0.0.0.2         10.1.2.0        2.2.2.2         661  0x80000003 0x726f\nSummary 0.0.0.2         10.1.3.0        2.2.2.2         85   0x80000003 0x5329\nSummary 0.0.0.2         10.1.5.0        2.2.2.2         200  0x80000003 0x3d3d\nSummary 0.0.0.2         10.2.4.0        2.2.2.2         58   0x80000003 0x508e\nSummary 0.0.0.2         10.3.4.0        2.2.2.2         549  0x80000003 0x3049\nAS Summ 0.0.0.2         5.5.5.5         2.2.2.2         1460 0x80000001 0x1265\nAS Ext  n/a             192.168.5.1     5.5.5.5         899  0x80000001 0x63ea\nAS Ext  n/a             192.168.5.2     5.5.5.5         899  0x80000001 0x59f3\nAS Ext  n/a             192.168.5.3     5.5.5.5         899  0x80000001 0x4ffc\n-------------------------------------------------------------------------------\nNo. of LSAs: 16\n===============================================================================\n\n\nA:R6# show router route-table 192.168.5.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.5.1/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n192.168.5.2/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n192.168.5.3/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n-------------------------------------------------------------------------------\nNo. of Routes: 3\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#ospf-stub-area","title":"OSPF Stub Area","text":"<p>OSPF Stub Areas help to optimize LSDB and routing tables of the routers. We will configure Area 2 as a stub area and this will tell ABR (R2) to not distribute any External routes and send a default route into Area 2 instead. To configure Area 2 as stub you need to configure all OSPF routers inside this area:</p> <p></p> <pre><code># on R2 (ABR)\nA:R2# configure router ospf\nA:R2&gt;config&gt;router&gt;ospf# area 2 stub\n\n\n# on R6 (Area 2 router)\nA:R6# configure router ospf area 2 stub\n</code></pre> <p>The following \"before/after\" comparison shows that after configuring Area 2's routers as stub, R6 router no longer receives ASBR Summary LSA and AS External LSA nor it has routes 192.168.5.1-3/32. Instead it has a new Summary LSA from the ABR with the Link State ID 0.0.0.0 which means default route.</p> <p>R6 before stub area configuration:</p> <pre><code>A:R6# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.2         2.2.2.2         2.2.2.2         432  0x80000004 0x76b3\nRouter  0.0.0.2         6.6.6.6         6.6.6.6         1309 0x80000006 0xf9f2\nSummary 0.0.0.2         1.1.1.1         2.2.2.2         535  0x80000003 0xe802\nSummary 0.0.0.2         2.2.2.2         2.2.2.2         1098 0x80000004 0xcc7d\nSummary 0.0.0.2         3.3.3.3         2.2.2.2         524  0x80000003 0x7806\nSummary 0.0.0.2         4.4.4.4         2.2.2.2         1427 0x80000003 0x5e80\nSummary 0.0.0.2         5.5.5.5         2.2.2.2         471  0x80000003 0x1c5a\nSummary 0.0.0.2         10.1.2.0        2.2.2.2         661  0x80000003 0x726f\nSummary 0.0.0.2         10.1.3.0        2.2.2.2         85   0x80000003 0x5329\nSummary 0.0.0.2         10.1.5.0        2.2.2.2         200  0x80000003 0x3d3d\nSummary 0.0.0.2         10.2.4.0        2.2.2.2         58   0x80000003 0x508e\nSummary 0.0.0.2         10.3.4.0        2.2.2.2         549  0x80000003 0x3049\nAS Summ 0.0.0.2         5.5.5.5         2.2.2.2         1460 0x80000001 0x1265\nAS Ext  n/a             192.168.5.1     5.5.5.5         899  0x80000001 0x63ea\nAS Ext  n/a             192.168.5.2     5.5.5.5         899  0x80000001 0x59f3\nAS Ext  n/a             192.168.5.3     5.5.5.5         899  0x80000001 0x4ffc\n-------------------------------------------------------------------------------\nNo. of LSAs: 16\n===============================================================================\n\n\n\nA:R6# show router route-table 192.168.5.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n192.168.5.1/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n192.168.5.2/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n192.168.5.3/32                                Remote  OSPF      00h28m27s  150\n       10.2.6.2                                                     1\n-------------------------------------------------------------------------------\nNo. of Routes: 3\n</code></pre> <p>R6 after stub area configuration:</p> <pre><code>*A:R6# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.2         2.2.2.2         2.2.2.2         199  0x80000002 0xecdf\nRouter  0.0.0.2         6.6.6.6         6.6.6.6         0    0x80000002 0x20d2\nSummary 0.0.0.2         0.0.0.0         2.2.2.2         5    0x80000004 0x5102\nSummary 0.0.0.2         1.1.1.1         2.2.2.2         199  0x80000002 0x9e4\nSummary 0.0.0.2         2.2.2.2         2.2.2.2         199  0x80000002 0xee5f\nSummary 0.0.0.2         3.3.3.3         2.2.2.2         199  0x80000002 0x98e8\nSummary 0.0.0.2         4.4.4.4         2.2.2.2         199  0x80000002 0x7e63\nSummary 0.0.0.2         5.5.5.5         2.2.2.2         199  0x80000002 0x3c3d\nSummary 0.0.0.2         10.1.2.0        2.2.2.2         199  0x80000002 0x9252\nSummary 0.0.0.2         10.1.3.0        2.2.2.2         199  0x80000002 0x730c\nSummary 0.0.0.2         10.1.5.0        2.2.2.2         199  0x80000002 0x5d20\nSummary 0.0.0.2         10.2.4.0        2.2.2.2         199  0x80000002 0x7071\nSummary 0.0.0.2         10.3.4.0        2.2.2.2         199  0x80000002 0x502c\n-------------------------------------------------------------------------------\nNo. of LSAs: 13\n===============================================================================\n\n\n*A:R6# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n0.0.0.0/0                                     Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     101\n1.1.1.1/32                                    Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     200\n2.2.2.2/32                                    Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     100\n3.3.3.3/32                                    Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     300\n4.4.4.4/32                                    Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     200\n5.5.5.5/32                                    Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     300\n6.6.6.6/32                                    Local   Local     01h39m00s  0\n       system                                                       0\n10.1.2.0/24                                   Remote  OSPF      00h00m44s  10\n       10.2.6.2                                                     200\n10.1.3.0/24                                   Remote  OSPF      00h00m45s  10\n       10.2.6.2                                                     300\n10.1.5.0/24                                   Remote  OSPF      00h00m45s  10\n       10.2.6.2                                                     300\n10.2.4.0/24                                   Remote  OSPF      00h00m45s  10\n       10.2.6.2                                                     200\n10.2.6.0/24                                   Local   Local     01h38m38s  0\n       toR2                                                         0\n10.3.4.0/24                                   Remote  OSPF      00h00m45s  10\n       10.2.6.2                                                     300\n-------------------------------------------------------------------------------\nNo. of Routes: 13\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#ospf-totally-stub-area","title":"OSPF Totally Stub Area","text":"<p>ABR router participating in a Totally stub area blocks not only Type 4 ASBR Summary and Type 5 AS External LSA but also Type 3 Summary LSA. This drastically reduces LSDB and route tables on Area 2 routers. As of this moment we have Area 2 configured as s_tub_area, lets configure it to be_totally stub_. To make Area 2_totally stub_ we need to configure ABR (R2) with keyword <code>no summaries</code> option:</p> <pre><code>*A:R2# configure router ospf area 2 stub no summaries&lt;/pre&gt;\n\nTake a look at R6's LSDB and route table\n\n```*A:R6# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.2         2.2.2.2         2.2.2.2         482  0x80000004 0x9497\nRouter  0.0.0.2         6.6.6.6         6.6.6.6         518  0x80000004 0x1cd4\nSummary 0.0.0.2         0.0.0.0         2.2.2.2         222  0x80000007 0x4b05\n-------------------------------------------------------------------------------\nNo. of LSAs: 3\n===============================================================================\n\n\n\n\n*A:R6# show router route-table\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n0.0.0.0/0                                     Remote  OSPF      00h02m16s  10\n       10.2.6.2                                                     101\n6.6.6.6/32                                    Local   Local     02h11m29s  0\n       system                                                       0\n10.2.6.0/24                                   Local   Local     02h11m07s  0\n       toR2                                                         0\n-------------------------------------------------------------------------------\nNo. of Routes: 3\n</code></pre> <p>Now Area 2 router R6 has only 3 LSAs in its LSDB. All Type 3 Summary LSA for networks inside Area 0 were substituted by Summary default route from ABR (R2).</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#not-so-stubby-area-nssa","title":"Not So Stubby Area (NSSA)","text":"<p> There is a typo on the pic. No type4 lsa will be advertised in Area0 by R1. Only Type5</p> <p>Not so stubby areas are basically stub areas with an ASBR inside it. It inherits the same rule of blocking Type 4 and Type 5 LSA. But, in order to distribute external routes from ASBR another LSA this area leverages Type 7 LSA.</p> <p>We will configure Area 1 to be NSSA by adding the following commands to R1 (ABR):</p> <pre><code># prior to configuring NSSA on R5 lets check that we have active neighbor - R1\n\nA:R5# show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR1                             1.1.1.1         Full       1    0       35\n   0.0.0.1\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n===============================================================================\n\n#configuring Area 1 on R5 to NSSA state\n\nA:R5# configure router ospf area 1 nssa\n\n\n# Re-check neighbor status\n\n*A:R5# show router ospf neighbor\n\n===============================================================================\nOSPFv2 (0) all neighbors\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nNo. of Neighbors: 0\n===============================================================================\n</code></pre> <p>As soon we configured Area 1 on R5 to be NSSA we lost R1 as neighbor. This is the effect of a mismatched Area type in OSPF Hello messages between the two routers. Recall that R1 is maintaining Area 1 as a basic area, and Area 1 on R5 was reconfigured to NSSA.</p> <p>We will fix neighboring by moving R1's Area 1 to NSSA operation as well:</p> <pre><code>A:R1# configure router ospf area 1 nssa\n\n\n# Now check neighboring one more time\n\n*A:R1# show router ospf neighbor \"toR5\"\n\n===============================================================================\nOSPFv2 (0) neighbors for interface \"toR5\"\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\ntoR5                             5.5.5.5         Full       1    0       33\n   0.0.0.1\n-------------------------------------------------------------------------------\nNo. of Neighbors: 1\n</code></pre> <p>Now when R1 and R5 are neighbors again lets see what has changed in their LSDBs:</p> <p>R5 LSDB prior to NSSA config:</p> <pre><code>A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1911 0x80000004 0x50ea\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         1690 0x80000006 0x4bb4\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         2160 0x80000004 0x1939\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         2145 0x80000003 0xd812\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         1697 0x80000003 0xaa3c\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         669  0x80000004 0x6617\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         1529 0x80000003 0xc6a\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         997  0x80000003 0x9055\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         142  0x80000004 0x8360\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         1824 0x80000003 0x5a24\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         2092 0x80000003 0x4438\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         459  0x80000004 0x4c30\nSummary 0.0.0.1         192.168.3.0     1.1.1.1         1824 0x80000003 0x5039\nAS Ext  n/a             192.168.5.1     5.5.5.5         1369 0x80000003 0x5fec\nAS Ext  n/a             192.168.5.2     5.5.5.5         1491 0x80000003 0x55f5\nAS Ext  n/a             192.168.5.3     5.5.5.5         934  0x80000003 0x4bfe\n-------------------------------------------------------------------------------\nNo. of LSAs: 16\n===============================================================================\n</code></pre> <p>R5 LSDB after NSSA config:</p> <pre><code>*A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         539  0x80000002 0xa884\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         538  0x80000004 0x6d96\nSummary 0.0.0.1         1.1.1.1         1.1.1.1         538  0x80000002 0x3b1b\nSummary 0.0.0.1         2.2.2.2         1.1.1.1         538  0x80000002 0xf8f4\nSummary 0.0.0.1         3.3.3.3         1.1.1.1         538  0x80000002 0xca1f\nSummary 0.0.0.1         4.4.4.4         1.1.1.1         538  0x80000002 0x88f8\nSummary 0.0.0.1         6.6.6.6         1.1.1.1         538  0x80000002 0x2c4d\nSummary 0.0.0.1         10.1.2.0        1.1.1.1         538  0x80000002 0xb038\nSummary 0.0.0.1         10.1.3.0        1.1.1.1         538  0x80000002 0xa542\nSummary 0.0.0.1         10.2.4.0        1.1.1.1         538  0x80000002 0x7a07\nSummary 0.0.0.1         10.2.6.0        1.1.1.1         538  0x80000002 0x641b\nSummary 0.0.0.1         10.3.4.0        1.1.1.1         538  0x80000002 0x6e12\nSummary 0.0.0.1         192.168.3.0     1.1.1.1         538  0x80000002 0x701c\nNSSA    0.0.0.1         192.168.5.1     5.5.5.5         577  0x80000001 0xe74a\nNSSA    0.0.0.1         192.168.5.2     5.5.5.5         577  0x80000001 0xdd53\nNSSA    0.0.0.1         192.168.5.3     5.5.5.5         577  0x80000001 0xd35c\n-------------------------------------------------------------------------------\nNo. of LSAs: 16\n===============================================================================\n</code></pre> <p>As we see, all Type 5 AS External LSA were substituted by Type 7 NSSA LSA. And if we had any other external routes we wouldn't see them in R5 database, since NSSA areas can not contain External routes.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#totally-nssa","title":"Totally NSSA","text":"<p> There is a typo on the pic. No type4 lsa will be advertised in Area0 by R1. Only Type5</p> <p>As with totally stubby areas, NSSA could be configured in a way that no Type 3 Summary LSA will present in such area. Totally NSSA configuration adds two additional commands on ABR:</p> <p>On R1:</p> <pre><code>*A:R1&gt;config&gt;router&gt;ospf&gt;area# info\n----------------------------------------------\n                nssa\n                exit\n                interface \"toR5\"\n                    interface-type point-to-point\n                    no shutdown\n                exit\n----------------------------------------------\n\n# Configuring NSSA to be totally NSSA\n*A:R1&gt;config&gt;router&gt;ospf&gt;area# nssa no summaries&lt;/pre&gt;\n</code></pre> <p>On R5:</p> <pre><code># As a result - no Type 3 Summary LSA are present in R5's database\n\n*A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1305 0x80000003 0xa685\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         1550 0x80000005 0x6b97\nNSSA    0.0.0.1         192.168.5.1     5.5.5.5         1654 0x80000002 0xe54b\nNSSA    0.0.0.1         192.168.5.2     5.5.5.5         852  0x80000002 0xdb54\nNSSA    0.0.0.1         192.168.5.3     5.5.5.5         816  0x80000002 0xd15d\n-------------------------------------------------------------------------------\nNo. of LSAs: 5\n===============================================================================\n</code></pre> <p>ABR configured with Totally NSSA area filters Type 3 LSA as totally stubby area does. But notice one major difference - there is no default route injected by ABR. This is the cause of ping failure to any Area 0 address:</p> <pre><code>*A:R5# ping 4.4.4.4\nPING 4.4.4.4 56 data bytes\nNo route to destination. Address: 4.4.4.4, Router: Base\n</code></pre> <p>To resolve this issue you should add another command to NSSA context:</p> <pre><code>*A:R1&gt;config&gt;router&gt;ospf&gt;area# nssa originate-default-route\n</code></pre> <p>This will cause R1 to inject Type 3 Summary LSA in Area 1:</p> <pre><code>*A:R5# show router ospf database\n\n===============================================================================\nOSPFv2 (0) Link State Database (Type : All)\n===============================================================================\nType    Area Id         Link State Id   Adv Rtr Id      Age  Sequence   Cksum\n-------------------------------------------------------------------------------\nRouter  0.0.0.1         1.1.1.1         1.1.1.1         1652 0x80000003 0xa685\nRouter  0.0.0.1         5.5.5.5         5.5.5.5         264  0x80000006 0x6998\nSummary 0.0.0.1         0.0.0.0         1.1.1.1         53   0x80000001 0x75e4\nNSSA    0.0.0.1         192.168.5.1     5.5.5.5         172  0x80000003 0xe34c\nNSSA    0.0.0.1         192.168.5.2     5.5.5.5         1199 0x80000002 0xdb54\nNSSA    0.0.0.1         192.168.5.3     5.5.5.5         1163 0x80000002 0xd15d\n-------------------------------------------------------------------------------\nNo. of LSAs: 6\n===============================================================================\n\n\n*A:R5# ping 4.4.4.4\nPING 4.4.4.4 56 data bytes\n64 bytes from 4.4.4.4: icmp_seq=1 ttl=62 time=19.6ms.\n</code></pre>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-sros-ospf-configuration-tutorial/#debugging-ospf-adjacency-issues","title":"Debugging OSPF adjacency issues","text":"<p>For two OSPF routers to become neighbors several parameters should be matched. And most of them - are the parameters communicated via OSPF Hello messages. If you do not see a neighbor on the other side there is a good chance that one of these parameters mismatches.</p> <p>When you need to investigate the reason casing a neighboring relationships to break leverage the debug commands available.</p> <p>Consider the following example where R1 lost its neighbor R5 in Area 1:</p> <pre><code>*A:R1# show router ospf neighbor \"toR5\"\n\n===============================================================================\nOSPFv2 (0) neighbors for interface \"toR5\"\n===============================================================================\nInterface-Name                   Rtr Id          State      Pri  RetxQ   TTL\n   Area-Id\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nNo. of Neighbors: 0\n===============================================================================\n</code></pre> <p>To see what is going wrong, lets create a logging instance and capture Hello packets to analyze their content:</p> <pre><code># Creating log #55\n*A:R1# configure log log-id 55\n\n# Configuring capture log messages from debug to memory \n*A:R1&gt;config&gt;log&gt;log-id$ from debug-trace\n*A:R1&gt;config&gt;log&gt;log-id$ to memory\n\n\n# Specifying what \"debug\" should process. We are hunting OSPF Hello packets\n# adding \"detail\" to see more verbose output\n\n*A:R1# debug router ospf packet hello detail\n</code></pre> <p>Now that we have log and debug objects configured, we could drill down to the contents of Hello messages:</p> <pre><code>*A:R1# show log log-id 55\n\n===============================================================================\nEvent Log 55\n===============================================================================\nDescription : (Not Specified)\nMemory Log contents  [size=100   next event=22  (not wrapped)]\n\n20 2015/01/01 03:11:31.28 UTC MINOR: DEBUG #2001 Base OSPFv2\n\"OSPFv2: PKT DROPPED\nhello interval mismatch\"\n\n19 2015/01/01 03:11:31.28 UTC MINOR: DEBUG #2001 Base OSPFv2\n\"OSPFv2: PKT\n\n&gt;&gt; Incoming OSPF packet on I/F toR5 area 0.0.0.1\nOSPF Version      : 2\nRouter Id         : 5.5.5.5\nArea Id           : 0.0.0.1\nChecksum          : ec98\nAuth Type         : Null\nAuth Key          : 00 00 00 00 00 00 00 00\nPacket Type       : HELLO\nPacket Length     : 44\n\nNetwork Mask      : 255.255.255.0\nHello Interval    : 5\nOptions           : 08 -----N----\nRtr Priority      : 1\nDead Interval     : 40\nDesignated Router : 0.0.0.0\nBackup Router     : 0.0.0.0\n</code></pre> <p>Now the problem is clear - incoming OSPF packet came in with Hello timer set to 5sec. And R1 tells us that this value mismatches its configured Hello timer value.</p> <p>This debug technique should indicate every discrepancy in OSPF values that should match, be it authentication mismatch or Area ID mismatch.</p> <p>And that is all for the moment.</p>","tags":["sr os","nokia","ospf"]},{"location":"2015/nokia-alcatel-lucent-configuring-packet-ip-filters/","title":"Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters","text":"<p>Packet\u00a0filters (or in Cisco terminology Access Control Lists, aka ACL) are one of the most used tools in a network engineer's tool set. Blocking telnet/ssh access, restricting specific traffic flows, implementing policy-based routing or NATing - all of these tasks use IP filter's capabilities.</p> <p>In this example I'll show you how to configure a basic SSH-blocking IP filter on a Nokia (Alcatel-Lucent) SROS running\u00a0<code>TiMOS-B-12.0.R8</code>.</p> <p>According to the topology provided we will block SSH access to R1's system IP. This particular task could be done in various ways, but we will configure IP filter on R2 (applied to R2's interface\u00a0<code>to_R4</code>\u00a0in the incoming direction).</p> <p></p> <p>And the rule we will configure on R2 will be as follows:</p> <ul> <li>If R2\u00a0receives a packet with a TCP destination port == 22 on interface\u00a0<code>to_R4</code>\u00a0it\u00a0must\u00a0drop it.</li> </ul> <p>Lets begin with testing ssh access before any configuration is done:</p> <pre><code>A:R4# ssh 1.1.1.1\nThe authenticity of host '1.1.1.1 (1.1.1.1)' can't be established.\nRSA key fingerprint is 9c:97:50:00:b0:f7:45:6f:9e:14:9a:06:11:ba:c6:e8.\nAre you sure you want to continue connecting (yes/no)? yes\n\nTiMOS-B-12.0.R8 both/i386 ALCATEL SR 7750 Copyright (c) 2000-2015 Alcatel-Lucent.\nAll rights reserved. All use subject to applicable license agreements.\nBuilt on Fri Jan 9 09:55:30 PST 2015 by builder in /rel12.0/b1/R8/panos/main\n\nadmin@1.1.1.1's password:\n\nA:R1# logout\nConnection to 1.1.1.1 closed.\n</code></pre> <p>Working, as expected, good. Now lets block SSH access via IP filter configuration on R2:</p> <pre><code>## Creating ip-filter \n*A:R2# configure filter ip-filter 100 create\n\n## Adding description (optional)\n*A:R2&gt;config&gt;filter&gt;ip-filter$ description \"block ssh to 1.1.1.1/32\"\n\n## Adding name to a filter (optional)\n*A:R2&gt;config&gt;filter&gt;ip-filter$ filter-name \"block_ssh_to_R1\"\n\n## Creating filter entry \n*A:R2&gt;config&gt;filter&gt;ip-filter$ entry 10 create\n\n## Specifying match statement for TCP packets, since SSH uses TCP\n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry$ match protocol \"tcp\"\n\n## In match context specifying the SSH port number \n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry&gt;match$ dst-port eq 22\n\n## optionally adding another match rule - Destination IP for R1\n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry&gt;match$ dst-ip 1.1.1.1/32\n\n## Leaving \"match\" context and adding DROP action to this filter's entry\n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry&gt;match$ back\n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry$ action drop\n\n## Moving one step back to filter's context and adding default action FORWARD, since implicitly it is DROP.\n*A:R2&gt;config&gt;filter&gt;ip-filter&gt;entry$ back\n*A:R2&gt;config&gt;filter&gt;ip-filter$ default-action forward\n\n## Lets see the whole filter config at once\n*A:R2# configure filter ip-filter 100\n*A:R2&gt;config&gt;filter&gt;ip-filter# info\n----------------------------------------------\n            filter-name \"block_ssh_to_R1\"\n            default-action forward\n            description \"block ssh to 1.1.1.1/32\"\n            entry 10 create\n                match protocol tcp\n                    dst-ip 1.1.1.1/32\n                    dst-port eq 22\n                exit\n                action drop\n            exit\n----------------------------------------------\n</code></pre> <p>We created a simple IP filter, but it was not applied to any interface. Lets do this:</p> <pre><code>*A:R2# configure router interface \"toR4\"\n*A:R2&gt;config&gt;router&gt;if# ingress filter ip\nip \n \"block_ssh_to_R1\"   100 ## you can refer to ip filter by its name or id\n\n*A:R2&gt;config&gt;router&gt;if# ingress filter ip \"block_ssh_to_R1\"\n\n## make sure that ip filter applied correctly\n*A:R2&gt;config&gt;router&gt;if# info\n----------------------------------------------\n            address 10.2.4.2/24\n            port 1/1/3\n            ingress\n                filter ip 100\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p></p> <p>Done, the filter has been applied to the appropriate interface and now should be running properly. Lets verify it by making SSH login attempt once again:</p> <pre><code>A:R4# ssh 1.1.1.1\nConnect to address 1.1.1.1 failed  ## Our filter is working as expected\n</code></pre> <p>You\u00a0use <code>show filter</code>\u00a0 command to see the details of the newly created filter along with a number of packets matched by this filter:</p> <pre><code>*A:R2# show filter ip 100\n\n===============================================================================\nIP Filter\n===============================================================================\nFilter Id    : 100                              Applied        : Yes\nScope        : Template                         Def. Action    : Forward\nRadius Ins Pt: n/a\nCrCtl. Ins Pt: n/a\nRadSh. Ins Pt: n/a\nEntries      : 1\nDescription  : block ssh to 1.1.1.1/32\n-------------------------------------------------------------------------------\nFilter Match Criteria : IP\n-------------------------------------------------------------------------------\nEntry        : 10\nDescription  : (Not Specified)\nLog Id       : n/a\nSrc. IP      : 0.0.0.0/0\nSrc. Port    : n/a\nDest. IP     : 1.1.1.1/32\nDest. Port   : eq 22\nProtocol     : 6                                Dscp           : Undefined\nICMP Type    : Undefined                        ICMP Code      : Undefined\nFragment     : Off                              Src Route Opt  : Off\nSampling     : Off                              Int. Sampling  : On\nIP-Option    : 0/0                              Multiple Option: Off\nTCP-syn      : Off                              TCP-ack        : Off\nOption-pres  : Off\nMatch action : Drop\nIng. Matches : 2 pkts (156 bytes)     ## See matched SSH packets\nEgr. Matches : 0 pkts\n\n===============================================================================\n</code></pre>","tags":["sr os","nokia"]},{"location":"2015/nokia-alcatel-lucent-configuring-packet-ip-filters/#match-list-and-port-list","title":"Match-list and Port list","text":"<p>In the example above we used one ip address and one port to create our filter, but what if we need to\u00a0match on the whole\u00a0range of IP addresses and ports? You need to use match-list and port-list in this case:</p> <pre><code>*A:R1&gt;config&gt;filter# info\n----------------------------------------------\n        match-list\n            ip-prefix-list \"3_routes\" create\n                prefix 10.10.10.10/32\n                prefix 20.20.20.20/32\n                prefix 30.30.30.30/32\n            exit\n            port-list \"allowed_ports\" create\n                port 22\n                port 80\n            exit\n        exit\n        ip-filter 10 create\n            default-action forward\n            entry 10 create\n                match protocol tcp\n                    dst-port port-list \"allowed_ports\"\n                    src-ip ip-prefix-list \"3_routes\"\n                exit\n                action drop\n            exit\n        exit\n----------------------------------------------\n</code></pre> <p>And that's all for this quick IP filter tutorial.</p>","tags":["sr os","nokia"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/","title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP","text":"<p>There is no way I would leave you without covering configuration steps for one of the most versatile, scalable and robust internet protocols also known as BGP. And here it is - BGP configuration guide for Nokia (Alcatel-Lucent) Service Routers.</p> <p>As with the OSPF configuration tutorial I will cover the configuration process for various BGP scenarios along with the verification and troubleshooting steps bundled with colorful figures, detailed code snippets and useful remarks.</p> <p>BGP is so huge that I had no other option but to write about it in several parts:</p> <ul> <li>Part 1 - basic eBGP and iBGP configuration</li> <li>Part 2 - BGP policies. Community</li> </ul> <p>Part 1 is dedicated to basic eBGP/iBGP configuration. We will practice with common BGP configuration procedures at first, then learn how to export routes into BGP process and prevent unnecessary route reflection by means of <code>split-horizon</code> over eBGP links.</p> <p>Next we go over iBGP configuration to spread the eBGP learned routes across the Autonomous Systems. I will explain the necessity of having a full-mesh iBGP topology and the use of the <code>next-hop-self</code> command for iBGP peers.</p> <p>It's a perfect time to configure some BGP, right?</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#common-bgp-configuration-steps","title":"Common BGP configuration steps","text":"<p>Despite what type of BGP (Internal or External) you are going to configure there are some basic steps we are about to discuss. Address planning, IGP configuration, router-id selection, autonomous-system number setting, peer groups and neighbor configuration - all of these task are common to each and every BGP configuration routine.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#igp-and-addressing","title":"IGP and addressing","text":"<p>BGP completely relies on IGP (or static routes) when resolving nexthop address received in BGP updates from its peers. This means that prior to BGP configuration you should have IGP up and running. During this session I will refer to this base topology:</p> <p></p> <p>A few words about the address plan and key pieces of this diagram: a BGP peering will take place between the two Autonomous Systems (hereinafter AS) 65510 and 65520.</p> <p>AS 65510 utilizes <code>10.10.0.0/16</code> network for local link addresses, system interfaces of its routers and customers-assigned networks, whereas AS 65520 uses <code>10.20.0.0/16</code> for the same purposes. Address plan details could be found at the Legend section of the \"base topology\" figure.</p> <p>We will be working with the two customers networks:</p> <ul> <li><code>R5_Customer - 10.10.55.0/24</code> in AS 65510</li> <li><code>R3_Ext_Customer - 172.16.33.0/24</code> in AS 65520</li> </ul> <p>As to Interior Gateway Protocol - I chose IS-IS, though you can choose an IGP protocol of your choice - it wont be any different. IS-IS configuration for this tutorial is super straightforward, system and network interfaces are participating in IS-IS process within the relevant ASes (except interfaces between R1-R3, R2-R4 as they are connecting different AS's and we will run BGP there). Inter-router links are all point-to-point type.</p> <p>IS-IS configuration section for reference:</p> <p>R1 (AS 65510):</p> <pre><code>*A:R1&gt;config&gt;router&gt;isis# info\n----------------------------------------------\n        level-capability level-1\n        area-id 10.10\n        reference-bandwidth 100000000\n        level 1\n            wide-metrics-only\n        exit\n        level 2\n            wide-metrics-only\n        exit\n        interface \"system\"\n            no shutdown\n        exit\n        interface \"toR2\"\n            interface-type point-to-point\n            no shutdown\n        exit\n        interface \"toR5\"\n            interface-type point-to-point\n            no shutdown\n        exit\n        no shutdown\n----------------------------------------------\n\n## IS-IS database for 65510 consists of LSP from every router in this AS\n*A:R1&gt;config&gt;router&gt;isis# show router isis database\n\n===============================================================================\nRouter Base ISIS Instance 0 Database\n===============================================================================\nLSP ID                                  Sequence  Checksum Lifetime Attributes\n-------------------------------------------------------------------------------\n\nDisplaying Level 1 database\n-------------------------------------------------------------------------------\nR1.00-00                                0xd       0x7740   1182     L1\nR2.00-00                                0xb       0x7ad1   812      L1\nR5.00-00                                0xc       0x8dfd   817      L1\nR6.00-00                                0xb       0xe8a5   842      L1\nLevel (1) LSP Count : 4\n\nDisplaying Level 2 database\n-------------------------------------------------------------------------------\nLevel (2) LSP Count : 0\n===============================================================================\n\n\n## Check if we have a route to every router's system address within AS\n*A:R1# show router route-table  10.10.10.0/24 longer\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.10.1/32                                 Local   Local     02h12m52s  0\n       system                                                       0\n10.10.10.2/32                                 Remote  ISIS      02h12m24s  15\n       10.10.99.1                                                   100\n10.10.10.5/32                                 Remote  ISIS      02h12m25s  15\n       10.10.99.5                                                   100\n10.10.10.6/32                                 Remote  ISIS      02h12m22s  15\n       10.10.99.1                                                   200\n-------------------------------------------------------------------------------\nNo. of Routes: 4\n</code></pre> <p>R3 (AS 65520):</p> <pre><code>A:R3&gt;config&gt;router&gt;isis# info\n----------------------------------------------\n        level-capability level-1\n        area-id 20.20\n        reference-bandwidth 100000000\n        level 1\n            wide-metrics-only\n        exit\n        level 2\n            wide-metrics-only\n        exit\n        interface \"system\"\n            no shutdown\n        exit\n        interface \"toR4\"\n            interface-type point-to-point\n            no shutdown\n        exit\n        no shutdown\n----------------------------------------------\n\n## AS 65520 consists of two routers R3 and R4, that is why we see only two LSP here\nA:R3&gt;config&gt;router&gt;isis# show router isis database\n\n===============================================================================\nRouter Base ISIS Instance 0 Database\n===============================================================================\nLSP ID                                  Sequence  Checksum Lifetime Attributes\n-------------------------------------------------------------------------------\n\nDisplaying Level 1 database\n-------------------------------------------------------------------------------\nR3.00-00                                0xc       0x8dc2   1160     L1\nR4.00-00                                0xa       0xfe4e   639      L1\nLevel (1) LSP Count : 2\n\nDisplaying Level 2 database\n-------------------------------------------------------------------------------\nLevel (2) LSP Count : 0\n===============================================================================\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#configuring-router-id-and-autonomous-system-number","title":"Configuring Router ID and Autonomous System number","text":"<p>Once IGP is configured its time to configure a common entity for almost every routing protocol - Router ID. For BGP there is more than one place to configure the Router ID. Here is the Router ID selection process sorted by a priority:</p> <ol> <li>Router ID is configured in BGP global context with the command <code>configure router bgp router-id &lt;ip-address&gt;</code></li> <li>Router ID is configured globally for a router with the command <code>configure router router-id &lt;ip-address&gt;</code></li> <li>Router ID is inherited from <code>system</code> IP-address.</li> </ol> <p>Important thing to remember is that if no <code>router-id</code> nor <code>system</code> interface is configured - BGP will not start. Since we have the <code>system</code> interface configured for every router we don't need to specify the <code>router-id</code> explicitly.</p> <p>AS number can be configured either globally for a router <code>configure router autonomous-system &lt;autonomous-system&gt;</code> or for a specified peer group with the <code>local-as</code> command. We will stick to the first option and configure our AS numbers globally for a router:</p> <pre><code>A:R3# configure router autonomous-system 65520\n\n*A:R3&gt;config&gt;router# info\n\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.20.20.3/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.0.99.1/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR4\"\n            address 10.20.99.0/31\n            port 1/1/2\n            no shutdown\n        exit\n        autonomous-system 65520\n#--------------------------------------------------\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#starting-ebgp","title":"Starting eBGP","text":"<p>Common parameters are now configured and we can jump to eBGP peers configuration. Recall that we have two routers within AS 65510 (R1 and R2) which will have eBGP peering sessions with R3 and R3 within AS 65520 accordingly. Thus we should configure eBGP peering between the pairs R1-R3, R2-R4.</p> <p></p> <p>Nokia BGP configuration policy requires you to configure at least one peer group to make BGP peering happen. Peer groups are logical containers for BGP peers that share common parameters. Every BGP neighbor you add should find its place in any of the BGP peer groups, in other words - peer groups are mandatory in SROS.</p> <p>I will guide you through basic eBGP configuration between R1 and R3. R2 and R4 configuration will be just the same.</p> <p>R1:</p> <pre><code>## entering BGP configuration context\n*A:R1# configure router bgp\n\n## creating group eBGP\n*A:R1&gt;config&gt;router&gt;bgp$ group \"eBGP\"\n\n## specifying AS Number for AS we would want to peer to (which is 65520)\n## for eBGP peer-as should differ from local AS\n## for iBGP peer-as should match local AS Number\n*A:R1&gt;config&gt;router&gt;bgp&gt;group$ peer-as 65520\n\n## setting IP address of the remote router in AS 65520\n*A:R1&gt;config&gt;router&gt;bgp&gt;group$ neighbor 10.0.99.1\n\n## specify local-address for eBGP peer\n*A:R1&gt;config&gt;router&gt;bgp&gt;group&gt;neighbor# local-address 10.0.99.0\n\n## Viewing resulting configuration\n*A:R1&gt;config&gt;router&gt;bgp&gt;group&gt;neighbor$ back\n*A:R1&gt;config&gt;router&gt;bgp&gt;group$ back\n*A:R1&gt;config&gt;router&gt;bgp$ info\n----------------------------------------------\n            group \"eBGP\"\n                peer-as 65520\n                neighbor 10.0.99.1\n                    local-address 10.0.99.0\n                exit\n            exit\n            no shutdown\n----------------------------------------------```\n</code></pre> <p>R3:</p> <pre><code>## all the comments are the same as for R1\n\n*A:R3# configure router bgp\n*A:R3&gt;config&gt;router&gt;bgp$ group \"eBGP\"\n*A:R3&gt;config&gt;router&gt;bgp&gt;group$ peer-as 65510\n*A:R3&gt;config&gt;router&gt;bgp&gt;group$ neighbor 10.0.99.0\nA:R3&gt;config&gt;router&gt;bgp&gt;group&gt;neighbor$ local-address 10.0.99.1\n*A:R3&gt;config&gt;router&gt;bgp&gt;group&gt;neighbor$ back\n*A:R3&gt;config&gt;router&gt;bgp&gt;group$ back\n*A:R3&gt;config&gt;router&gt;bgp$ info\n----------------------------------------------\n            group \"eBGP\"\n                peer-as 65510\n                neighbor 10.0.99.0\n                    local-address 10.0.99.1\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>As simple as that, eBGP in its simplest form has been configured in 5 lines. Pay additional attention to <code>local-address</code> command. It is a common practice to specify a link IP address for an eBGP peer, otherwise SROS router will try to establish TCP session from its system IP address and will fail.</p> <p>To verify the established eBGP peering use the <code>show router bgp summary</code> command (another way is to use <code>show router bgp neighbor &lt;neighbor-ip-address&gt;</code>)</p> <pre><code>A:R1# show router bgp summary\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\nBGP Admin State         : Up          BGP Oper State              : Up\nTotal Peer Groups       : 1           Total Peers                 : 1\nTotal BGP Paths         : 7           Total Path Memory           : 1260\nTotal IPv4 Remote Rts   : 0           Total IPv4 Rem. Active Rts  : 0\nTotal McIPv4 Remote Rts : 0           Total McIPv4 Rem. Active Rts: 0\nTotal McIPv6 Remote Rts : 0           Total McIPv6 Rem. Active Rts: 0\nTotal IPv6 Remote Rts   : 0           Total IPv6 Rem. Active Rts  : 0\nTotal IPv4 Backup Rts   : 0           Total IPv6 Backup Rts       : 0\n\nTotal Supressed Rts     : 0           Total Hist. Rts             : 0\nTotal Decay Rts         : 0\n\nTotal VPN Peer Groups   : 0           Total VPN Peers             : 0\nTotal VPN Local Rts     : 0\nTotal VPN-IPv4 Rem. Rts : 0           Total VPN-IPv4 Rem. Act. Rts: 0\nTotal VPN-IPv6 Rem. Rts : 0           Total VPN-IPv6 Rem. Act. Rts: 0\nTotal VPN-IPv4 Bkup Rts : 0           Total VPN-IPv6 Bkup Rts     : 0\n\nTotal VPN Supp. Rts     : 0           Total VPN Hist. Rts         : 0\nTotal VPN Decay Rts     : 0\n\nTotal L2-VPN Rem. Rts   : 0           Total L2VPN Rem. Act. Rts   : 0\nTotal MVPN-IPv4 Rem Rts : 0           Total MVPN-IPv4 Rem Act Rts : 0\nTotal MDT-SAFI Rem Rts  : 0           Total MDT-SAFI Rem Act Rts  : 0\nTotal MSPW Rem Rts      : 0           Total MSPW Rem Act Rts      : 0\nTotal RouteTgt Rem Rts  : 0           Total RouteTgt Rem Act Rts  : 0\nTotal McVpnIPv4 Rem Rts : 0           Total McVpnIPv4 Rem Act Rts : 0\nTotal MVPN-IPv6 Rem Rts : 0           Total MVPN-IPv6 Rem Act Rts : 0\nTotal EVPN Rem Rts      : 0           Total EVPN Rem Act Rts      : 0\nTotal FlowIpv4 Rem Rts  : 0           Total FlowIpv4 Rem Act Rts  : 0\nTotal FlowIpv6 Rem Rts  : 0           Total FlowIpv6 Rem Act Rts  : 0\n\n===============================================================================\nBGP Summary\n===============================================================================\nNeighbor\n                   AS PktRcvd InQ  Up/Down   State|Rcv/Act/Sent (Addr Family)\n                      PktSent OutQ\n-------------------------------------------------------------------------------\n10.0.99.1\n                65520      85    0 00h41m58s 0/0/0 (IPv4)\n                           85    0\n-------------------------------------------------------------------------------\n</code></pre> <p>At the end of this show command you can find information about the BGP neighbors and their states. With <code>show router bgp summary</code> command issued on R1 we see <code>10.0.99.1</code> as a neighbor (which is R3's interface IP address).</p> <p>If a session is established then you see the session uptime and number of routes received/active/sent. If the session has not been established yet an operator will see the current BGP state instead of the exchanged routes counters.</p> <p>String <code>0/0/0 (IPv4)</code> is an indicator that the peering has been successfully established and R1 router received and sent exactly zero IPv4 routes. Zero counters are expected, since we just started the eBGP session but did not export any routes to it. It is very important to remember that by default SROS does not add any non-BGP routes to the BGP process.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#exporting-routes-to-bgp","title":"Exporting routes to BGP","text":"<p>No fun at all to play with zero NLRI (network layer reachability information). Lets fix this and add some routes to our eBGP process. We have a good candidate for this in our address plan - <code>R5_Customer - 10.10.55.0/24</code> network. To emulate this customer's network we must add a loopback interface to R5 and announce this network via IGP:</p> <pre><code>A:R5# configure router interface R5_Customer_loopback\n*A:R5&gt;config&gt;router&gt;if$ loopback\n*A:R5&gt;config&gt;router&gt;if$ address 10.10.55.1/24\n\n## adding artificial R5_Customer network to IS-IS\n*A:R5# configure router isis interface \"R5_Customer_loopback\"\n</code></pre> <p>After we created a network for our customer and announced it via IS-IS we should check if R1 could see it in its routing table:</p> <pre><code>A:R1# show router route-table 10.10.55.0/24\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.55.0/24                                 Remote  ISIS      00h00m03s  15\n       10.10.99.5                                                   100\n-------------------------------------------------------------------------------\nNo. of Routes: 1\n</code></pre> <p>Now R1 is aware of <code>10.10.55.0/24</code>, but this is not sufficient for the BGP process on R1 to advertise this prefix to R3. We should explicitly tell BGP process running on R1 to take <code>10.10.55.0/24</code> prefix into consideration and the way to do so is to create a policy-statement and export it to BGP.</p> <p>Step-by-step plan goes like this:</p> <ul> <li>create a prefix-list to match a desired prefix</li> <li>create a policy-statement accepting prefixes from the prefix-list and delivering it to the BGP process</li> <li>add customer's network to BGP via <code>export &lt;policy_statement&gt;</code> command under peer group context.</li> </ul> <p>Lets implement this plan:</p> <pre><code>A:R1# configure router policy-options\n\n## entering to policy options edit mode\nA:R1&gt;config&gt;router&gt;policy-options# begin\n\n## creating prefix-list for R5_Customer network\n*A:R1&gt;config&gt;router&gt;policy-options# prefix-list \"R5_Customer_pfx\"\n*A:R1&gt;config&gt;router&gt;policy-options&gt;prefix-list$ prefix 10.10.55.0/24 exact\n*A:R1&gt;config&gt;router&gt;policy-options&gt;prefix-list$ back\n\n## creating policy statement\n*A:R1&gt;config&gt;router&gt;policy-options# policy-statement \"R5_Customer_export\"\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement$ entry 10\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ from prefix-list \"R5_Customer_pfx\"\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ to protocol bgp\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ action accept\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry&gt;action$ back\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ back\n*A:R1&gt;config&gt;router&gt;policy-options&gt;policy-statement$ back\n\n## applying changes to policy options\n*A:R1&gt;config&gt;router&gt;policy-options# commit\n\n## reviewing configuration\n*A:R1&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"R5_Customer_pfx\"\n                prefix 10.10.55.0/24 exact\n            exit\n            policy-statement \"R5_Customer_export\"\n                entry 10\n                    from\n                        prefix-list \"R5_Customer_pfx\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n</code></pre> <p>After we have the policy statement configured we can reference it in the <code>export</code> command under the eBGP peer group section of R1:</p> <pre><code>*A:R1# configure router bgp\n*A:R1&gt;config&gt;router&gt;bgp# group \"eBGP\"\n\n## hitting TAB after \"export\" keyword displays all available policy statements\n*A:R1&gt;config&gt;router&gt;bgp&gt;group# export\n&lt;policy-name&gt; [&lt;policy-name&gt;...(upto 15 max)]\n \"R5_Customer_export\"\n\n*A:R1&gt;config&gt;router&gt;bgp&gt;group# export \"R5_Customer_export\"\n</code></pre> <p>Export command triggers R1 to send a BGP Update message with the NLRI for <code>R5_Customer</code> network to R3 (note that I made the same configuration on R2-R4 pair, so the figure below shows R2's BGP Update message as well):</p> <p></p> <p>Take a look at <code>show router bgp summary</code> once again on R1:</p> <pre><code>A:R1# show router bgp summary\n\n## output omitted ##\n\n===============================================================================\nBGP Summary\n===============================================================================\nNeighbor\n                   AS PktRcvd InQ  Up/Down   State|Rcv/Act/Sent (Addr Family)\n                      PktSent OutQ\n-------------------------------------------------------------------------------\n10.0.99.1\n                65520     265    0 02h13m28s 1/0/1 (IPv4)\n                          266    0\n-------------------------------------------------------------------------------\n</code></pre> <p>Nice, we have sent and received one IPv4 NLRI. It is surprising to see one prefix received considering that R3 does not have any exported networks, but we will deal with this later. Now lets check R3 to see if it has <code>R5_Customer</code> network in its routing table?</p> <p>BGP mechanics I have to pause for a moment and share with you the BGP route processing diagram. It helps to understand what BGP databases are there on Nokia SROS and what path it takes through the BGP route machinery.  credits: Alcatel-Lucent Service Routing Architect (SRA) Self-Study guide, WILEY I will refer to these databases from now on as BGP RIB In, BGP Local-RIB and BGP RIB Out.</p> <p>To see what routes are in BGP RIB In and BGP Local Routing Information Base (BGP Local-RIB) use the <code>show router bgp routes</code> command:</p> <pre><code>A:R3# show router bgp routes\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  10.10.55.0/24                                      None        100\n      10.0.99.0                                          None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>Perfect, <code>10.10.55.0/24</code> network made its way into BGP Local-RIB</p> <ul> <li><code>*</code> flag means it passed validation checks</li> <li><code>u</code> tells us that this route is used and is present in the R3 routing table</li> </ul> <pre><code>A:R3# show router route-table 10.10.55.0\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.55.0/24                                 Remote  BGP       00h12m32s  170\n       10.0.99.0                                                    0\n-------------------------------------------------------------------------------\nNo. of Routes: 1\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#alcatel-lucent-ebgp-reflecting-routes-issue","title":"Alcatel-Lucent eBGP reflecting routes issue","text":"<p>Now it is time to deal with that rogue route received by R1 from its neighbor R3.</p> <pre><code>*A:R1# show router bgp routes\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     10.10.55.0/24                                      None        None\n      10.0.99.1                                          None        -\n      65520 65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>AS 65510 local prefix <code>10.10.55.0/24</code> is in R1's own BGP Local-RIB, but why do wee see it there? Well, because R3 sent it to R1. But why the hell R3 sent back the <code>10.10.55.0/24</code> prefix to R1 given that it came from it?</p> <p>Lets investigate NLRI propagation for this prefix:</p> <ol> <li>R1 sends BGP Update message to R3 with the <code>10.10.55.0/24</code> prefix and AS Path 65510.</li> <li>R3 receives this update, stores it in BGP RIB In database and checks if this NLRI is valid (nexthop resolvable, no AS Path loop) in order to put this prefix in R3's BGP Local-RIB.</li> <li>All the checks passed and R3 sends NLRI <code>10.10.55.0/24</code> back to R1 since this is not prohibited by RFC 4271 A Border Gateway Protocol 4 (BGP-4) appending AS Path with its AS number.</li> <li>R1 receives this update and stores it in its BGP RIB In but this route will never make its way to BGP Local-RIB due to AS Loop error.</li> </ol> <p>Based on the output from the <code>show router bgp routes</code> command and the fact that there is only one flag <code>i</code> associated with the  <code>10.10.55.0/24</code> prefix we can conclude that this prefix was not delivered to Route Table Manager (RTM) and left alone in the BGP RIB In of R1. But why is that? If we issue super-useful command <code>show router bgp routes &lt;prefix&gt; hunt</code> for this prefix we could see what happened:</p> <pre><code>*A:R1# show router bgp routes 10.10.55.0/24 hunt\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.1\nPath Id        : None\nFrom           : 10.0.99.1\nRes. Nexthop   : 10.0.99.1\nLocal Pref.    : None                   Interface Name : toR3\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : No Community Members\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.3\nFwd Class      : None                   Priority       : None\nFlags          : Invalid  IGP  AS-Loop\nRoute Source   : External\nAS-Path        : 65520 65510\nRoute Tag      : 0\nNeighbor-AS    : 65520\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.0\nPath Id        : None\nTo             : 10.0.99.1\nRes. Nexthop   : n/a\nLocal Pref.    : n/a                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : 100\nAIGP Metric    : None\nConnector      : None\nCommunity      : No Community Members\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.3\nOrigin         : IGP\nAS-Path        : 65510\nRoute Tag      : 0\nNeighbor-AS    : 65510\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRoutes : 2\n===============================================================================\n</code></pre> <p>RIB In Entries section of this output and especially the lines \"Flags\" and \"AS Path\" answer the question why R1 will not pass <code>10.10.55.0/24</code> to the BGP Local-RIB \u2014 there is an AS Path Loop for this NLRI. And this is the reason why this NLRI is in BGP RIB In only.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#ebgp-split-horizon","title":"eBGP split-horizon","text":"<p>For those of you who came from Cisco or Juniper camps its quite strange to see that R3 send the same prefix back to R1. I agree, its hard to find a case when it would be desired to receive previously announced prefix over eBGP. To mitigate this round-trip exchange you can use the <code>split-horizon</code> command on R3. This split-horizon has nothing to do with standard iBGP split-horizon behavior (which is \"do not advertise prefixes received from one iBGP peer to the other iBPG peers\").</p> <pre><code>## check that 10.10.55.0/24 prefix is announcing back to R1\n*A:R3# show router bgp routes 10.10.55.0/24 hunt\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\n&lt; output omitted &gt;\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.1\nPath Id        : None\nTo             : 10.0.99.0\nRes. Nexthop   : n/a\nLocal Pref.    : n/a                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : No Community Members\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.1\nOrigin         : IGP\nAS-Path        : 65520 65510\nRoute Tag      : 0\nNeighbor-AS    : 65520\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRoutes : 2\n===============================================================================\n\n\n## issue split-horizon command to disable this useless behavior\nA:R3# configure router bgp group \"eBGP\"\nA:R3&gt;config&gt;router&gt;bgp&gt;group# split-horizon \n\n\n## there is now no routes in RIB Out Entries section\n*A:R3# show router bgp routes 10.10.55.0/24 hunt\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.0\nPath Id        : None\nFrom           : 10.0.99.0\nRes. Nexthop   : 10.0.99.0\n\n&lt;output omitted&gt;\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n\n\n\n## and R1 now has no routes in its RIB In and Local-RIB databases.\nA:R1# show router bgp routes\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#ebgp-resulting-configuration","title":"eBGP resulting configuration","text":"<p>Check the resulting eBGP config that you would have on your routers at this moment:</p> <p>R1:</p> <pre><code>A:R1&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            group \"eBGP\"\n                export \"R5_Customer_export\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.1\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n\nA:R1&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"R5_Customer_pfx\"\n                prefix 10.10.55.0/24 exact\n            exit\n            policy-statement \"R5_Customer_export\"\n                entry 10\n                    from\n                        prefix-list \"R5_Customer_pfx\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n\nA:R1# show router bgp neighbor 10.0.99.1 advertised-routes\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     10.10.55.0/24                                      n/a         100\n      10.0.99.0                                          None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n\n\nA:R1# show router bgp neighbor 10.0.99.1 received-routes\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n</code></pre> <p>R2:</p> <pre><code>A:R2&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            group \"eBGP\"\n                export \"R5_Customer_export\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.3\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n\nA:R2&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"R5_Customer_pfx\"\n                prefix 10.10.55.0/24 exact\n            exit\n            policy-statement \"R5_Customer_export\"\n                entry 10\n                    from\n                        prefix-list \"R5_Customer_pfx\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n\n\n\nA:R2# show router bgp neighbor 10.0.99.3 advertised-routes\n===============================================================================\n BGP Router ID:10.10.10.2       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     10.10.55.0/24                                      n/a         200\n      10.0.99.2                                          None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n\n\n\nA:R2# show router bgp neighbor 10.0.99.3 received-routes\n===============================================================================\n BGP Router ID:10.10.10.2       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n</code></pre> <p>R3:</p> <pre><code>A:R3&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            group \"eBGP\"\n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.0\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n\n\n\n\nA:R3# show router bgp neighbor 10.0.99.0 advertised-routes\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n\n\n\nA:R3# show router bgp neighbor 10.0.99.0 received-routes\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  10.10.55.0/24                                      n/a         100\n      10.0.99.0                                          None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>R4:</p> <pre><code>A:R4&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            group \"eBGP\"\n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.2\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n\n\n\nA:R4# show router bgp neighbor 10.0.99.2 advertised-routes\n===============================================================================\n BGP Router ID:10.20.20.4       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n\n\n\n\nA:R4# show router bgp neighbor 10.0.99.2 received-routes\n===============================================================================\n BGP Router ID:10.20.20.4       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  10.10.55.0/24                                      n/a         200\n      10.0.99.2                                          None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#ibgp-configuration","title":"iBGP configuration","text":"<p>iBGP sessions are established inside BGP Autonomous System and are used to distribute BGP routes between the routers there. In our case we have two Autonomous Systems, so we will configure full mesh of iBGP sessions within AS 65510 and AS 66520:</p> <p></p> <p>The reason we need to provide a full-mesh of iBGP sessions is dictated by the iBGP split-horizon rule. Starting with AS 65510, configure iBGP peer group for every router inside this AS and specify the other routers <code>system</code> IP addresses as a neighbor. For iBGP it is quite common to use <code>system</code> or loopback IP address (in contrast with link addresses used in eBGP) as a neighbor address because this enables IGP path redundancy.</p> <p>Configuration sequence is straightforward:</p> <ol> <li>create \"iBGP\" peer group</li> <li>set local AS Number as a <code>peer-as</code> inside the iBGP peer group</li> <li>specify the neighbors using their <code>system</code> interface addresses</li> </ol> <pre><code>## repeat configuration steps on all routers in AS 65510 and AS 65520\n*A:R1# configure router bgp group \"iBGP\"\n*A:R1&gt;config&gt;router&gt;bgp&gt;group# info\n----------------------------------------------\n                ## iBGP peers share the same AS Number in peer-as command \n                peer-as 65510\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.5\n                exit\n                neighbor 10.10.10.6\n                exit\n----------------------------------------------\n</code></pre> <p>Note, that you do not need to specify the local-address statement (though it is not prohibited) since SROS router will initiate TCP socket opening from its system IP address by default.</p> <p>To verify that iBGP sessions have been successfully established you can use good-old <code>show router bgp summary</code> or fancy <code>show router bgp group &lt;group name&gt;</code> commands:</p> <pre><code>A:R1# show router bgp group \"iBGP\"\n\n===============================================================================\nBGP Group : iBGP\n===============================================================================\n-------------------------------------------------------------------------------\nGroup            : iBGP\n-------------------------------------------------------------------------------\nDescription      : (Not Specified)\nGroup Type       : No Type              State            : Up\nPeer AS          : 65510                Local AS         : 65510\nLocal Address    : n/a                  Loop Detect      : Ignore\nImport Policy    : None Specified / Inherited\nExport Policy    : None Specified / Inherited\nHold Time        : 90                   Keep Alive       : 30\nMin Hold Time    : 0\nCluster Id       : None                 Client Reflect   : Enabled\nNLRI             : Unicast              Preference       : 170\nTTL Security     : Disabled             Min TTL Value    : n/a\nGraceful Restart : Disabled             Stale Routes Time: n/a\nRestart Time     : n/a\nAuth key chain   : n/a\nBfd Enabled      : Disabled             Disable Cap Nego : Disabled\nCreation Origin  : manual\nFlowspec Validate: Disabled             Default Route Tgt: Disabled\nAigp Metric      : Disabled\nSplit Horizon    : Disabled\nDamp Peer Oscill*: Disabled\nGR Notification  : Disabled             Fault Tolerance  : Disabled\nNext-Hop Unchang*: None\n\nList of Peers\n- 10.10.10.2 :\n- 10.10.10.5 :\n- 10.10.10.6 :\n\nTotal Peers      : 3                    Established      : 3\n-------------------------------------------------------------------------------\nPeer Groups : 1\n===============================================================================\n</code></pre> <p>Ok, now we got iBGP full-mesh configured for both ASes but to start playing with iBGP let me introduce you to another customer network - <code>R3_Ext_Customer - 172.16.33.0/24</code>. This customer network resides beside R3 and you should add it to the BGP with the same policies/export routine as we did before for <code>R5_Customer</code>.</p> <pre><code>## 1. create interface to emulate R3_Ext_Customer network\n*A:R3# configure router interface R3_Ext_Customer\n*A:R3&gt;config&gt;router&gt;if$ address 172.16.33.1/24\n*A:R3&gt;config&gt;router&gt;if$ loopback\n\n## 2. configure policy statement to export specific prefix to bgp\n*A:R3&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"R3_Ext_Customer\"\n                prefix 172.16.33.0/24 exact\n            exit\n            policy-statement \"export_R3_Ext_Customer\"\n                entry 10\n                    from\n                        prefix-list \"R3_Ext_Customer\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n\n## 3. export\n*A:R3# configure router bgp group \"eBGP\" export \"export_R3_Ext_Customer\"\n</code></pre> <p>What happens next? Correct, <code>R3_Ext_Customer</code> NLRI goes from R3 to R1 via eBGP. R1 checks if this NLRI is valid by checking the AS Path for looping and the next-hop for reachability.</p> <p>Since the default behavior of the eBGP is to set its egress interface's IP address as a next-hop, we see that R1 receives BGP routes with <code>10.0.99.1</code> address as the next-hop:</p> <pre><code>A:R1# show router bgp neighbor 10.0.99.1 received-routes\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  172.16.33.0/24                                     n/a         None\n      10.0.99.1                                          None        -\n      65520\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>This next-hop is reachable to R1 since it has <code>toR3</code> interface in this network. So R1 has every right to pass NLRI <code>10.0.99.1</code> to the BGP Local-RIB and then to the Routing Table Manager (RTM then installs this route to R1's routing table).</p> <p>And now iBGP on R1 comes into play by advertising NLRI <code>10.0.99.1</code> to its iBGP peers. This is the default BGP's behavior to advertise valid BGP routes came from eBGP peer to all iBGP peers, and the most important part of this eBGP-&gt;iBGP redistribution is that the next-hop once set by eBGP peer (R3 in our case) goes unchanged in iBGP updates:</p> <p></p> <p>Take a look at R5's BGP routes:</p> <pre><code>A:R5# show router bgp routes\n===============================================================================\n BGP Router ID:10.10.10.5       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     172.16.33.0/24                                     100         None\n      10.0.99.1                                          None        -\n      65520\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>R5 received <code>R3_Ext_Customer 172.16.33.0/24</code> NLRI but it cant use it (<code>u</code> flag is absent). Invoke <code>hunt</code> command to see whats wrong:</p> <pre><code>A:R5# show router bgp routes 172.16.33.0/24 hunt\n===============================================================================\n BGP Router ID:10.10.10.5       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 172.16.33.0/24\nNexthop        : 10.0.99.1\nPath Id        : None\nFrom           : 10.10.10.1\nRes. Nexthop   : Unresolved\nLocal Pref.    : 100                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : No Community Members\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.1\nFwd Class      : None                   Priority       : None\nFlags          : Invalid  IGP  Nexthop-Unresolved\nRoute Source   : Internal\nAS-Path        : 65520\nRoute Tag      : 0\nNeighbor-AS    : 65520\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>Aha, R5 cant validate received NLRI since its next-hop is unresolvable to R5. Recall the R1 did not change next-hop information it received from R3, so R5 received the same IP address <code>10.0.99.1</code> as a next-hop and R5 has no route towards it. That is the reason that R5's routing table has no network <code>10.0.99.1</code>:</p> <pre><code>A:R5# show router route-table 176.16.33.0/24\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nNo. of Routes: 0\n</code></pre> <p>There are two approaches to fix this:</p> <ol> <li>use <code>next-hop-self</code> command on R1</li> <li>adding eBGP interfaces to IGP process (as passive interface) making them known to every router participating in the IGP domain. Or implementing static or default routes in AS 65510 to reach R3's interface network</li> </ol> <p>We will stick to the first option.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#ibgp-next-hop-self","title":"iBGP next-hop-self","text":"<p>The <code>next-hop-self</code> command forces iBGP speaker, who received an eBGP update message to substitute next-hop information with its <code>system</code> IP address.</p> <pre><code>A:R1# configure router bgp group \"iBGP\"\nA:R1&gt;config&gt;router&gt;bgp&gt;group# next-hop-self\n</code></pre> <p>Get back to R5 and check whats changed:</p> <pre><code>A:R5# show router bgp routes\n===============================================================================\n BGP Router ID:10.10.10.5       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  172.16.33.0/24                                     100         None\n      10.10.10.1                                         None        -\n      65520\n-------------------------------------------------------------------------------\nRoutes : 1\n===============================================================================\n</code></pre> <p>Now it is a totally different story! R5 successfully validates the received NLRI and can use it thanks to resolvable next-hop which is R1's system IP address <code>10.10.10.1</code>.</p> <p>The next step is to pass this route to the RTM which is responsible for routing-table provisioning. If we take a look at R5 routing table for the recently received <code>172.16.33.0/24</code> prefix we will see that next-hop isn't <code>10.10.10.1</code>:</p> <pre><code>A:R5# show router route-table 172.16.33.0/24\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n172.16.33.0/24                                Remote  BGP       00h41m45s  170\n       10.10.99.4                                                   0\n-------------------------------------------------------------------------------\nNo. of Routes: 1\n</code></pre> <p>The reason behind this discrepancy is that the routing table should have connected networks as a next-hop and since <code>10.10.10.1</code> is far from being connected to R5 it performs an operation called recursive lookup. R5 takes next-hop value received from the iBGP update <code>10.10.10.1</code> and performs the route-table lookup:</p> <pre><code>A:R5# show router route-table 10.10.10.1\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.10.1/32                                 Remote  ISIS      00h44m28s  15\n       10.10.99.4                                                   100\n-------------------------------------------------------------------------------\nNo. of Routes: 1\n</code></pre> <p>R5 knows how to reach <code>10.10.10.1</code> by means of IS-IS protocol and the next-hop for this prefix is indeed <code>10.10.99.4</code> which is a connected network:</p> <pre><code>A:R5# show router route-table 10.10.99.4\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.99.4/31                                 Local   Local     00h44m45s  0\n       toR1                                                         0\n-------------------------------------------------------------------------------\nNo. of Routes: 1\n</code></pre> <p>That is why we see a different next-hop in the routing and BGP Local-RIB tables.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-1---basic-ebgp-ibgp/#wrapping-up","title":"Wrapping up","text":"<p>To this moment we have done a good job - we have configured the peering between two autonomous systems AS 65510 and AS 65520 and successfully exchanged the prefixes. Now, a client residing in the R3_Ext_Customer network can reach hosts from the R5_Customer network:</p> <pre><code>## we have to specify a source address for ping to success,\n## since by default ALU routers perform ping from their system interface\n## and AS 65510 know nothing about system addresses of foreign AS.\nA:R3# ping 10.10.55.1 source 172.16.33.1 \nPING 10.10.55.1 56 data bytes\n64 bytes from 10.10.55.1: icmp_seq=1 ttl=63 time=9.38ms.\n64 bytes from 10.10.55.1: icmp_seq=2 ttl=63 time=3.58ms.\n64 bytes from 10.10.55.1: icmp_seq=3 ttl=63 time=3.13ms.\n64 bytes from 10.10.55.1: icmp_seq=4 ttl=63 time=28.7ms.\n64 bytes from 10.10.55.1: icmp_seq=5 ttl=63 time=103ms.\n\n---- 10.10.55.1 PING Statistics ----\n5 packets transmitted, 5 packets received, 0.00% packet loss\n</code></pre> <p>This was accomplished by mutual exchange of the corresponding routes both via eBGP and iBGP protocols.</p> <p>If some of you want to get the full picture - see this configuration snapshot captured on every router of this topology:</p> <p>R1:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"system\"\n            address 10.10.10.1/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.10.99.0/31\n            port 1/1/1\n            no shutdown\n        exit\n        interface \"toR3\"\n            address 10.0.99.0/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR5\"\n            address 10.10.99.4/31\n            port 1/1/4\n            no shutdown\n        exit                          \n        autonomous-system 65510\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only     \n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR2\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR5\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service\n        customer 1 create\n            description \"Default customer\"\n        exit                          \n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"R5_Customer_pfx\"\n                prefix 10.10.55.0/24 exact\n            exit\n            policy-statement \"R5_Customer_export\"\n                entry 10\n                    from\n                        prefix-list \"R5_Customer_pfx\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                export \"R5_Customer_export\" \n                peer-as 65520         \n                split-horizon\n                neighbor 10.0.99.1\n                    local-address 10.0.99.0\n                exit\n            exit\n            group \"iBGP\"\n                next-hop-self\n                peer-as 65510\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.5\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n    exit\n\n\nexit all\n</code></pre> <p>R2:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"system\"\n            address 10.10.10.2/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.10.99.1/31\n            port 1/1/1\n            no shutdown\n        exit\n        interface \"toR4\"\n            address 10.0.99.2/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR6\"\n            address 10.10.99.2/31     \n            port 1/1/4\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit                      \n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR6\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service                           \n        customer 1 create\n            description \"Default customer\"\n        exit\n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"R5_Customer_pfx\"\n                prefix 10.10.55.0/24 exact\n            exit\n            policy-statement \"R5_Customer_export\"\n                entry 10\n                    from\n                        prefix-list \"R5_Customer_pfx\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp                           \n            group \"eBGP\"\n                export \"R5_Customer_export\" \n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.3\n                    local-address 10.0.99.2\n                exit\n            exit\n            group \"iBGP\"\n                peer-as 65510\n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.5\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"System Time NTP Configuration\"\n#--------------------------------------------------\n    system\n        time\n            ntp\n            exit\n        exit\n    exit\n\nexit all\n</code></pre> <p>R3:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"R3_Ext_Customer\"\n            address 172.16.33.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"system\"\n            address 10.20.20.3/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.0.99.1/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR4\"              \n            address 10.20.99.0/31\n            port 1/1/2\n            no shutdown\n        exit\n        autonomous-system 65520\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 20.20\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only     \n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR4\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service\n        customer 1 create\n            description \"Default customer\"\n        exit\n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"R3_Ext_Customer\"\n                prefix 172.16.33.0/24 exact\n            exit\n            policy-statement \"export_R3_Ext_Customer\"\n                entry 10              \n                    from\n                        prefix-list \"R3_Ext_Customer\"\n                    exit\n                    to\n                        protocol bgp\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                export \"export_R3_Ext_Customer\" \n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.0\n                    local-address 10.0.99.1\n                exit\n            exit                      \n            group \"iBGP\"\n                next-hop-self\n                peer-as 65520\n                neighbor 10.20.20.4\n                exit\n            exit\n            no shutdown\n        exit\n    exit\n\n\nexit all\n</code></pre> <p>R4:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"system\"\n            address 10.20.20.4/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.0.99.3/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR3\"\n            address 10.20.99.1/31\n            port 1/1/2\n            no shutdown\n        exit\n        autonomous-system 65520\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf                          \n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 20.20\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR3\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown               \n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service\n        customer 1 create\n            description \"Default customer\"\n        exit\n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"             \n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.2\n                    local-address 10.0.99.3\n                exit\n            exit\n            group \"iBGP\"\n                next-hop-self\n                peer-as 65520\n                neighbor 10.20.20.3\n                exit\n            exit\n            no shutdown\n        exit\n    exit                              \n\n\nexit all\n</code></pre> <p>R5:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"R5_Customer_loopback\"\n            address 10.10.55.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"system\"\n            address 10.10.10.5/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.10.99.5/31\n            port 1/1/4\n            no shutdown\n        exit\n        interface \"toR6\"\n            address 10.10.99.6/31\n            port 1/1/1\n            no shutdown\n        exit                          \n        autonomous-system 65510\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit                      \n            interface \"R5_Customer_loopback\"\n                passive\n                no shutdown\n            exit\n            interface \"toR1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR6\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service\n        customer 1 create\n            description \"Default customer\"\n        exit                          \n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"iBGP\"\n                peer-as 65510         \n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n    exit\n\n\nexit all\n</code></pre> <p>R6:</p> <pre><code>#--------------------------------------------------\necho \"Router (Network Side) Configuration\"\n#--------------------------------------------------\n    router \n        interface \"system\"\n            address 10.10.10.6/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.10.99.3/31\n            port 1/1/4\n            no shutdown\n        exit\n        interface \"toR5\"\n            address 10.10.99.7/31\n            port 1/1/1\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf                          \n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR2\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR5\"          \n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n    exit\n\n#--------------------------------------------------\necho \"Service Configuration\"\n#--------------------------------------------------\n    service\n        customer 1 create\n            description \"Default customer\"\n        exit\n    exit\n#--------------------------------------------------\necho \"Router (Service Side) Configuration\"\n#--------------------------------------------------\n    router \n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf                          \n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"iBGP\"\n                peer-as 65510\n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.5\n                exit\n            exit\n            no shutdown\n        exit                          \n    exit\n\n\nexit all\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/","title":"Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities","text":"<p>In the first part of this BGP tutorial we prepared the ground by configuring eBGP/iBGP peering. We did a good job overall, yet the plain BGP peering is not something you would not normally see in production. The power of BGP is in its ability for granular management of multiple routes from multiple sources. And the tools that help BGP to handle this complex task are BGP policies at their full glory.</p> <p>In this part we will discuss and practice:</p> <ul> <li>BGP export/import policies for route advertisement/filtering</li> <li>BGP communities operations</li> <li>BGP routes aggregation: route summarization and the corresponding <code>aggregate</code> and <code>atomic-aggregate</code> path attributes</li> </ul>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#what-are-bgp-policies-for","title":"What are BGP policies for?","text":"<p>The BGP peering configuration process is simple, you saw it in Part 1, but, frankly, no network engineer leaves a BGP router in a default \"receive all, advertise all\" state. We use BGP policies to tell the router which routes to accept and which to advertise.</p> <p>This is just an example on where the BGP policies play a part, but they are used for many other tasks as well.</p> <p>In BGP you can define two types of policies: Import and Export. To demonstrate where and when does a particular policy take place I paste here the BGP route processing diagram:</p> <p></p> <p>Export polices are used for:</p> <ul> <li>export routes from different protocols to BGP (like IGP routes being exported to BGP in Part 1)</li> <li>granular control of the advertised routes<ul> <li>prohibit unwanted prefixes advertising</li> <li>set the path attributes to a desired NLRI</li> </ul> </li> <li>reducing control plane traffic by advertising aggregate routes</li> </ul> <p>Import policies are used for:</p> <ul> <li>filtering unwanted NLRI<ul> <li>by prefix, prefix-length, community value</li> </ul> </li> <li>manipulation with the outbound traffic<ul> <li>applying <code>Local-Pref</code> attribute to desired prefixes</li> <li>modifying/setting <code>MED</code> value or any other transitive attribute</li> </ul> </li> </ul> <p>In SROS BGP policy configuration takes place in a router's policy-options context - <code>configure router policy-options</code>.</p> <p>To practice with BGP policies configuration we will go through a set of tasks that an ISP engineer can be expected to do in theirs day-to-day operations. We will simulate a simple ISP scenario using the following network topology:</p> <p></p> <p>Network interfaces, IGP and basic BGP configuration are done exactly the same as in the Part 1 of this series. If you are interested in the final configuration output, please refer to the Wrapping up section.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#community-attribute","title":"Community attribute","text":"<p>Lets statr with BGP communities introduction. A BGP community (not extended) is an optional transitive BGP Path attribute that is a group of destinations which share some common property as per the RFC 1997.</p> <p>I like to think of a community as a label (or a tag) which BGP speaker puts on a NLRI to give it a context. These labels could serve different purposes, for example:</p> <ul> <li>to mark/identify the prefixes originated from a specific geographic region, customer or service,</li> <li>or to indicate that a specific treatment is desired like for the prefixes with the use of <code>no-export</code> or <code>no-advertise</code> community values</li> <li>or could mean any other property which BGP speaker wants to communicate within an NLRI.</li> </ul> <p>BGP Communities are just the means to augment the specific prefixes with the metadata, they are useless until you bind some actions to them. For example, you tag some prefixes with a <code>community A</code> value and others with a <code>community B</code>; then you could tell your BGP peers to, say, set the <code>Local Preference 200</code> attribute for the prefixes that have <code>community A</code> value and leave Local Preference intact for the ones marked with <code>community B</code> value. In this examples communities allowed us to set a specific action based on the community value associated with the prefixes.</p> <p>Communities are represented by a community string which is a 32 bit value. First two bytes of a community attribute have to be encoded with an AS number where community was born, and the other two bytes are set by an AS network engineer as he pleases (or in other words, the community string follows this template <code>&lt;2byte-asnumber:community-value&gt;</code>).</p> <p>The community attribute values range from <code>0x0000000 (0:0)</code> through <code>0x0000FFFF (0:65535)</code>. The range from <code>0xFFFF0000 (65535:0)</code> through <code>0xFFFFFFFF (65535:65535)</code> is reserved.</p> <p>Lets run our lab and refer to the following diagram before we start configuring community attributes:</p> <p></p> <p>We will introduce three different communities for our customer routes:</p> <ul> <li><code>65510:100</code> - community for every route originated from the West side of AS 65510</li> <li><code>65510:200</code> - community for every route originated from the East side of our AS 65510</li> <li><code>65510:2</code> - community for the <code>Customer_2</code> routes</li> </ul>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#adding-community","title":"Adding community","text":"<p>Community strings are configured under the <code>policy-options</code> context. Before we will be able to add communities to the prefixes, we need to:</p> <ol> <li>Specify the <code>prefix-lists</code> for our the routes we would like to mark with a community</li> <li>Declare the community strings we want to refer later to.</li> </ol> <p>R5 policy config:</p> <pre><code>*A:R5&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"Customer_1\"\n                prefix 10.10.55.0/24 exact\n                prefix 10.10.66.0/24 exact\n            exit\n            prefix-list \"Customer_2\"\n                prefix 172.10.55.0/24 exact\n                prefix 172.10.66.0/24 exact\n            exit\n            community \"East\" members \"65510:200\"\n            community \"West\" members \"65510:100\"\n            community \"Customer_2\" members \"65510:2\"\n----------------------------------------------\n</code></pre> <p>The same <code>policy-options</code> configuration block is applied to R6.</p> <p>To tag the routes with a community value we have to additionally configure a <code>policy-statement</code> which will associate a community string with the selected prefixes. This policy will be later referenced in the BGP configuration as the export policy.</p> <p>Consider the following <code>policy-statement \"Adv_Customers_nets\"</code> that is configured on R5:</p> <pre><code>*A:R5&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n&lt;prefix-lists and community strings are omitted&gt;\n\n            policy-statement \"Adv_Customers_nets\"\n## =======================================================================\n                entry 10 \n                    from\n                        protocol direct ## matches every connected network\n                    exit\n                    action next-entry \n## next-entry means that upon successful completion of this action \n## we DON'T stop the policy evaluation process and proceed to the next entry\n                        community add \"West\" ## adding community to every direct connected route\n                                             ## matched by the \"from\" statement\n                    exit\n                exit\n## =======================================================================\n                entry 20\n                    from\n                        prefix-list \"Customer_2\" ## matches Customer_2 routes\n                    exit\n                    action accept\n## action \"accept\" stops policy evaluation effective immediately\n## and its okay in this situation, since we have only two customers\n## so if we matched \"Customer_2\" and marked it with its community, \n## then we have nothing to do else and can stop policy evaluation\n                        community add \"Customer_2\"\n                    exit\n                exit\n## =======================================================================\n                entry 30\n                    from\n                        prefix-list \"Customer_1\"\n                    exit\n                    action accept\n## we do not need to add communities for the Customer_1 routes, but we need to create\n## an \"action accept\" for its prefixes, since the default action is deny-all\n                    exit\n                exit\n            exit\n----------------------------------------------\n</code></pre> <p>The same policy statement configuration should be created on R6 router.</p> <p>In the example above we added the community string using the <code>community add</code> operation; SROS also has additional operations provided for the community strings:</p> <pre><code>- community add &lt;name&gt; [&lt;name&gt;...(upto 28 max)]\n- community remove &lt;name&gt; [&lt;name&gt;...(upto 28 max)]\n- community replace &lt;name&gt; [&lt;name&gt;...(upto 28 max)]\n- no community\n</code></pre> <p>As was explained before, the policy statement should then be applied as an <code>export</code> policy in the respective BGP configuration on both R5 and R6:</p> <pre><code>*A:R5# configure router bgp group \"iBGP\"\n*A:R5&gt;config&gt;router&gt;bgp&gt;group# export \"Adv_Customers_nets\"\n\n*A:R6# configure router bgp group \"iBGP\"\n*A:R6&gt;config&gt;router&gt;bgp&gt;group# export \"Adv_Customers_nets\"\n</code></pre> <p>Now take a look at the <code>show router bgp summary</code> command on R5. There are 2 routes advertised to every iBGP neighbor and 2 rotes received from its fellow - R6:</p> <pre><code>===============================================================================\nBGP Summary\n===============================================================================\nNeighbor\n                   AS PktRcvd InQ  Up/Down   State|Rcv/Act/Sent (Addr Family)\n                      PktSent OutQ\n-------------------------------------------------------------------------------\n10.10.10.1\n                65510     593    0 04h55m11s 0/0/2 (IPv4)\n                          595    0\n10.10.10.2\n                65510     593    0 04h55m09s 0/0/2 (IPv4)\n                          595    0\n10.10.10.6\n                65510     592    0 04h53m51s 2/2/2 (IPv4)\n                          592    0\n-------------------------------------------------------------------------------\n</code></pre> <p>The BGP summary output does not disclose if any community attributes were applied to the NLRIs a BGP speaker sent or received, to verify that we should ask for a specific prefix details.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#show-communities","title":"Show communities","text":"<p>To ensure that the correct communities were passed along with the NLRI we can leverage the <code>show router bgp routes &lt;prefix&gt; detail</code> command and check the <code>Community</code> column in its output:</p> <pre><code>A:R1# show router bgp routes 172.10.55.0/24 detail\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 172.10.55.0/24\nNexthop        : 10.10.10.5\nPath Id        : None\nFrom           : 10.10.10.5\nRes. Nexthop   : 10.10.99.5\nLocal Pref.    : 100                    Interface Name : toR5\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : 65510:2 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.5\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP\nRoute Source   : Internal\nAS-Path        : No As-Path\nRoute Tag      : 0\nNeighbor-AS    : N/A\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n\n\n\n\nA:R1# show router bgp routes 10.10.55.0/24 detail\n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 10.10.55.0/24\nNexthop        : 10.10.10.5\nPath Id        : None\nFrom           : 10.10.10.5\nRes. Nexthop   : 10.10.99.5\nLocal Pref.    : 100                    Interface Name : toR5\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.5\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP\nRoute Source   : Internal\nAS-Path        : No As-Path\nRoute Tag      : 0\nNeighbor-AS    : N/A\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n</code></pre> <p>Everything works as expected. <code>Customer_1</code> routes are tagged with just the \"West\" community, and the <code>Customer_2</code> routes are being tagged with both \"West\" and \"Customer_2\" communities. Lets go a bit deeper and see how the community strings are carried in the BGP Updates messages:</p> <p></p> <p>Community path attribute propagates across eBGP links as well. To check this we can filter the routes with a specfic community that R3 has in its BGP routes tables. Note, that R3 resides in the AS 65520 and thus is a eBGP peer:</p> <pre><code>## show BGP routes labelled with a specific community string\n\nA:R3# show router bgp routes community 65510:2\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\nu*&gt;i  172.10.55.0/24                                     None        None\n      10.0.99.0                                          None        -\n      65510\n*i    172.10.55.0/24                                     100         None\n      10.20.20.4                                         None        -\n      65510\nu*&gt;i  172.10.66.0/24                                     None        None\n      10.0.99.0                                          None        -\n      65510\n*i    172.10.66.0/24                                     100         None\n      10.20.20.4                                         None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 4\n===============================================================================\n</code></pre> <p>And of course you can use <code>show router bgp routes &lt;prefix&gt; hunt</code> to see the verbose output of a given BGP route.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#operation-replace-well-known-communities","title":"Operation replace &amp; Well-known communities","text":"<p>RFC 1997 specifies the following well-known communities as the well-known communities:</p> <ul> <li><code>NO_EXPORT (0xFFFFFF01)</code> All routes received carrying a communities attribute containing this value MUST NOT be advertised outside a BGP confederation boundary (a stand-alone autonomous system that is not part of a confederation should be considered a confederation itself).</li> <li><code>NO_ADVERTISE (0xFFFFFF02)</code> All routes received carrying a communities attribute containing this value MUST NOT be advertised to other BGP peers.</li> <li><code>NO_EXPORT_SUBCONFED (0xFFFFFF03)</code> All routes received carrying a communities attribute containing this value MUST NOT be advertised to external BGP peers (this includes peers in other members autonomous systems inside a BGP confederation).</li> </ul> <p>You will encounter <code>no-export</code> and <code>no-advertise</code> communities quite often, as they are naturally used for route advertisement manipulations.</p> <p>To demonstrate the power of the default communities I would like to introduce you to another AS 65530 which has the single-homed peering with AS 65520:</p> <p></p> <p>Router R7 residing in AS 65530 has the default BGP configuration:</p> <pre><code>A:R7&gt;config&gt;router&gt;bgp# info \n----------------------------------------------\n            group \"no_export_example\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.4\n                    local-address 10.0.99.5\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>As a result of this configuration it receives all BGP routes that R3 has in its BGP Rib-Out database:</p> <p>R3 BGP routes:</p> <pre><code>A:R3# show router bgp routes \n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path                                                        \n-------------------------------------------------------------------------------\nu*&gt;i  10.10.55.0/24                                      None        None\n      10.0.99.0                                          None        -\n      65510                                                           \n*i    10.10.55.0/24                                      100         None\n      10.20.20.4                                         None        -\n      65510                                                           \nu*&gt;i  10.10.66.0/24                                      None        None\n      10.0.99.0                                          None        -\n      65510                                                           \n*i    10.10.66.0/24                                      100         None\n      10.20.20.4                                         None        -\n      65510                                                           \nu*&gt;i  172.10.55.0/24                                     None        None\n      10.0.99.0                                          None        -\n      65510                                                           \n*i    172.10.55.0/24                                     100         None\n      10.20.20.4                                         None        -\n      65510                                                           \nu*&gt;i  172.10.66.0/24                                     None        None\n      10.0.99.0                                          None        -\n      65510                                                           \n*i    172.10.66.0/24                                     100         None\n      10.20.20.4                                         None        -\n      65510                                                           \n-------------------------------------------------------------------------------\nRoutes : 8\n===============================================================================\n</code></pre> <p>R7 BGP routes:</p> <pre><code>A:R7# show router bgp routes \n===============================================================================\n BGP Router ID:10.30.30.7       AS:65530       Local AS:65530      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path                                                        \n-------------------------------------------------------------------------------\nu*&gt;i  10.10.55.0/24                                      None        None\n      10.0.99.4                                          None        -\n      65520 65510                                                     \nu*&gt;i  10.10.66.0/24                                      None        None\n      10.0.99.4                                          None        -\n      65520 65510                                                     \nu*&gt;i  172.10.55.0/24                                     None        None\n      10.0.99.4                                          None        -\n      65520 65510                                                     \nu*&gt;i  172.10.66.0/24                                     None        None\n      10.0.99.4                                          None        -\n      65520 65510                                                     \n-------------------------------------------------------------------------------\nRoutes : 4\n===============================================================================\n</code></pre> <p>Moreover, R7 receives all the community values with the received NLRIs. For instance, the community string <code>65510:100</code> is present for the <code>10.10.55.0/24</code> prefix:</p> <pre><code>A:R7# show router bgp routes 10.10.55.0/24 detail \n===============================================================================\n BGP Router ID:10.30.30.7       AS:65530       Local AS:65530      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.4\nPath Id        : None                   \nFrom           : 10.0.99.4\nRes. Nexthop   : 10.0.99.4\nLocal Pref.    : n/a                    Interface Name : toR4\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None                 \nCommunity      : 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.3\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : External\nAS-Path        : 65520 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65520\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n</code></pre> <p>Lets see how by tagging the <code>10.10.55.0/24</code> prefix with the <code>no-export</code> community we can change the BGP route advertisement process. To do so I will add a policy-statement to our border router R1 which will replace the community string <code>65510:100</code> of the  <code>10.10.55.0/24</code> prefix with the <code>no-export</code> one:</p> <p></p> <p>Lets track the community mutations of a <code>10.10.55.0/24</code> prefix. R3 receives this prefix from R1 and installs in its RIB. Community value for this prefix is <code>65510:100</code>:</p> <pre><code>A:R3# show router bgp routes 10.10.55.0/24 detail \n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.0\nPath Id        : None                   \nFrom           : 10.0.99.0\nRes. Nexthop   : 10.0.99.0\nLocal Pref.    : n/a                    Interface Name : toR1\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None                 \nCommunity      : 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.1\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : External\nAS-Path        : 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65510\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n</code></pre> <p>In order to replace the existing community value with a new one, we need to go to R1 and add the necessary policy configuration:</p> <pre><code>*A:R1&gt;config&gt;router&gt;policy-options# info \n----------------------------------------------\n            prefix-list \"Customer_1_55_network\"\n                prefix 10.10.55.0/24 exact\n            exit\n            ## We have to specify community strings \n            ## we want to use with policy-statements\n            community \"West\" members \"65510:100\"\n            community \"no-export\" members \"no-export\"\n            policy-statement \"replace_with_no_exp\"\n                entry 10\n                    from\n                        prefix-list \"Customer_1_55_network\"\n                    exit\n                    action accept                      ## it is allowed to set up to 28 communities \n                        community replace \"no-export\"  ## which will replaces all the current ones\n                    exit                              \n                exit\n            exit\n----------------------------------------------\n\n## add new policy-statement as \"export\" policy on R1 \nA:R1&gt;config&gt;router&gt;bgp# group \"eBGP\"\nA:R1&gt;config&gt;router&gt;bgp&gt;group# export \"replace_with_no_exp\"\n\n\n*A:R1&gt;config&gt;router&gt;bgp&gt;group# info \n----------------------------------------------\n                export \"replace_with_no_exp\" \n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.1\n                    local-address 10.0.99.0\n                exit\n----------------------------------------------\n</code></pre> <p>Ok, lets see if this change was casted properly, examine R1's RIB-In and RIB-Out tables and watch closely the <code>Community</code> value:</p> <pre><code>*A:R1&gt;config&gt;router&gt;bgp&gt;group# show router bgp routes 10.10.55.0/24 hunt \n===============================================================================\n BGP Router ID:10.10.10.1       AS:65510       Local AS:65510      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.10.10.5\nPath Id        : None                   \nFrom           : 10.10.10.5\nRes. Nexthop   : 10.10.99.5\nLocal Pref.    : 100                    Interface Name : toR5\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None\nCommunity      : 65510:100  ## R1 receives this community from R5\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.5\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : Internal\nAS-Path        : No As-Path\nRoute Tag      : 0                      \nNeighbor-AS    : N/A\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.0\nPath Id        : None                   \nTo             : 10.0.99.1\nRes. Nexthop   : n/a\nLocal Pref.    : n/a                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None\nCommunity      : no-export  ## R1 replaced community string 65510:100 with no-export\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.3\nOrigin         : IGP                    \nAS-Path        : 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65510\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRoutes : 2\n===============================================================================\n</code></pre> <p>Good, R1 modified the community attribute of this prefix and replaced the value it had received with a <code>no-export</code> well known community. It is important to note that <code>community replace</code> removes all communities for a prefix and sets a <code>no-export</code> instead them. So if we had &gt;1 communities for the <code>10.10.55.0/24</code> prefix we would end up with just one <code>no-export</code> in the end.</p> <p>Well, lets check with R3, how is this fella doing?</p> <pre><code>A:R3# show router bgp routes 10.10.55.0/24 detail \n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.0\nPath Id        : None                   \nFrom           : 10.0.99.0\nRes. Nexthop   : 10.0.99.0\nLocal Pref.    : n/a                    Interface Name : toR1\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None                 \nCommunity      : no-export\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.1\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : External\nAS-Path        : 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65510\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n</code></pre> <p>R3 receives and uses this prefix with the <code>no-export</code> community!</p> <p>Before we jump to R7 let the dust to settle and think about the propagation path of the examined prefix.</p> <ul> <li>R5 originates this prefix and sets community <code>West \"65510:100\"</code>.</li> <li>R5 then advertise via its BGP UPDATE message this prefix with this community to all of its iBGP peers (R1, R2 and R6 are the iBGP peers of R5).</li> <li>R1 (since we configured it this way) replaces community <code>West</code> with <code>no-export</code> for its eBGP peer R3, but R2 does not.</li> </ul> <p>This leads to an interesting situation when R2 advertises <code>10.10.55.0/24</code> prefix with its original community <code>West \"65510:100\"</code> so R4 selects this route as the best (because R4 prefers eBGP routes over iBGP):</p> <pre><code>A:R4# show router bgp routes 10.10.55.0/24 detail \n===============================================================================\n BGP Router ID:10.20.20.4       AS:65520       Local AS:65520      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nOriginal Attributes\n\nNetwork        : 10.10.55.0/24\nNexthop        : 10.0.99.2\nPath Id        : None                   \nFrom           : 10.0.99.2\nRes. Nexthop   : 10.0.99.2\nLocal Pref.    : n/a                    Interface Name : toR2\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None                 \nCommunity      : 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.2\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : External\nAS-Path        : 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65510\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n</code></pre> <p>This highlights the fact that if you have more then one eBGP peers and want to communicate specific community value - you should do this on every BGP border router.</p> <p>Check R7 BGP routes now. We expect that R3 does not advertise the prefix <code>10.10.55.0/24</code> to its eBGP peer R7, since it is instructed to do so by the <code>no-export</code> community:</p> <pre><code>## making sure R3 does not advertise 10.10.55.0/24 to R7\n\nA:R3# show router bgp neighbor 10.0.99.5 advertised-routes\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     10.10.66.0/24                                      n/a         None\n      10.0.99.4                                          None        -\n      65520 65510\ni     172.10.55.0/24                                     n/a         None\n      10.0.99.4                                          None        -\n      65520 65510\ni     172.10.66.0/24                                     n/a         None\n      10.0.99.4                                          None        -\n      65520 65510\n-------------------------------------------------------------------------------\nRoutes : 3\n===============================================================================\n\n\n\n## Now check R7\n\nA:R7# show router bgp routes 10.10.55.0/24 \n===============================================================================\n BGP Router ID:10.30.30.7       AS:65530       Local AS:65530      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path                                                        \n-------------------------------------------------------------------------------\nNo Matching Entries Found\n===============================================================================\n</code></pre> <p>As expected, no <code>10.10.55.0/24</code> prefix is advertised by R3. That is what <code>no-export</code> community for.</p> <p>Note, R3 does not advertise a route with the <code>no-export</code> to its eBGP peers, but it does advertise it to its iBGP peers:</p> <pre><code>## R3 advertises 10.10.55.0/24 with no-export community to iBGP peer R4\nA:R3# show router bgp neighbor 10.20.20.4 advertised-routes\n===============================================================================\n BGP Router ID:10.20.20.3       AS:65520       Local AS:65520\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\nFlag  Network                                            LocalPref   MED\n      Nexthop (Router)                                   Path-Id     Label\n      As-Path\n-------------------------------------------------------------------------------\ni     10.10.55.0/24                                      100         None\n      10.20.20.3                                         None        -\n      65510\ni     10.10.66.0/24                                      100         None\n      10.20.20.3                                         None        -\n      65510\ni     172.10.55.0/24                                     100         None\n      10.20.20.3                                         None        -\n      65510\ni     172.10.66.0/24                                     100         None\n      10.20.20.3                                         None        -\n      65510\n-------------------------------------------------------------------------------\nRoutes : 4\n===============================================================================\n</code></pre> <p>The <code>no-advertise</code> community works a bit different - if a router receives a route with the <code>no-advertise</code> community it will not advertise it at all even to its iBGP peers.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#removing-community","title":"Removing community","text":"<p>Currently R2 does not modify any routes flying off to its eBGP peer R4. Lets imagine that we now have to remove the community <code>Customer_2</code> from all the Customer_2 routes on R2 before advertising them to the AS 65520. To do so we have to perform a community remove operation.</p> <p>To filter all the routes with a community string <code>65510:2</code> we could use prefix lists or filter by the community string value:</p> <pre><code>*A:R2&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            community \"Customer2\" members \"65510:2\"\n            policy-statement \"remove_Cust2_community\"\n                entry 10\n                    from\n                        community \"Customer2\" ## filter routes with specific community value\n                    exit\n                    action accept\n                        community remove \"Customer2\"\n                    exit\n                exit\n            exit\n----------------------------------------------\n\n\n*A:R2&gt;config&gt;router&gt;bgp&gt;group# export \"remove_Cust2_community\"\n*A:R2&gt;config&gt;router&gt;bgp&gt;group# info\n----------------------------------------------\n                export \"remove_Cust2_community\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.3\n                    local-address 10.0.99.2\n                exit\n----------------------------------------------\n</code></pre> <p>Now R2 selects the routes with the community value of <code>65510:2</code> and removes it before sending it to its eBGP peer R4:</p> <p>The following output verifies this behavior:</p> <pre><code>*A:R2# show router bgp routes 172.10.55.0/24 hunt\n===============================================================================\n BGP Router ID:10.10.10.2       AS:65510       Local AS:65510\n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 172.10.55.0/24\nNexthop        : 10.10.10.5\nPath Id        : None\nFrom           : 10.10.10.5\nRes. Nexthop   : 10.10.99.0\nLocal Pref.    : 100                    Interface Name : toR1\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : 65510:2 65510:100   ## R2 receives this prefix with 2 communities\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.5\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP\nRoute Source   : Internal\nAS-Path        : No As-Path\nRoute Tag      : 0\nNeighbor-AS    : N/A\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\nNetwork        : 172.10.55.0/24\nNexthop        : 10.0.99.2\nPath Id        : None\nTo             : 10.0.99.3\nRes. Nexthop   : n/a\nLocal Pref.    : n/a                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None\nConnector      : None\nCommunity      : 65510:100    ## on export operation R2 removes Customer_2 community\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.4\nOrigin         : IGP\nAS-Path        : 65510\nRoute Tag      : 0\nNeighbor-AS    : 65510\nOrig Validation: NotFound\nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRoutes : 2\n===============================================================================\n\n\n## same thing happened to another Customer_2 route\n\nA:R2# show router bgp routes 172.10.66.0/24 hunt \n===============================================================================\n BGP Router ID:10.10.10.2       AS:65510       Local AS:65510      \n===============================================================================\n Legend -\n Status codes  : u - used, s - suppressed, h - history, d - decayed, * - valid\n                 l - leaked\n Origin codes  : i - IGP, e - EGP, ? - incomplete, &gt; - best, b - backup\n\n===============================================================================\nBGP IPv4 Routes\n===============================================================================\n-------------------------------------------------------------------------------\nRIB In Entries\n-------------------------------------------------------------------------------\nNetwork        : 172.10.66.0/24\nNexthop        : 10.10.10.6\nPath Id        : None                   \nFrom           : 10.10.10.6\nRes. Nexthop   : 10.10.99.3\nLocal Pref.    : 100                    Interface Name : toR6\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None\nCommunity      : 65510:2 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.10.10.6\nFwd Class      : None                   Priority       : None\nFlags          : Used  Valid  Best  IGP  \nRoute Source   : Internal\nAS-Path        : No As-Path\nRoute Tag      : 0                      \nNeighbor-AS    : N/A\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRIB Out Entries\n-------------------------------------------------------------------------------\nNetwork        : 172.10.66.0/24\nNexthop        : 10.0.99.2\nPath Id        : None                   \nTo             : 10.0.99.3\nRes. Nexthop   : n/a\nLocal Pref.    : n/a                    Interface Name : NotAvailable\nAggregator AS  : None                   Aggregator     : None\nAtomic Aggr.   : Not Atomic             MED            : None\nAIGP Metric    : None                   \nConnector      : None\nCommunity      : 65510:100\nCluster        : No Cluster Members\nOriginator Id  : None                   Peer Router Id : 10.20.20.4\nOrigin         : IGP                    \nAS-Path        : 65510 \nRoute Tag      : 0                      \nNeighbor-AS    : 65510\nOrig Validation: NotFound               \nSource Class   : 0                      Dest Class     : 0\n\n-------------------------------------------------------------------------------\nRoutes : 2\n===============================================================================\n</code></pre> <p>Remove operation removes a single community per action. Like in this example R2's RIB-In had the prefix <code>172.10.55.0/24</code> with both <code>65510:2</code> and <code>65510:200</code> values set the remove operation removed the <code>65510:2</code> community only.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#matching-community","title":"Matching community","text":"<p>The whole thing with the communities is about performing the actions against the prefixed matched with it. As you saw previously, the action based on a community should first match the community by its value and there are several ways to do the matching.</p> <p>How can you pick the routes with the different communities and modify them altogether? One way would be to write multiple policy-statements with the same action and different <code>from community &lt;name&gt;</code> statements. This is a bruteforce. There are far more elegant techniques we are going to explore.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#and-and-or-operators","title":"\"AND\" and \"OR\" operators","text":"<p>To create a community that matches some of the community values you can use the <code>|</code> operator like that: <code>community West_or_Customer_2 members 65510:2|65510:100</code>. This community statement will match prefixes with the community strings like <code>65510:2</code>, <code>65510:2 65510:100</code> or <code>65510:2 63300:2 54487:200</code>.</p> <p>To create a community statement that will match on a string that has multiple community values (aka <code>AND</code> operator) you can compose the following community statement: <code>community \"A_and_B\" members \"65510:2\" \"65510:100\"</code>. This community will match <code>65510:2 65510:100</code> or <code>100:100 200:200 65510:2 65510:100</code> but not <code>65510:2</code> or <code>100:100 65510:2</code>.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#expressions","title":"Expressions","text":"<p>Nokia SR-OS has the built-in expressions engine equipped with a set of most commonly used operators. The Expressions syntax is described in the Routing Protocols guide:</p> <pre><code>community &lt;name&gt; expression &lt;expression&gt; [exact]\n\n&lt;expression&gt;         : [900 chars max] - &lt;expression&gt; is one of the following:\n                       &lt;expression&gt; {AND|OR} &lt;expression&gt;\n                       [NOT] ( &lt;expression&gt; )\n                       [NOT] &lt;comm-id&gt;\n&lt;exact&gt;              : keyword\n</code></pre> <p>A good example on the expressions syntax is demonstrated by the statement below which matches exactly the prefixes with agiven list of communities attached to them:</p> <pre><code>community \"West_and_Cust_2_ONLY\" expression \"65510:2 AND 65510:100\" exact\n</code></pre> <p>This community, once used in the <code>from</code> statement of a policy will match prefixes with the community string <code>65510:2 65510:100</code> only (enforced by the <code>exact</code> statement).</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#regular-expressions","title":"Regular Expressions","text":"<p>Another filtering technique is based on the regular expressions.</p> <p>Let me show you how easy it is to filter the routes with a community string containing <code>65510:100</code> or <code>65510:2</code> values:</p> <pre><code>*A:R1&gt;config&gt;router&gt;policy-options# info\n----------------------------------------------\n            prefix-list \"Customer_1_55_network\"\n                prefix 10.10.55.0/24 exact\n            exit\n            community \"West\" members \"65510:100\"\n            community \"no-export\" members \"no-export\"\n            community \"no-advertise\" members \"no-advertise\"\n\n            ## this community string equals to the form:\n            ## members list contains 65510:2 OR 65510:100\n            community \"Customer_2_from_West\" members \"65510:(2|100)\"\n\n            &lt;...omitted...&gt;\n\n            policy-statement \"test_regexp_community\"\n                entry 10\n                    from\n                        community \"Customer_2_from_West\"\n                    exit\n                    action accept\n                        community replace \"no-advertise\"\n                    exit\n                exit\n            exit\n----------------------------------------------\n</code></pre> <p>Here I defined the <code>Customer_2_from_West</code> community and used a simple regular expression for to match on the communities <code>65510:100</code> or <code>65510:2</code>. This regexp community string will match strings like  <code>65510:2</code> , <code>65510:2 65510:100</code> , <code>65510:2 63300:2 54487:200</code></p> <p>Of course you can create a far more complex regexps, check the table of the supported operators of the Routing Protocols Guide to build a regexp that meets your needs.</p>","tags":["nokia","sr os","bgp"]},{"location":"2015/nokia-alcatel-lucent-bgp-configuration-tutorial-part-2---communities/#wrapping-up","title":"Wrapping up","text":"<p>As usual, check out the full config that you will have at the end of this tutorial:</p> <p>R1:</p> <pre><code>A:R1&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.10.10.1/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.10.99.0/31\n            port 1/1/1\n            no shutdown\n        exit\n        interface \"toR3\"\n            address 10.0.99.0/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR5\"\n            address 10.10.99.4/31\n            port 1/1/4\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR2\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR5\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"Customer_1_55_network\"\n                prefix 10.10.55.0/24 exact\n            exit\n            community \"West\" members \"65510:100\"\n            community \"no-export\" members \"no-export\"\n            community \"no-advertise\" members \"no-advertise\"\n            community \"Customer_2_from_West\" members \"65510:(2|100)\"\n            policy-statement \"replace_with_no_exp\"\n                entry 10\n                    from\n                        prefix-list \"Customer_1_55_network\"\n                    exit\n                    action accept\n                        community replace \"no-export\"\n                    exit\n                exit\n            exit\n            policy-statement \"test_regexp_community\"\n                entry 10\n                    from\n                        community \"Customer_2_from_West\"\n                    exit\n                    action accept\n                        community replace \"no-advertise\"\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                export \"replace_with_no_exp\" \"test_regexp_community\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.1\n                    local-address 10.0.99.0\n                exit\n            exit\n            group \"iBGP\"\n                next-hop-self\n                peer-as 65510\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.5\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n</code></pre> <p>R2:</p> <pre><code>A:R2&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.10.10.2/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.10.99.1/31\n            port 1/1/1\n            no shutdown\n        exit\n        interface \"toR4\"\n            address 10.0.99.2/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR6\"\n            address 10.10.99.2/31\n            port 1/1/4\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR6\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            community \"Customer2\" members \"65510:2\"\n            policy-statement \"remove_Cust2_community\"\n                entry 10\n                    from\n                        community \"Customer2\"\n                    exit\n                    action accept\n                        community remove \"Customer2\"\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                export \"remove_Cust2_community\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.3\n                    local-address 10.0.99.2\n                exit\n            exit\n            group \"iBGP\"\n                peer-as 65510\n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.5\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>R3:</p> <pre><code>A:R3&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"R3_Ext_Customer\"\n            address 172.16.33.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"system\"\n            address 10.20.20.3/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.0.99.1/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR4\"\n            address 10.20.99.0/31\n            port 1/1/2\n            no shutdown\n        exit\n        interface \"toR7\"\n            address 10.0.99.4/31\n            port 1/1/5\n            no shutdown\n        exit\n        autonomous-system 65520\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 20.20\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR4\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                export \"export_R3_Ext_Customer\"\n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.0\n                    local-address 10.0.99.1\n                exit\n            exit\n            group \"iBGP\"\n                next-hop-self\n                peer-as 65520\n                neighbor 10.20.20.4\n                exit\n            exit\n            group \"no_export_example\"\n                peer-as 65530\n                split-horizon\n                neighbor 10.0.99.5\n                    local-address 10.0.99.4\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>R4:</p> <pre><code>A:R4&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.20.20.4/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.0.99.3/31\n            port 1/1/3\n            no shutdown\n        exit\n        interface \"toR3\"\n            address 10.20.99.1/31\n            port 1/1/2\n            no shutdown\n        exit\n        autonomous-system 65520\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 20.20\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR3\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"eBGP\"\n                peer-as 65510\n                split-horizon\n                neighbor 10.0.99.2\n                    local-address 10.0.99.3\n                exit\n            exit\n            group \"iBGP\"\n                next-hop-self\n                peer-as 65520\n                neighbor 10.20.20.3\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>R5:</p> <pre><code>A:R5&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"R5_Customer_1\"\n            address 10.10.55.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"R5_Customer_2\"\n            address 172.10.55.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"system\"\n            address 10.10.10.5/32\n            no shutdown\n        exit\n        interface \"toR1\"\n            address 10.10.99.5/31\n            port 1/1/4\n            no shutdown\n        exit\n        interface \"toR6\"\n            address 10.10.99.6/31\n            port 1/1/1\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR6\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"Customer_1\"\n                prefix 10.10.55.0/24 exact\n                prefix 10.10.66.0/24 exact\n            exit\n            prefix-list \"Customer_2\"\n                prefix 172.10.55.0/24 exact\n                prefix 172.10.66.0/24 exact\n            exit\n            community \"East\" members \"65510:200\"\n            community \"West\" members \"65510:100\"\n            community \"Customer_2\" members \"65510:2\"\n            policy-statement \"Adv_Customers_nets\"\n                entry 10\n                    from\n                        protocol direct\n                    exit\n                    action next-entry\n                        community add \"West\"\n                    exit\n                exit\n                entry 20\n                    from\n                        prefix-list \"Customer_2\"\n                    exit\n                    action accept\n                        community add \"Customer_2\"\n                    exit\n                exit\n                entry 30\n                    from\n                        prefix-list \"Customer_1\"\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"iBGP\"\n                export \"Adv_Customers_nets\"\n                peer-as 65510\n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.6\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>R6:</p> <pre><code>A:R6&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"R6_Customer_1\"\n            address 10.10.66.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"R6_Customer_2\"\n            address 172.10.66.1/24\n            loopback\n            no shutdown\n        exit\n        interface \"system\"\n            address 10.10.10.6/32\n            no shutdown\n        exit\n        interface \"toR2\"\n            address 10.10.99.3/31\n            port 1/1/4\n            no shutdown\n        exit\n        interface \"toR5\"\n            address 10.10.99.7/31\n            port 1/1/1\n            no shutdown\n        exit\n        autonomous-system 65510\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 10.10\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            level 2\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toR2\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toR5\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"Customer_1\"\n                prefix 10.10.55.0/24 exact\n                prefix 10.10.66.0/24 exact\n            exit\n            prefix-list \"Customer_2\"\n                prefix 172.10.55.0/24 exact\n                prefix 172.10.66.0/24 exact\n            exit\n            community \"East\" members \"65510:200\"\n            community \"West\" members \"65510:100\"\n            community \"Customer_2\" members \"65510:2\"\n            policy-statement \"Adv_Customers_nets\"\n                entry 10\n                    from\n                        protocol direct\n                    exit\n                    action next-entry\n                        community add \"West\"\n                    exit\n                exit\n                entry 20\n                    from\n                        prefix-list \"Customer_2\"\n                    exit\n                    action accept\n                        community add \"Customer_2\"\n                    exit\n                exit\n                entry 30\n                    from\n                        prefix-list \"Customer_1\"\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"iBGP\"\n                export \"Adv_Customers_nets\"\n                peer-as 65510\n                neighbor 10.10.10.1\n                exit\n                neighbor 10.10.10.2\n                exit\n                neighbor 10.10.10.5\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>R7:</p> <pre><code>A:R7&gt;config&gt;router# info\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.30.30.7/32\n            no shutdown\n        exit\n        interface \"toR4\"\n            address 10.0.99.5/31\n            port 1/1/5\n            no shutdown\n        exit\n        autonomous-system 65530\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"no_export_example\"\n                peer-as 65520\n                split-horizon\n                neighbor 10.0.99.4\n                    local-address 10.0.99.5\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre>","tags":["nokia","sr os","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/","title":"Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX","text":"<p>The topic of this post is Layer 3 VPN (L3VPN or VPRN as we call it in SROS) configuration, and I decided to kill two birds with one stone by inviting Juniper vMX to our cozy SROS environment.</p> <p>The BGP/MPLS VPN (RFC 4364) configuration will undergo the following milestones:</p> <ul> <li>PE-PE relationship configuration with VPN IPv4 address family introduction</li> <li>PE-CE routing configuration with both BGP and OSPF as routing protocols</li> <li>Export policy configuration for advertising VPN routes on PE routers</li> <li>AS override configuration</li> <li>and many more</li> </ul> <p>We'll wrap it up with the Control Plane/Data Plane evaluation diagrams which help a lot with understanding the whole BGP VPN mechanics. Take your seats, and buckle up!</p> <p>The topology I use throughout this tutorial consists of two customers (namely Alcatel and Juniper) which have two remote sites and want to get connectivity between them by means of an L3VPN service:</p> <p></p> <p></p> <p>We start off with ISIS (any IGP would suffice) running smoothly in our provider core so every router can reach every other's loopback address. Another prerequisite is to have an MPLS enabled core, since L3VPN uses MPLS encapsulation for dataplane communication. I configured RSVP-TE tunnels between PE routers for this tutorial in the way that PE1_ALU can resolve PE2_JUN (and vice versa) loopback address via RSVP-TE tunnel.</p> <p>Lets have a look at the relevant configuration blocks on the three routers PE1_ALU, P1_ALU and PE2_JUN</p> <p>PE1_ALU:</p> <pre><code>A:PE1_ALU&gt;config&gt;router# info \n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.10.10.1/32\n            no shutdown\n        exit\n        interface \"toP1\"\n            address 10.99.99.0/31\n            port 1/1/2\n            no shutdown\n        exit\n        autonomous-system 100\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 49.10\n            traffic-engineering\n            reference-bandwidth 100000000\n            level 1                   \n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toP1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"MPLS Configuration\"\n#--------------------------------------------------\n        mpls\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toP1\"\n                no shutdown\n            exit\n        exit\n#--------------------------------------------------\necho \"RSVP Configuration\"\n#--------------------------------------------------\n        rsvp\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toP1\"\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"MPLS LSP Configuration\"\n#--------------------------------------------------\n        mpls\n            path \"loose\"\n                no shutdown\n            exit\n            lsp \"toPE2\"\n                to 10.10.10.3\n                cspf\n                primary \"loose\"\n                exit\n                no shutdown\n            exit\n            no shutdown\n        exit\n\n## Verification commands\nA:PE1_ALU# show router route-table 10.10.10.3\n\n===============================================================================\nRoute Table (Router: Base)\n===============================================================================\nDest Prefix[Flags]                            Type    Proto     Age        Pref\n      Next Hop[Interface Name]                                    Metric\n-------------------------------------------------------------------------------\n10.10.10.3/32                                 Remote  ISIS      01d00h57m  15\n       10.99.99.1                                                   200\n-------------------------------------------------------------------------------\n\n\nA:PE1_ALU# show router tunnel-table 10.10.10.3\n\n===============================================================================\nTunnel Table (Router: Base)\n===============================================================================\nDestination           Owner Encap TunnelId  Pref     Nexthop        Metric\n-------------------------------------------------------------------------------\n10.10.10.3/32         rsvp  MPLS  1         7        10.99.99.1     200\n-------------------------------------------------------------------------------\n</code></pre> <p>P1_ALU:</p> <pre><code>A:P1_ALU&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 10.10.10.2/32\n            no shutdown\n        exit\n        interface \"toPE1\"\n            address 10.99.99.1/31\n            port 1/1/1\n            no shutdown\n        exit\n        interface \"toPE2\"\n            address 10.99.99.2/31\n            port 1/1/2\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"ISIS Configuration\"\n#--------------------------------------------------\n        isis\n            level-capability level-1\n            area-id 49.10\n            traffic-engineering\n            reference-bandwidth 100000000\n            level 1\n                wide-metrics-only\n            exit\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toPE1\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            interface \"toPE2\"\n                interface-type point-to-point\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"MPLS Configuration\"\n#--------------------------------------------------\n        mpls\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toPE1\"\n                no shutdown\n            exit\n            interface \"toPE2\"\n                no shutdown\n            exit\n        exit\n#--------------------------------------------------\necho \"RSVP Configuration\"\n#--------------------------------------------------\n        rsvp\n            interface \"system\"\n                no shutdown\n            exit\n            interface \"toPE1\"\n                no shutdown\n            exit\n            interface \"toPE2\"\n                no shutdown\n            exit\n            no shutdown\n        exit\n#--------------------------------------------------\necho \"MPLS LSP Configuration\"\n#--------------------------------------------------\n        mpls\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>PE2_JUN:</p> <pre><code>root@PE2_JUN# show \n## Last changed: 2015-10-20 17:08:01 UTC\nversion 14.1R1.10;\n&lt;... omitted &gt;\ninterfaces {\n    ge-0/0/0 {\n        unit 0 {\n            family inet {\n                address 10.99.99.3/31;\n            }\n            family iso;\n            family mpls;\n        }\n    }\n    lo0 {\n        unit 0 {\n            family inet {\n                address 10.10.10.3/32;\n            }\n            family iso {\n                address 49.1001.0010.0100.0300;\n            }\n            family mpls;\n        }\n    }\n}\nrouting-options {\n    autonomous-system 100;\n}\nprotocols {\n    rsvp {\n        interface ge-0/0/0.0;\n    }\n    mpls {\n        label-switched-path toPE1 {\n            to 10.10.10.1;\n        }\n        interface ge-0/0/0.0;\n    }\n    isis {\n        reference-bandwidth 100g;\n        level 1 wide-metrics-only;\n        level 2 wide-metrics-only;\n        interface ge-0/0/0.0 {\n            point-to-point;\n            level 2 disable;\n        }\n        interface lo0.0;\n    }\n}\n\n## Verification commands\nroot@PE2_JUN# run show route 10.10.10.1\n\ninet.0: 6 destinations, 6 routes (6 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n10.10.10.1/32      *[IS-IS/15] 1d 00:10:30, metric 200\n                    &gt; to 10.99.99.2 via ge-0/0/0.0\n\ninet.3: 1 destinations, 1 routes (1 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n10.10.10.1/32      *[RSVP/7/1] 11:22:07, metric 200\n                    &gt; to 10.99.99.2 via ge-0/0/0.0, label-switched-path toPE1\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#bgp-l3vpn-terminology","title":"BGP L3VPN terminology","text":"<p>Before we dive deep into the BGP L3VPN configuration it is necessary to refresh on some basic theory. To get a deeper and broader knowledge on the following topic please consider Juniper's JUNOS MPLS and VPNs student guide and Alcatel-Lucent's Service Routing Architect guide.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#vrfs","title":"VRFs","text":"<p>In order to maintain different customer's routes independently PE routers use separate logical routing tables called Virtual Routing and Forwarding (VRF).</p> <p>RFC 4364. VRFs: Multiple Forwarding Tables in PEs</p> <p>Each PE router maintains a number of separate forwarding tables. One of the forwarding tables is the \"default forwarding table\". The others are \"VPN Routing and Forwarding tables\", or \"VRFs\".</p> <p>3.1. VRFs and Attachment Circuits Every PE/CE attachment circuit is associated, by configuration, with one or more VRFs. An attachment circuit that is associated with a VRF is known as a \"VRF attachment circuit\".</p> <p>In the simplest case and most typical case, a PE/CE attachment circuit is associated with exactly one VRF. When an IP packet is received over a particular attachment circuit, its destination IP address is looked up in the associated VRF. The result of that lookup determines how to route the packet.</p> <p>Provider Edge routers must have a VRF configured for each connected site. VRFs are totally separated in routers control plane by default, so we can depict VRFs as the routers on their own caged in a single hardware unit:</p> <p></p> <p>VRFs also remain local to the corresponding hosting PE routers and their number representation or names are never propagated to the other PEs. In our example we have four VRFs in total, two VRFs (VRF Alcatel and VRF Juniper) per PE.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#route-distinguisher","title":"Route Distinguisher","text":"<p>Since one router can have many routing instances (VRFs) inside, it is necessary to help a router to distinct between the different routes in the different VRFs. It is highly likely that customers connected to a single PE will have overlapping IP addresses and this will potentially lead to troubles as the router won't know which customer a route belongs to.</p> <p>I emulated this situation to help you better understand the problem; see, Juniper's loopback address for the emulated customers CE1/CE2 overlaps with Alcatel's customer loopback addresses. How will a PE router PE1_ALU distinct between these routes?</p> <p></p> <p>Route Distinguisher (RD) comes to the rescue.</p> <p>RFC 4364. Route Distinguisher definition An RD is simply a number, and it does not contain any inherent information; it does not identify the origin of the route or the set of VPNs to which the route is to be distributed. The purpose of the RD is solely to allow one to create distinct routes to a common IPv4 address prefix.</p> <p></p> <p>RD can be written in several forms, but it is handy to use the IP address in the Administrator subfield and VPN number in the Assigned number subfield:</p> <pre><code>A:PE1_ALU&gt;config&gt;service&gt;vprn# route-distinguisher\n  - no route-distinguisher\n  - route-distinguisher &lt;rd&gt;\n\n &lt;rd&gt;                 : &lt;ip-addr:comm-val&gt;|&lt;2byte-asnumber:ext-comm-val&gt;|\n                        &lt;4byte-asnumber:comm-val&gt;\n                        ip-addr        - a.b.c.d\n                        comm-val       - [0..65535]\n                        2byte-asnumber - [1..65535]\n                        ext-comm-val   - [0..4294967295]\n                        4byte-asnumber - [1..4294967295]\n\n### EXAMPLE ##\nA:PE1_ALU&gt;config&gt;service&gt;vprn# route-distinguisher 10.10.10.1:20\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#vpn-ipv4-routes","title":"VPN-IPv4 routes","text":"<p>A combination of a Route Distinguisher and an IPv4 route effectively produces what is called the VPN-IPv4 route. VPN-IPv4 routes are 12 byte length (8b RD + 4b IPv4) addresses exchanged by MP-BGP speakers. PE routers compose VPN-IPv4 addresses and allocate MPLS labels for the routes before sending them to the MP-BGP neighbors. Consider the picture above to get a visual representation of an VPN-IPv4 route.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#route-targets","title":"Route Targets","text":"<p>So Route Distinguishers make every VPN-IPv4 route unique in a providers core, but we still need a mechanism to tell what VRF a single VPN-IPv4 route belongs to? We need a way to extend the VPN-IPv4 route with the information about which routing instance this route should be put into.</p> <p>BGP community is a good way to solve this problem. For L3VPNs a specific extended community was defined in RFC 4364 Section 4.3.1 called Route Target.</p> <p>RFC 4364. Route Target definition</p> <p>Every VRF is associated with one or more Route Target (RT) attributes. When a VPN-IPv4 route is created (from an IPv4 route that the PE has learned from a CE) by a PE router, it is associated with one or more Route Target attributes. These are carried in BGP as attributes of the route.</p> <p>Any route associated with Route Target T must be distributed to every PE router that has a VRF associated with Route Target T. When such route is received by a PE router, it is eligible to be installed in those of the PE's VRFs that are associated with Route Target T. (Whether it actually gets installed depends upon the outcome of the BGP decision process, and upon the outcome of the decision process of the IGP (i.e., the intra-domain routing protocol) running on the PE/CE interface.)</p> <p>A Route Target attribute can be thought of as identifying a set of sites. (Though it would be more precise to think of it as identifying a set of VRFs.) Associating a particular Route Target attribute with a route allows that route to be placed in the VRFs that are used for routing traffic that is received from the corresponding sites.</p> <p>There is a set of Route Targets that a PE router attaches to a route received from site S; these may be called the \"Export Targets\". And there is a set of Route Targets that a PE router uses to determine whether a route received from another PE router could be placed in the VRF associated with site S; these may be called the \"Import Targets\". The two sets are distinct, and need not be the same. Note that a particular VPN-IPv4 route is only eligible for installation in a particular VRF if there is some Route Target that is both one of the route's Route Targets and one of the VRF's Import Targets.</p> <p>Usually the RTs are represented as <code>&lt;AS Number of a client network&gt;:&lt;VRF ID&gt;</code>:</p> <pre><code>*A:PE1_ALU&gt;config&gt;service&gt;vprn$ vrf-target\n  - vrf-target {&lt;ext-community&gt;|export &lt;ext-community&gt;|import &lt;ext-community&gt;}\n  - no vrf-target\n\n &lt;ext-community&gt;      : target:{&lt;ip-addr:comm-val&gt;|\n                        &lt;2byte-asnumber:ext-comm-val&gt;|\n                        &lt;4byte-asnumber:comm-val&gt;}\n                        ip-addr        - a.b.c.d\n                        comm-val       - [0..65535]\n                        2byte-asnumber - [0..65535]\n                        ext-comm-val   - [0..4294967295]\n                        4byte-asnumber - [0..4294967295]\n\n### EXAMPLE ##\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ vrf-target target:200:20\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#pe-pe-mp-bgp-configuration","title":"PE&lt;-&gt;PE MP-BGP configuration","text":"<p>First thing to accomplish in the L3VPN configuration is the BGP peering inside the provider's core network. We have two Provider Edge routers (PE) and one core provider (P) router in our simple network. Our business goal is to provide the L3VPN service to our beloved JUN and ALU customers. To do so, we need to configure BGP peering between all the PE routers involved in the L3VPN service setup, these two routers are PE1_ALU and PE2_JUN.</p> <p>The BGP configuration part for PE1_ALU and PE2_JUN routers follows a simple iBGP configuration routine (check BGP configuration tutorial to grab the basics), the only part which is different is a need of a new BGP address family. We need to enable this address family to deal with the VPN routes, which are different from the IPv4 routes.</p> <p>In Juniper this family is called <code>inet-vpn</code>, in SROS it is <code>vpn-ipv4</code>, but nonetheless it is just an address family which enables communication of VPN routes between the peers. We will see later how this family differs from a classic IPv4, but for now just look at the BGP configuration part for both PE routers:</p> <p>PE1_ALU:</p> <pre><code>*A:PE1_ALU&gt;config&gt;router&gt;bgp# info \n----------------------------------------------\n            group \"iBGP\"\n                family ipv4 vpn-ipv4\n                peer-as 100\n                local-address 10.10.10.1\n                neighbor 10.10.10.3\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>PE2_JUN:</p> <pre><code>bgp {\n        group iBGP {\n            local-address 10.10.10.3;\n            family inet {               \n                unicast;\n            }\n            family inet-vpn {\n                unicast;\n            }\n            peer-as 100;\n            neighbor 10.10.10.1;\n        }\n    }\n</code></pre> <p></p> <p>As you see, the only part which is related to L3VPN is this new VPN address family.</p> <p>Support for the additional address families transforms a classical BGP to a fancy Multi-Protocol BGP (RFC 4760). Lets see how this family is communicated in the BGP messages:</p> <p></p> <p>Both routers announces the capability to exchange VPN Unicast IPv4 routes in the <code>BGP OPEN</code> messages. If a BGP peer sees this capability in an incoming OPEN message, it assumes that the neighbor speaks VPN IPv4 routes.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#configuring-vrfs-on-pe1_alu-sros","title":"Configuring VRFs on PE1_ALU (SROS)","text":"<p>So far we have configured PE-PE relationship which is a foundation for a working L3VPN service. Our next step is a VRF configuration which can be seen as a customers facing dedicated routers inside a singel PE router hardware unit. We will start with PE1_ALU and configure VRFs 20 and 30.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#1-ports-configuration","title":"1. Ports configuration","text":"<p>At first we should ensure that customer facing ports operate in <code>access</code> mode.</p> <pre><code>### customer router `CE1_ALU` connects to a PE via port 1/1/1\n*A:PE1_ALU# configure port 1/1/1 shutdown\n*A:PE1_ALU# configure port 1/1/1 ethernet mode access\n*A:PE1_ALU# configure port 1/1/1 no shutdown\n\n### customer router `CE1_JUN` connects to a PE via port 1/1/3\n*A:PE1_ALU# configure port 1/1/3 shutdown\n*A:PE1_ALU# configure port 1/1/3 ethernet mode access\n*A:PE1_ALU# configure port 1/1/3 no shutdown```\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#2-customers-creation","title":"2. Customers creation","text":"<p>SROS uses the concept of the <code>customers</code> which is similar to the tenants in a virtualization world. I will create two new customers (<code>Customer 1</code> is a default one) to map them to the customers we have in our network:</p> <pre><code>*A:PE1_ALU&gt;config&gt;service# info\n----------------------------------------------\n        customer 1 create\n            description \"Default customer\"\n        exit\n        customer 20 create\n            description \"Juniper\"\n        exit\n        customer 30 create\n            description \"Alcatel\"\n        exit\n----------------------------------------------\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#3-vrf-configuration","title":"3. VRF configuration","text":"<p>After that I create a VPRN service (which is a fancy SROS name for a L3VPN) for each customer:</p> <pre><code>### create vprn service\n*A:PE1_ALU# configure service vprn 20 customer 20 create\n\n### give it a name\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ description \"Juniper Site A\"\n\n### create route-distinguisher for VRF 20\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ route-distinguisher 10.10.10.1:20\n\n### set route target for this VRF\n### here I configure the use of a single target \n### for both import and export operations following this form &lt;AS_Num&gt;:&lt;Service_Num&gt;\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ vrf-target target:200:20\n\n### Create an interface in this VRF\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ interface toCE1 create\n*A:PE1_ALU&gt;config&gt;service&gt;vprn&gt;if$ address 10.20.99.0/31\n\n### map a port to this interface. SAP here goes for \"Service Access Point\"\n*A:PE1_ALU&gt;config&gt;service&gt;vprn&gt;if$ sap 1/1/3 create\n*A:PE1_ALU&gt;config&gt;service&gt;vprn&gt;if&gt;sap$ back\n*A:PE1_ALU&gt;config&gt;service&gt;vprn&gt;if$ back\n\n### tell a router to resolve Next-Hop address in this VRF with MPLS tunnels\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ auto-bind mpls\n\n### enable VPRN service\n*A:PE1_ALU&gt;config&gt;service&gt;vprn$ no shutdown\n</code></pre> <p>VRF 30 configuration repeats the same steps:</p> <pre><code>A:PE1_ALU&gt;config&gt;service# info \n----------------------------------------------\n        vprn 30 customer 30 create\n            route-distinguisher 10.10.10.1:30\n            auto-bind mpls\n            vrf-target target:300:30\n            interface \"toCE1\" create\n                address 10.30.99.0/31\n                sap 1/1/1 create\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#configuring-pe-ce-routing-protocols","title":"Configuring PE -&gt; CE routing protocols","text":"<p>Ok, our VRFs 20 and 30 are configured on PE1_ALU router and we have customers interfaces attached. What we need to do next is to configure a routing protocol which will propagate customers routes to the PE router. On PE1_ALU router we will use BGP as a routing protocol towards the CE routers, consequently CE routers will use BGP as well. Lets configure BGP instances for VRFs 20 and 30:</p> <p>BGP configuration for VRF 20:</p> <pre><code>/configure service vprn 20 customer 20\n### specify AS number for BGP speaker in VRF 20\n            autonomous-system 100\n\n### configure BGP peer and use \"as-override\" technique\n            bgp\n                group \"toCE\"\n                    as-override\n                    peer-as 200\n                    local-address 10.20.99.0\n                    split-horizon\n                    neighbor 10.20.99.1\n                    exit\n                exit\n                no shutdown\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#as-override","title":"AS override","text":"<p>The <code>as-override</code> command under the BGP section is used to resolve the issue with AS-PATH loop prevention mechanism. When a BGP UPDATE message goes from CE1_JUN over eBGP to PE1_ALU it has AS-PATH value of <code>200</code>. Then this UPDATE message traverses Service Provider's network and as it goes over eBGP session to CE2_JUN its AS-PATH value becomes <code>\"100 200\"</code>. But CE2_JUN is a part of AS <code>200</code> itself, so it will silently discard a route update with AS-PATH value containing its AS number (AS PATH loop prevention mechanism makes it so).</p> <p><code>as-override</code> command placed under the BGP context of the receiving VRF on a PE router replaces the customers AS number with Service Providers own AS number, so AS-PATH string of <code>\"100 200\"</code> will become <code>\"100 100\"</code> and will be accepted by the CE router residing in AS 200 since no loop will be detected.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#export-policies","title":"Export policies","text":"<p>Note, that it is mandatory to create an export policy on SROS PE routes for incoming BGP-VPN routes to leave the VRF over the PE -&gt; CE routing protocol to the CE router:</p> <pre><code>*A:PE1_ALU# configure router policy-options \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options# begin \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options# policy-statement \"MP-BGP_to_CE\"\n*A:PE1_ALU&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ from protocol bgp-vpn \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ action accept \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry&gt;action$ back \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options&gt;policy-statement&gt;entry$ back \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options&gt;policy-statement$ back \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options# commit \n*A:PE1_ALU&gt;config&gt;router&gt;policy-options# info \n----------------------------------------------\n            policy-statement \"MP-BGP_to_CE\"\n                entry 10\n                    from\n                        protocol bgp-vpn\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n----------------------------------------------\n</code></pre> <p>Now add this policy under the BGP context of the VRF:</p> <pre><code>*A:PE1_ALU# configure service vprn 20 bgp group \"toCE\" export \"MP-BGP_to_CE\"\n</code></pre> <p>Super, now repeat the steps for <code>VRF 30</code>. The complete service configuration part on PE1_ALU should look as follows:</p> <pre><code>A:PE1_ALU&gt;config&gt;service# info\n----------------------------------------------\n        customer 1 create\n            description \"Default customer\"\n        exit\n        customer 20 create\n            description \"Juniper\"\n        exit\n        customer 30 create\n            description \"Alcatel\"\n        exit\n        vprn 20 customer 20 create\n            description \"Juniper Site A\"\n            autonomous-system 100\n            route-distinguisher 10.10.10.1:20\n            auto-bind mpls\n            vrf-target target:200:20\n            interface \"toCE1\" create\n                address 10.20.99.0/31\n                sap 1/1/3 create\n                exit\n            exit\n            bgp\n                group \"toCE\"\n                    as-override\n                    export \"MP-BGP_to_CE\"\n                    peer-as 200\n                    local-address 10.20.99.0\n                    split-horizon\n                    neighbor 10.20.99.1\n                    exit\n                exit\n                no shutdown\n            exit\n            no shutdown\n        exit\n        vprn 30 customer 30 create\n            description \"Alcatel Site A\"\n            autonomous-system 100\n            route-distinguisher 10.10.10.1:30\n            auto-bind mpls\n            vrf-target target:300:30\n            interface \"toCE1\" create\n                address 10.30.99.0/31\n                sap 1/1/1 create\n                exit\n            exit\n            bgp\n                group \"toCE\"\n                    as-override\n                    export \"MP-BGP_to_CE\"\n                    peer-as 300\n                    local-address 10.30.99.0\n                    split-horizon\n                    neighbor 10.30.99.1\n                    exit\n                exit\n                no shutdown\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#configuring-vrfs-on-pe2_jun-juniper","title":"Configuring VRFs on PE2_JUN (Juniper)","text":"<p>Juniper JUNOS does not use concept of network/access ports, thats why you deal with CE-facing interfaces just like you do with the normal ones:</p> <pre><code>set interfaces ge-0/0/1 unit 0 family inet address 10.20.99.2/31\nset interfaces ge-0/0/2 unit 0 family inet address 10.30.99.2/31\n</code></pre> <p>Now the VRF part; VRF is called a routing-instance in JUNOS.</p> <pre><code>[edit routing-instances]\nroot@PE2_JUN# show | display set \n### create a routing instance and set its type to VRF\n### in JUNOS its possible to set VRF name like \n### set routing-instance \"Juniper_Site_B\" but we will use numerical id for consistency\nset routing-instances 20 instance-type vrf\n\n### give VRF a description\nset routing-instances 20 description \"Juniper Site B\"\n\n### provision interface to CE router, RT and RD\nset routing-instances 20 interface ge-0/0/1.0\nset routing-instances 20 route-distinguisher 10.10.10.3:20\nset routing-instances 20 vrf-target target:200:20\n\n### and for VRF 30\nset routing-instances 30 description \"Alcatel Site B\"\nset routing-instances 30 instance-type vrf\nset routing-instances 30 interface ge-0/0/2.0\nset routing-instances 30 route-distinguisher 10.10.10.3:30\nset routing-instances 30 vrf-target target:300:30\n</code></pre> <p>VRF configuration on Juniper looks almost identical to Nokia. The major difference here is that you don't have to tell JUNOS to resolve VRF's next-hop address via MPLS tunnel. And you don't have to configure an export policy in case you are using eBGP as a PE-CE protocol. Juniper defaults to that behavior.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#pe-ce-configuration-on-juniper","title":"PE -&gt; CE configuration on Juniper","text":"<p>Note, that with Juniper we omit the explicit AS number configuration under the BGP configuration. In that case the globally configured AS number will be used.</p> <p>Configuration portion for VRF 20 will look as follows:</p> <pre><code>root@PE2_JUN# show | display set \n&lt;... omitted ...&gt;\nset routing-instances 20 protocols bgp group toCE local-address 10.20.99.2\nset routing-instances 20 protocols bgp group toCE peer-as 200\nset routing-instances 20 protocols bgp group toCE as-override\nset routing-instances 20 protocols bgp group toCE neighbor 10.20.99.3\n</code></pre> <p>So far we've played with BGP as a PE-CE routing protocol, but frankly speaking OSPF is not a stranger for this task as well. Lets see how to configure the OSPF adjacency between the Juniper PE and a CE2_ALU router.</p> <p>A key piece in this configuration block is the export policy which lets vpn-ipv4 routes imported into VRF 30 to be exported to CE2_ALU over OSPF.</p> <p>Configure an export policy first:</p> <pre><code>[edit]\nroot@PE2_JUN# show\n&lt;... omitted ...&gt;\npolicy-options {\n    policy-statement MP-BGP_to_CE_via_OSPF {\n        term export {\n            from protocol bgp;\n            then accept;\n        }\n    }\n}\n</code></pre> <p>Then configure PE-CE OSPF protocol with the export policy applied:</p> <pre><code>### optionally set router-id for OSPF process to use\nrouting-options {\n    router-id 100.100.100.100;\n}\nprotocols {\n    ospf {\n        export MP-BGP_to_CE_via_OSPF;\n        ## configure OSPF area and interfaces\n        area 0.0.0.0 {\n            interface ge-0/0/2.0 {\n                interface-type p2p;\n            }\n        }\n    }\n}\n\n### DISPLAY SET\nset routing-instances 30 routing-options router-id 100.100.100.100\nset routing-instances 30 protocols ospf export MP-BGP_to_CE_via_OSPF\nset routing-instances 30 protocols ospf area 0.0.0.0 interface ge-0/0/2.0 interface-type p2p```\n</code></pre> <p>Complete VRF configuration for PE2_JUN goes like this:</p> <pre><code>[edit routing-instances]\nroot@PE2_JUN# show\n20 {\n    description \"Juniper Site B\";\n    instance-type vrf;\n    interface ge-0/0/1.0;\n    route-distinguisher 10.10.10.3:20;\n    vrf-target target:200:20;\n    protocols {\n        bgp {\n            group toCE {\n                local-address 10.20.99.2;\n                peer-as 200;\n                as-override;\n                neighbor 10.20.99.3;\n            }\n        }\n    }\n}\n30 {\n    description \"Alcatel Site B\";\n    instance-type vrf;\n    interface ge-0/0/2.0;\n    route-distinguisher 10.10.10.3:30;\n    vrf-target target:300:30;\n    routing-options {\n        router-id 100.100.100.100;\n    }\n    protocols {\n        ospf {\n            export MP-BGP_to_CE_via_OSPF;\n            area 0.0.0.0 {\n                interface ge-0/0/2.0 {\n                    interface-type p2p;\n                }\n            }\n        }\n    }\n}\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#ce-pe-routing-protocol-configuration","title":"CE -&gt; PE routing protocol configuration","text":"<p>Now it is time to connect our customers to the service provider's network via VRFs created earlier and finally add some VPN routes. CE routers completely unaware of a complex L3VPN configuration on the PE routers, what they need to do is just setup a routing protocol over which customers routes could be delivered to (and received from) the Service Provider.</p> <p>Starting with Juniper CE1_JUN and CE2_JUN that run eBGP with PE routers:</p> <p>CE1_JUN:</p> <pre><code>root@CE1_JUN# show \n### Last changed: 2015-10-28 17:40:05 UTC\nversion 14.1R1.10;\n&lt;... omitted ...&gt;\n\n### 1. configure PE-facing interface\ninterfaces {\n    ge-0/0/0 {\n        mac 50:01:00:06:00:02;\n        unit 0 {                        \n            family inet {\n                address 10.20.99.1/31;\n            }\n        }\n    }\n### 2. dont forget about loopback, this address will be exported to remote site\n    lo0 {\n        unit 0 {\n            family inet {\n                address 1.1.1.1/32;\n            }\n        }\n    }\n}\n\n### 3. AS number is a mandatory for eBGP session to run\nrouting-options {\n    autonomous-system 200;\n}\n\n### 4. and a simple eBGP configuration\nprotocols {\n    bgp {\n        group toPE {\n            local-address 10.20.99.1;\n\n            ## 5. we need to export loopback address to eBGP\n            export export_loopback;\n            peer-as 100;\n            neighbor 10.20.99.0;        \n        }\n    }\n}\n\n### 6. policy to export loopback address\npolicy-options {\n    prefix-list loopback {\n        1.1.1.1/32;\n    }\n    policy-statement export_loopback {\n        term Loopback {\n            from {\n                prefix-list loopback;\n            }\n            then accept;\n        }\n    }\n}\n</code></pre> <p>CE2_JUN:</p> <pre><code>root@CE2_JUN# show \n### Last changed: 2015-10-28 16:50:23 UTC\nversion 14.1R1.10;\n\n### 1. configure PE-facing interface\ninterfaces {\n    ge-0/0/0 {\n        unit 0 {\n            family inet {               \n                address 10.20.99.3/31;\n            }\n        }\n    }\n### 2. dont forget about loopback, this address will be exported to remote site\n    lo0 {\n        unit 0 {\n            family inet {\n                address 2.2.2.2/32;\n            }\n        }\n    }\n}\n\n### 3. AS number is a mandatory for eBGP session to run\nrouting-options {\n    autonomous-system 200;\n}\n\n### 4. and a simple eBGP configuration\nprotocols {\n    bgp {\n        group toPE {\n            local-address 10.20.99.3;\n            family inet {\n                unicast;\n            }\n\n            ## 5. we need to export loopback address to eBGP\n            export export_loopback;     \n            peer-as 100;\n            neighbor 10.20.99.2;\n        }\n    }\n}\n\n### 6. policy to export loopback address\npolicy-options {\n    prefix-list loopback {\n        2.2.2.2/32;\n    }\n    policy-statement export_loopback {\n        term Loopback {\n            from {\n                prefix-list loopback;\n            }\n            then accept;\n        }\n    }\n}\n</code></pre> <p>Now its Nokia time. Pay attention to the CE2_ALU router, since we are using OSPF on CE2-PE2 link configuration it is a little bit different from other CE's configs.</p> <p>CE1_ALU:</p> <pre><code>A:CE1_ALU&gt;config&gt;router# info\n----------------------------------------------\n\n### 1. Interfaces and AS Num config\n\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 1.1.1.1/32\n            no shutdown\n        exit\n        interface \"toPE\"\n            address 10.30.99.1/31\n            port 1/1/1\n            no shutdown\n        exit\n        autonomous-system 300\n\n### 2. Policy for exporting loopback address\n#--------------------------------------------------\necho \"Policy Configuration\"\n#--------------------------------------------------\n        policy-options\n            begin\n            prefix-list \"loopback\"\n                prefix 1.1.1.1/32 exact\n            exit\n            policy-statement \"export_loopback\"\n                entry 10\n                    from\n                        prefix-list \"loopback\"\n                    exit\n                    action accept\n                    exit\n                exit\n            exit\n            commit\n        exit\n#--------------------------------------------------\necho \"BGP Configuration\"\n#--------------------------------------------------\n        bgp\n            group \"toPE\"\n                export \"export_loopback\" ## tell CE to export its system address to eBGP peer\n                peer-as 100\n                local-address 10.30.99.1\n                split-horizon\n                neighbor 10.30.99.0\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre> <p>CE2_ALU:</p> <pre><code>A:CE2_ALU&gt;config&gt;router# info\n----------------------------------------------\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n        interface \"system\"\n            address 2.2.2.2/32\n            no shutdown\n        exit\n        interface \"toPE\"\n            address 10.30.99.3/31\n            port 1/1/1\n            no shutdown\n        exit\n        autonomous-system 300\n\n### we running OSPF as PE-CE protocol\n#--------------------------------------------------\necho \"OSPFv2 Configuration\"\n#--------------------------------------------------\n        ospf\n            area 0.0.0.0\n                interface \"system\"\n                    no shutdown\n                exit\n                interface \"toPE\"\n                    interface-type point-to-point\n                    no shutdown\n                exit\n            exit\n            no shutdown\n        exit\n----------------------------------------------\n</code></pre>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#control-plane-walkthrough","title":"Control plane walkthrough","text":"<p>We are done with the configuration, all in all it was not a complex task, what is more important is to understand what's going on with control and data planes. I believe you will like this step-by-step walkthrough via every node in the network.</p> <p>I will start with dissection of a control plane operation from the point where MP-BGP session between PE routers has been already established and we are enabling VRFs on customers routers. Refer to this overview chart and see how an information about CE1_JUN's loopback interface propagates through the entire network to CE2_JUN counterpart:</p> <p>All the pictures are clickable, to see the full sized pics choose \"open an image in a separate tab\" option in your browser.</p> <p></p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-1","title":"Step 1","text":"<p>CE1_JUN router has an export policy <code>export_loopback</code> configured which is used by BGP to construct the BGP UPDATE message with <code>lo0</code> prefix as an NLRI.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-2","title":"Step 2","text":"<p>CE1_JUN sends a regular BGP UPDATE message to its eBGP peer PE1_ALU.</p> <p></p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-3","title":"Step 3","text":"<p>PE1_ALU router receives this update via its interface <code>toCE1</code> configured in <code>vprn 20</code> context. PE1_ALU populates its VRF 20 with a route to <code>1.1.1.1/32</code> via <code>10.20.99.1</code>.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-4","title":"Step 4","text":"<p>PE1_ALU router has an established MP-iBGP session with PE2_JUN so it takes a BGP route from VRF 20 and automatically sends an MP-BGP UPDATE message to its peer. Note, that ALU routers will send MP-BGP update automatically only for the connected to VRF routes and the routes received via BGP. If we had OSPF between CE1 and PE1, we would need to configure an export policy to propagate this update over MP-BGP session.</p> <p>Since PE1_ALU router wants to send an update for a route in the VRF it should construct an MP-BGP Update message which has a specific Path attribute - MP_REACH_NLRI - to communicate this routing information. And PE1_ALU will transform the <code>1.1.1.1/32</code> IPv4 prefix to an VPN-IPv4 one.</p> <p></p> <p>Take a closer look at this BGP message. See how PE1_ALU router added some valuable additional information to correctly pass CE1_ALU's loopback address via MP-BGP. First of all examine how NLRI has been transformed in MP-BGP: it now has a Route Distinguisher which we configured for VRF 20 earlier, it has the IPv4 prefix itself and it has the MPLS label <code>131068</code>.</p> <p>PE1_ALU router allocated a VPN label which it associated with the VRF 20. This label tells PE1_ALU router that if it ever receives a data packet with this label it should associate the data encapsulated within it with VRF 20! This way ingress PE routers tell others PEs what label should be used as a VPN label for the routes residing in a particular VRF.</p> <p>There are two methods of allocating the VPN labels (they are also called Service labels):</p> <ol> <li>per VRF: all routes originated from a single VRF will have the same VPN label. SROS routers default to this.</li> <li>per VRF per next-hop: If a VRF has &gt;1 CE interfaces, PE router will allocate different labels for different CE interfaces inside one VRF. Juniper routers default to this.</li> </ol> <p>If we zoom over the Extended Community attribute of the BGP UPDATE message, we can spot the Route Target <code>200:20</code> value there.</p> <p>Important things happened to the Next-Hop, not only it looks now like a VPN-IPv4 route with a Route Distinguisher value of <code>0:0</code> and without MPLS label, but Next-Hop IPv4 address has been changed to PE1_ALU's system (loopback) interface <code>10.10.10.1</code>. This is how PE1 router tells PE2 that it can reach VRF 20 routes via PE1.</p> <p>In the end of the day, PE1_ALU's update reaches PE2_JUN since it has the IP destination address of 10.10.10.3.</p> <p>Notice, that BGP updates traverse Service Provider's network in a form of the simple IP packets, MPLS is out of the picture at this moment. Service Provider's core router - P1_ALU - simply routes IP packets and has no take in BGP at all.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-5","title":"Step 5","text":"<p>PE2_JUN receives the BGP UPDATE with VPN-IPv4 route. Once this route passes validation checks (Nexhop resolvable, no AS Path loop) PE2 submits this route to a specific table named <code>bgp.l3vpn.0</code>. This table stores all BGP VPN routes, refer to this figure to examine some of its content:</p> <p></p> <p>PE2 extracts the routing information from this update an based on the Route Target value installs the IPv4 route <code>1.1.1.1/32</code> into the VRF 20 table - <code>20.inet.0</code>. PE2 resolves the next-hop address of the fellow PE1_ALU (10.10.10.1) via MPLS Label Switched Path (LSP) and stores this information in the <code>20.inet.0</code> table:</p> <pre><code>20.inet.0: 5 destinations, 5 routes (5 active, 0 holddown, 0 hidden)\n+ = Active Route, - = Last Active, * = Both\n\n1.1.1.1/32         *[BGP/170] 2d 08:38:15, localpref 100, from 10.10.10.1\n                      AS path: 200 I, validation-state: unverified\n                    &gt; to 10.99.99.2 via ge-0/0/0.0, label-switched-path toPE1\n</code></pre> <p>Remember that it is mandatory to have an active LSP to the remote PE, since we have to have an MPLS transport to the remote end to carry the data packets.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-6","title":"Step 6","text":"<p>Since we installed the route for the <code>1.1.1.1/32</code> IPv4 prefix into VRF 20 and we have an active eBGP peer in VRF 20, we should send an update for this IPv4 prefix to the CE2_JUN router to let the CE2 site to be aware of the remote prefix. This update goes as an ordinary eBGP update.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-7","title":"Step 7","text":"<p>CE2_JUN receives the BGP UPDATE and installs a route into the only table it has for IPv4 routes - <code>inet.0</code>.</p> <p>This completes Control Plane operation regarding the prefix <code>1.1.1.1/32</code>, same process goes for the other loopbacks and connected to VRFs link addresses for both Alcatel and Juniper customers.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#data-plane-walkthrough","title":"Data plane walkthrough","text":"<p>To complete this post we should examine the data plane operations. We will see how data packets destined to <code>1.1.1.1</code> propagate through the network using the labels allocated during the control plane operations.</p> <p></p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-1_1","title":"Step 1","text":"<p>CE2_JUN wants to send a data packet to CE1_JUN via L3VPN service provided by our network. CE2 has an active route in its route table <code>inet.0</code> that says that it can reach <code>1.1.1.1/32</code> via <code>10.20.99.2</code> address via the <code>ge-0/0/0</code> interface. CE2 has a MAC address for <code>10.20.99.2</code> so it constructs the whole frame and puts it on the wire.</p> <p></p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-2_1","title":"Step 2","text":"<p>PE2_JUN receives the Ethernet frame on its interface <code>ge-0/0/1</code> which belongs to VRF 20, that is how PE2 decides to associate this packet with VRF 20. PE2 consults with the VRF 20 routing table and sees that it has to use the LSP <code>toPE1</code> to send the incoming data packet further. Then PE2 gets MPLS label which it received earlier from its RSVP neighbor P1_ALU during the LSP signalization process.</p> <p></p> <p>But this was just a transport MPLS label, it helps PE2_JUN to reach PE1_ALU, but PE2 needs one label more - the VPN Label - to tell PE1_ALU to which VRF this data belongs. This label was signalled earlier (see Control Plane operation section) via MP-BGP.</p> <p>Now PE2 has everything it needs:</p> <ol> <li>MPLS VPN label to encapsulate the data packets from its VRF 20 destined to the VRF 20 on PE1_ALU</li> <li>Transport MPLS Label to get to PE1_ALU via MPLS core</li> </ol> <p>and thus it constructs a packet with two labels stacked and fires it off.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-3_1","title":"Step 3","text":"<p>P1_ALU is totally unaware of the whole services and customers mess, it just switches MPLS packets by replacing the incoming transport label with the outgoing one.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-4_1","title":"Step 4","text":"<p>PE1_ALU receives an MPLS packet from P1_ALU. It pops out the transport label (fig. 4.1) and examines the enclosed MPLS label. This label value <code>131068</code> was signalled by PE1_ALU via MP-BGP during the Control Plane operation. So PE1 knows that it has to pop this label and associate the enclosed packet with the VPRN 20 (VRF 20) (fig. 4.2)</p> <p></p> <p>VRF's 20 routing table says that packets destined to <code>1.1.1.1</code> should be forwarded to <code>10.20.99.3</code> address (fig. 4.3), which is a connected network leading to CE1_JUN (fig. 4.4). PE1_ALU constructs the packet and moves it via Ethernet out of the 1/1/2 port (fig. 4.5).</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2015/basic-l3vpn-bgpmpls-vpn-or-vprn-configuration-on-nokia-alcatel-lucent-sros--juniper-mx/#step-5_1","title":"Step 5","text":"<p>CE2_JUN receives an ordinary IP packet with a destination address matching its interface. It decapsulates ICMP echo request and sends back the echo reply.</p> <p>This concludes the control and data plane operations walk through. If you followed along the explanations and practiced the configuration steps, you should be in a good shape to implement the basic L3VPN services and also should have a pretty solid understanding of the service establishment mechanics.</p>","tags":["nokia","juniper","l3vpn","vprn","bgp"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/","title":"Building Web front end for Python scripts with Flask","text":"<p>Recently I revived my relationship with Python in an effort to tackle the routine tasks appearing here and there. So I started to write some pocket scripts and, luckily, was not the only one on this battlefield - my colleagues also have a bunch of useful scripts. With all those code snippets sent in the emails, cloned from the repos, grabbed on the network shares... I started to wonder how much easier would it be if someone had them all aggregated and presented with a Web UI for a shared access?</p> <p>Thus, I started to build web front-end to the python scripts we used daily with these goals in mind:</p> <ul> <li>allow people with a zero knowledge of Python to use the scripts by interacting with them through a simple Web UI;</li> <li>make script's output more readable by leveraging CSS and HTML formatting;</li> <li>aggregate all the scripts in one a single repo but in a separate sandboxed directories to maintain code manageability.</li> </ul> <p>This short demo should give you some taste of what it is:</p> <p></p> <p>Disclaimer: I am nowhere near even a professional python or web developer. And what makes it even worse is that I used (a lot) a very dangerous coding paradigm - SDD - Stack Overflow Driven Development. So, hurt me plenty if you see some awful mistakes.</p> <p>Project source code</p> <p>PLAZA (this is the name I gave this project) implements a straightforward user experience flow: a user opens a web page, selects a script from the menu, fills in the necessary input data and run a script to get the results back.</p> <p></p> <p>By hitting <code>submit</code> data goes to the back-end part, where the chosen python script does it's noble job and produces some data. This data gets pushed back to the browser and as displayed to a user.</p> <p>Obviously, one will need some front-end technologies to build the web layer and some back-end to process the incoming data.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#tools-technologies","title":"Tools &amp; Technologies","text":"","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#front-end","title":"Front-end","text":"<p>To build a fairly fresh-looking (fresh as in 2016yr), dynamic web view we need a web framework to leverage. I used Bootstrap package (CSS and JS) as it is well documented and have tons of implementations and examples.</p> <p>What tastes good with Bootstrap - JQuery, of course. JQuery was used to handle AJAX response/request messages between the front-end and the back-end without reloading the whole page. Since I had no previous experience with both of these technologies, I heavily used everything google served me. Here is my list of useful resources I found noteworthy:</p> <ol> <li>Layoutit.com - there you can create Bootstrap grid and play with elements in a drag and drop fashion. Load the result in a zip file and your grid system is almost ready.</li> <li>Bootply.com - visual builder for Bootstrap layout. It has some good examples which cover basic Bootstrap elements behavior (navbar, grid rules, etc).</li> <li>Form validator by 1000hz - well, it's a form validator. And since every script needs to get input data from a user, form validation is a must-have for a sleek user experience.</li> <li>Bootsnipp.com - crowdsource collection of snippets written with Bootstrap. I grabbed my side menu from it. Another useful section from this site is Form Builder.</li> <li>Formden - another form builder.</li> </ol>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#back-end","title":"Back-end","text":"<p>The heavy lifting in the back is done by the gorgeous Flask, which is a micro framework for writing web applications. It includes a web-server, Jinja2 templating engine and lots of features to make back-end easy even for dummies like me.</p> <p>As to the Flask related resources I cherry-picked the following:</p> <ol> <li>Famous Flask Mega Tutorial by Miguel Grinberg</li> <li>Discover Flask - A full season of youtube videos from Michael Herman</li> <li>Official documentation of course!</li> <li>Another good post on AJAX+Flask interaction from giantflyingsaucer.com</li> </ol>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#project-structure-overview","title":"Project structure overview","text":"<p>Having frameworks set and tools figured out I started to outline the project's high-level structure.</p> <p>Flask maintains a simple yet flexible project structure. In my case, I didn't deviate far away from a basic setup, since the overall simplicity is one of the project's objectives.</p> <pre><code>\u251c\u2500\u2500 app.py        # Flask application\n\u251c\u2500\u2500 config.py     # Flask configuration\n\u251c\u2500\u2500 .env          # env variables for dev/prod environments\n\u251c\u2500\u2500 scripts_bank  # directory to store all python scripts we're going to use via Web\n\u251c\u2500\u2500 static        # static data for Bootstrap CSS, JS, custom fonts, etc\n\u2502   \u251c\u2500\u2500 css\n\u2502   \u251c\u2500\u2500 fonts\n\u2502   \u2514\u2500\u2500 js\n\u251c\u2500\u2500 templates     # HTML templates used to render pages&lt;/pre&gt;\n</code></pre> <p>Although the comments above give enough information about the structure, let's go into details a bit</p> <ol> <li>Flask application - <code>app.py</code> - is an entry point for the whole project. It starts the web-server, loads the <code>routes</code> (aka links to the pages of your web project) and plugs in python scripts stored in the <code>scripts_bank</code> directory.</li> <li>As every other app, Flask app should be configured differently for development and production. This is done via the <code>config.py</code> and the environment variables <code>.env</code> file.</li> <li>In the <code>static</code> directory you normally store your CSS, JS, pictures, custom fonts. So did I.</li> <li>HTML pages are in the <code>templates</code> directory.</li> <li>And the pythonic scripts with all the relevant files (unique HTML templates for input forms, additional front-end Javascript code, etc) are living inside the <code>scripts_bank</code> directory.</li> </ol>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#configuring-flask","title":"Configuring Flask","text":"<p>Once you have Flask installed and got familiar with its basics (either through official quick start guide or tons of tutorials) it is time to configure it. There are several ways to configure a Flask application. The basic one is to specify the configuration statements as the arguments to your <code>app</code> instance:</p> <pre><code>app = Flask(__name__)\n\n## pass secret_key and SQLAlchemy params\napp.secret_key = 'test'\napp.config[SQLALCHEMY_DATABASE_URI] = 'sqlite:///db/sql.db'\n\nif __name__ == '__main__':\n    app.run(debug=True)     ## pass DEBUG param&lt;/pre&gt;\n\nA bit more advanced way is to specify all the config parameters in uppercase in your `app.py` and tell the `app` instance to get config from this file:\n\n```DEBUG = True\nSECRET_KEY = 'yekterces'\nSQLALCHEMY_DATABASE_URI = 'sqlite:///db/sql.db'\n\napp = Flask(__name__)\napp.config.from_object(__name__)   # get config from this module&lt;/pre&gt;\n</code></pre> <p>But the methods discussed so far can't let you have different configurations for Dev and Prod environments (which you'd want to have eventually).</p> <p>When I was choosing the configuration method for this app I followed a path which consists of these three key points:</p> <ol> <li>creating configuration classes for different environments using inheritance (explained here)</li> <li>choosing the right configuration class based on the current value of the environment variable</li> <li>storing environment variables in a file (<code>.env</code>) and parsing its contents for parameters (more here)</li> </ol> <p>Detailed explanation of Flask app configuration</p> <p>Going from bottom to top, <code>.env</code> is a file, which stores application parameters in a way like classic environment variables do.</p> <pre><code># This file is used to store configuration settings for\n# Dev and Prod environments. PLAZA_SETTINGS value is used by app.py to\n# properly detect which configuration class to use\n\n# uncomment/modify desired section prior to use\n\n# dev\nPLAZA_SETTINGS = config.Development \n\n# prod\n# PLAZA_SETTINGS = config.Production\n# HOST = 0.0.0.0&lt;/pre&gt;\n</code></pre> <p>Then Flask application initializes and gets configuration from a class, stored in <code>PLAZA_SETTINGS</code> variable:</p> <pre><code>from flask import Flask, render_template\nimport os\nimport config\n\nroot_folder_path = os.path.dirname(os.path.abspath(__file__))\n\n# get env_settings list\nenv_settings = config.EnvironmentSettings(root_folder_path)\n\n# initialize Flask app\napp = Flask(__name__)\n\n# configure Flask app from a class, stored in PLAZA_SETTINGS variable\napp.config.from_object(env_settings['PLAZA_SETTINGS'])\n\n\nif __name__ == '__main__':\n   # if we are in Prod, use HOST and PORT specified\n   try:\n       app.run(host=str(env_settings['HOST']), port=80)\n   except config.ConfigurationError:\n       app.run()&lt;/pre&gt;\n</code></pre> <p>Functions subject to configuration along with configuration classes are stored in the <code>config.py</code> file:</p> <pre><code>import os\n\n\n# default config class\nclass Base(object):\n    DEBUG = False\n    SECRET_KEY = 'your_secret'\n\n\nclass Development(Base):\n    DEBUG = True\n\n\nclass Production(Base):\n    DEBUG = False\n\n\nclass EnvironmentSettings:\n    \"\"\"\n    Access to environment variables via system os or .env file for different environments (Prod vs Dev)\n    \"\"\"\n    def __init__(self, root_folder_path):\n        self._root_folder_path = root_folder_path\n\n    def __getitem__(self, key):\n        return self._get_env_variable(key)\n\n    def __setitem__(self, key, value):\n        raise InvalidOperationException('Environment Settings are read-only')\n\n    def __delitem__(self, key):\n        raise InvalidOperationException('Environment Settings are read-only')\n\n    def _get_env_variable(self, var_name, default=False):\n        \"\"\"\n        Get the environment variable or return exception\n        :param var_name: Environment Variable to lookup\n        \"\"\"\n        try:\n            return os.environ[var_name]\n        except KeyError:\n            from io import StringIO\n            from configparser import ConfigParser\n\n            env_file = os.environ.get('PROJECT_ENV_FILE', self._root_folder_path + \"/.env\")\n            try:\n                config = StringIO()\n                config.write(\"[DATA]\\n\")\n                config.write(open(env_file).read())\n                config.seek(0, os.SEEK_SET)\n                cp = ConfigParser()\n                cp.read_file(config)\n                value = dict(cp.items('DATA'))[var_name.lower()]\n                if value.startswith('\"') and value.endswith('\"'):\n                    value = value[1:-1]\n                elif value.startswith(\"'\") and value.endswith(\"'\"):\n                    value = value[1:-1]\n                os.environ.setdefault(var_name, value)\n                return value\n            except (KeyError, IOError):\n                if default is not False:\n                    return default\n                error_msg = \"Either set the env variable '{var}' or place it in your \" \\\n                            \"{env_file} file as '{var} = VALUE'\"\n                raise ConfigurationError(error_msg.format(var=var_name, env_file=env_file))\n\n\nclass ConfigurationError(Exception):\n    pass\n\n\nclass InvalidOperationException(Exception):\n    pass\n</code></pre>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#setting-up-front-end","title":"Setting up front-end","text":"<p>Good, Flask app has been configured and is ready to render some pages, so let's go and prepare out front-end to display projects' web pages. Download Bootstrap, JQuery, Fontawesome and store theirs minified <code>min.css</code> and <code>min.js</code> artifacts in the <code>static</code> directory of the project. This is how it should look like:</p> <pre><code>\u251c\u2500\u2500 static\n\u2502   \u251c\u2500\u2500 css\n\u2502   \u2502   \u251c\u2500\u2500 bootstrap.min.css\n\u2502   \u2502   \u251c\u2500\u2500 font-awesome.min.css\n\u2502   \u2502   \u2514\u2500\u2500 style.css  # custom styles css for every page\n\u2502   \u251c\u2500\u2500 fonts\n\u2502   \u2502   \u251c\u2500\u2500 FontAwesome.otf\n\u2502   \u2502   \u251c\u2500\u2500 NokiaPureHeadline_ExtraBold.ttf  # custom fonts like this also live here\n\u2502   \u2502   \u251c\u2500\u2500 fontawesome-webfont.eot\n\u2502   \u2502   \u251c\u2500\u2500 fontawesome-webfont.svg\n\u2502   \u2502   \u251c\u2500\u2500 fontawesome-webfont.ttf\n\u2502   \u2502   \u251c\u2500\u2500 fontawesome-webfont.woff\n\u2502   \u2502   \u251c\u2500\u2500 fontawesome-webfont.woff2\n\u2502   \u2502   \u251c\u2500\u2500 glyphicons-halflings-regular.eot\n\u2502   \u2502   \u251c\u2500\u2500 glyphicons-halflings-regular.svg\n\u2502   \u2502   \u251c\u2500\u2500 glyphicons-halflings-regular.ttf\n\u2502   \u2502   \u251c\u2500\u2500 glyphicons-halflings-regular.woff\n\u2502   \u2502   \u2514\u2500\u2500 glyphicons-halflings-regular.woff2\n\u2502   \u2514\u2500\u2500 js\n\u2502       \u251c\u2500\u2500 bootstrap.min.js\n\u2502       \u251c\u2500\u2500 jquery-2.2.0.min.js\n\u2502       \u251c\u2500\u2500 loadingoverlay.min.js  # CSS for overlay animation. \n\u2502       \u251c\u2500\u2500 scripts.js             # custom JS scripts \n\u2502       \u2514\u2500\u2500 validator.min.js       # form validation JS code\n</code></pre>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#layout","title":"Layout","text":"<p>Before diving into HTML it is advised to think about pages layout. I recommend you to get familiar with Bootstrap CSS rules and choose a layout that fits your project. I decided to go with a 3+9 scheme. Three parts are for side menu and nine parts are for a content area with a navigation bar at the top of the page.</p> <p>I composed a sketch of the page depicting how I would like to see my projects web view for an arbitrary script:</p> <p></p> <p>Follow the link to see my script's page template on codepen and see how things interact. Do not worry if you can't pick rock solid layout right now, you will be able to modify it on-the-fly and decide what suits your needs better.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#flask-routes-templates","title":"Flask routes &amp; templates","text":"","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#routes","title":"Routes","text":"<p>Flask uses routes to create URL's for the web pages. If we need to show the main page for example for the URL <code>abc.com</code> we need to define the root route - <code>/</code> - like this:</p> <pre><code>@app.route('/')\ndef index():\n    return 'Index Page'\n</code></pre> <p>This will effectively bind the <code>index()</code> function to the route <code>/</code> , so when a user navigates to the application's root it will trigger the <code>index()</code> function.</p> <pre><code>@app.route('/')\ndef index():\n    return render_template('index.html')\n</code></pre> <p>My <code>index()</code> function does one simple thing, it asks Flask to render specific template - <code>index.html</code>.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#templates","title":"Templates","text":"<p>You might guess that a template has to do something with the HTML content rendered by a browser. Yes, it has, but it is far more powerful than a static HTML file.</p> <p>Generating HTML from within Python is not fun, and actually pretty cumbersome because you have to do the HTML escaping on your own to keep the application secure. Because of that Flask configures the Jinja2 template engine for you automatically.</p> <p>So Flask's template is a Jinja2-based template which allows you to build dynamic web-pages instead of a static content. To render a template you can use the <code>render_template()</code> method. All you have to do is to provide the name of the template and the variables you want to pass to the template engine.</p> <p>You can name your templates as you like, but normally it will have an <code>.html</code> extension to reflect their purpose. This is my <code>index.html</code> template mentioned earlier bound to the route <code>/</code>.</p> <pre><code>{% extends 'base.html' %}\n{% block content %}\n &lt;h2&gt; Welcome to &lt;span class=\"text-primary\"&gt;PLAZA&lt;/span&gt;. &lt;small&gt;front-end for python scripts we used to run from console&lt;/small&gt;&lt;/h2&gt;\n &lt;h3 class=\"text-primary\"&gt;\n  What is PLAZA?\n &lt;/h3&gt;\n &lt;p &gt;\n  PLAZA is a web front-end to python scripts built with these goals in mind:\n  &lt;ul&gt;\n   &lt;li&gt;allow people with zero python knowledge to use the scripts by interaction through simple Web GUI;&lt;/li&gt;\n   &lt;li&gt;beautify scripts' output with modern CSS and HTML formatting;&lt;/li&gt;\n   &lt;li&gt;aggregate all the scripts in one repo but in a separate sandboxed directories to increase code manageability.&lt;/li&gt;\n  &lt;/ul&gt;\n &lt;/p&gt;\n&lt;h3 class=\"text-primary\"&gt;\n  How to use?\n &lt;/h3&gt;\n &lt;p &gt;\n  Navigate through the side menu to the desired script and follow the instructions.\n &lt;/p&gt;\n&lt;h3 class=\"text-primary\"&gt;\n  Contacts\n &lt;/h3&gt;\n &lt;p &gt;\n  Have any ideas, questions, problems? Visit &lt;a href=\"/contacts\"&gt;contacts&lt;/a&gt; page for all the details.\n &lt;/p&gt;\n{% endblock %}\n</code></pre> <p>And this is how it gets rendered:</p> <p></p> <p>Dynamic version of the index page can be found on the codepen as well. The trick behind that magic <code>template-&gt;rendered page</code> transformation is in the first two lines. This is template inheritance magic - <code>{% extends 'base.html' %}</code> - and that is what makes templating so powerful.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#template-inheritance","title":"Template inheritance","text":"<p>Inheritance drill described briefly in the official documentation and the main part of it sounds like this:</p> <p>Template inheritance allows you to build a base \u201cskeleton\u201d template that contains all the common elements of your site and defines blocks that child templates can override.</p> <p>Apart from the official docs, you can watch this video from the \u201cDiscover Flask\u201d series to better understand how does template inheritance work.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#main-template","title":"Main template","text":"<p>One of the best practices regarding template inheritance is to compose a <code>base template</code> or a layout for the whole site so every other template will inherit from it. My \"main\" template is called <code>base.html</code> and it describes the logical parts for each page in this project.</p> <p></p> <p>The main template consists of the static parts like Navbar, side menu, it also connects core CSS, JS and fonts. And finally, it specifies where would child template's content be placed.</p> <pre><code>&lt;!DOCTYPE HTML&gt;\n&lt;HTML lang=\"en\"&gt;\n    &lt;head&gt;\n        &lt;meta charset=\"utf-8\"&gt;\n        &lt;meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge\"&gt;\n        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1\"&gt;\n        &lt;title&gt;PLAZA Project&lt;/title&gt;\n        &lt;meta name=\"description\" content=\"NOKIA Launchpad for scripts\"&gt;\n        &lt;meta name=\"author\" content=\"Roman Dodin\"&gt;\n        &lt;link href=\"/static/css/bootstrap.min.css\" rel=\"stylesheet\"&gt;\n        &lt;link href=\"/static/css/style.css\" rel=\"stylesheet\"&gt; &lt;!-- custom CSS --&gt;\n        &lt;link href=\"/static/css/font-awesome.min.css\" rel=\"stylesheet\"&gt;\n    &lt;/head&gt;\n    &lt;body&gt;\n        &lt;!-- NAV BAR --&gt;\n        &lt;div class=\"navbar navbar-nokia navbar-fixed-top\"&gt;\n            &lt;div class=\"container\"&gt;\n                &lt;div class=\"navbar-header\"&gt;\n                    &lt;button type=\"button\" class=\"navbar-toggle\" data-toggle=\"collapse\" data-target=\".navbar-collapse\"&gt;\n                        &lt;span class=\"icon-bar\"&gt;&lt;/span&gt;\n                    &lt;/button&gt;\n                    &lt;a class=\"navbar-brand\" href=\"/\"&gt;PLAZA&lt;/a&gt;\n                &lt;/div&gt;\n                &lt;div class=\"collapse navbar-collapse\"&gt;\n                    &lt;ul class=\"nav navbar-nav\"&gt;\n\n                        &lt;li&gt;\n                            &lt;a href=\"#contact\"&gt;Contact&lt;/a&gt;\n                        &lt;/li&gt;\n                        &lt;li&gt;\n                           &lt;a href=\"#modalSearch\" data-toggle=\"modal\" data-target=\"#modalSearch\"&gt;\n                               Search &lt;span id=\"searchGlyph\" class=\"glyphicon glyphicon-search\"&gt;&lt;/span&gt;\n                           &lt;/a&gt;\n                       &lt;/li&gt;\n                    &lt;/ul&gt;\n                &lt;/div&gt;\n                &lt;!--/.nav-collapse --&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n        &lt;!-- NAV BAR --&gt;\n\n\n        &lt;!-- Search Modal (http://www.w3schools.com/bootstrap/bootstrap_modal.asp) --&gt;\n        &lt;div id=\"modalSearch\" class=\"modal fade\" role=\"dialog\"&gt;\n           &lt;div class=\"modal-dialog modal-lg\"&gt;\n\n               &lt;!-- Modal content--&gt;\n               &lt;div class=\"modal-content\"&gt;\n                   &lt;div class=\"modal-header\"&gt;\n                       &lt;button type=\"button\" class=\"close\" data-dismiss=\"modal\"&gt;&amp;times;&lt;/button&gt;\n                       &lt;h4 class=\"modal-title\"&gt;Search PLAZA (under development)&lt;/h4&gt;\n                   &lt;/div&gt;\n                   &lt;div class=\"modal-body\"&gt;\n                       &lt;!-- Add the modal body here --&gt;\n                   &lt;/div&gt;\n                   &lt;div class=\"modal-footer\"&gt;\n                       &lt;button type=\"button\" class=\"btn btn-default\" data-dismiss=\"modal\"&gt;Close&lt;/button&gt;\n                   &lt;/div&gt;\n               &lt;/div&gt;\n           &lt;/div&gt;\n        &lt;/div&gt;\n\n\n        &lt;div class=\"container\"&gt;\n            &lt;div class=\"row\"&gt;\n                &lt;div class=\"col-md-3\"&gt;\n                  &lt;div class=\"nav-side-menu\"&gt;\n                    &lt;div class=\"brand\"&gt;MENU&lt;/div&gt;\n                    &lt;i class=\"fa fa-bars fa-3x toggle-btn\" data-toggle=\"collapse\" data-target=\"#menu-content\"&gt;&lt;/i&gt;\n\n                    &lt;!-- START OF SIDE MENU AREA--&gt;\n                    &lt;div class=\"menu-list\"&gt;\n                      &lt;ul id=\"menu-content\" class=\"menu-content collapse out\"&gt;\n                          &lt;!-- PLACEHOLDER FOR ELEMENT W/O CHILDREN\n                          &lt;li&gt;\n                              &lt;a href=\"#\"&gt;\n                                  &lt;i class=\"fa fa-angle-right fa-fw\"&gt;&lt;/i&gt;Element wo children\n                              &lt;/a&gt;\n                          &lt;/li&gt;\n                          --&gt;\n\n                          &lt;li data-toggle=\"collapse\" data-target=\"#5620sam\" class=\"collapsed\"&gt;\n                              &lt;a href=\"#\"&gt;\n                                  &lt;i class=\"fa fa-angle-double-right fa-fw chevron-rotate\"&gt;&lt;/i&gt;5620 SAM\n                              &lt;/a&gt;\n                          &lt;/li&gt;\n                              &lt;ul class=\"sub-menu collapse out\" id=\"5620sam\"&gt;\n                                  &lt;li&gt;\n                                      &lt;i class=\"fa fa-angle-right fa-fw\"&gt;&lt;/i&gt;\n                                      &lt;a href=\"/5620SAM/SAM-O_XML_API_Tester\"&gt;\n                                          SAM-O XML API Test Engine\n                                      &lt;/a&gt;\n                                  &lt;/li&gt;\n                              &lt;/ul&gt;\n\n                          &lt;li data-toggle=\"collapse\" data-target=\"#vmware\" class=\"collapsed\"&gt;\n                              &lt;a href=\"#\"&gt;\n                                  &lt;i class=\"fa fa-angle-double-right fa-fw chevron-rotate\"&gt;&lt;/i&gt;VMWare\n                              &lt;/a&gt;\n                          &lt;/li&gt;\n                              &lt;ul class=\"sub-menu collapse out\" id=\"vmware\"&gt;\n                                  &lt;li&gt;\n                                      &lt;i class=\"fa fa-angle-right fa-fw\"&gt;&lt;/i&gt;\n                                      &lt;a href=\"/vmware/get_vmrc_links\"&gt;\n                                          VMRC Link Composer\n                                      &lt;/a&gt;\n                                  &lt;/li&gt;\n\n                                  &lt;li data-toggle=\"collapse\" data-target=\"#products2\" class=\"collapsed\"&gt;\n                                      &lt;i class=\"fa fa-angle-double-right fa-fw chevron-rotate\"&gt;&lt;/i&gt;\n                                      &lt;a href=\"#\"&gt;Placeholder-sublevel1\n                                      &lt;/a&gt;\n                                  &lt;/li&gt;\n                                      &lt;ul class=\"sub-sub-menu collapse out\" id=\"products2\"&gt;\n                                          &lt;li class=\"sub-level2\"&gt;\n                                              &lt;i class=\"fa fa-angle-right fa-fw\"&gt;&lt;/i&gt;\n                                              &lt;a href=\"#\"&gt;\n                                                  Placeholder-sublevel2\n                                              &lt;/a&gt;\n                                          &lt;/li&gt;\n                                      &lt;/ul&gt;\n                              &lt;/ul&gt;\n                      &lt;/ul&gt;\n                    &lt;/div&gt;\n                  &lt;/div&gt;\n                &lt;/div&gt;\n                &lt;!-- END OF SIDE MENU AREA --&gt;\n                &lt;!-- START OF CONTENT AREA --&gt;\n                &lt;div class=\"col-md-9\"&gt;\n                    {% block content %}{% endblock %}\n                &lt;/div&gt;\n                &lt;!-- END OF CONTENT AREA --&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n\n\n        &lt;script src=\"/static/js/jquery-2.2.0.min.js\"&gt;&lt;/script&gt;\n        &lt;script src=\"/static/js/bootstrap.min.js\"&gt;&lt;/script&gt;\n        &lt;script src=\"/static/js/scripts.js\"&gt;&lt;/script&gt;            &lt;!-- custom JS --&gt;\n        &lt;script src=\"/static/js/validator.min.js\"&gt;&lt;/script&gt;      &lt;!-- https://github.com/1000hz/bootstrap-validator --&gt;\n        &lt;script src=\"/static/js/loadingoverlay.min.js\"&gt;&lt;/script&gt; &lt;!-- http://gasparesganga.com/labs/jquery-loading-overlay/--&gt;\n        {% block added_js %}{% endblock %}                       &lt;!-- block for JS added on pages on demand --&gt;\n    &lt;/body&gt;\n&lt;/HTML&gt;\n</code></pre> <p>I marked the lines on which child template insertion occurs. Once again, read the docs on templating, read some blogs and you will catch it quickly.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#child-template","title":"Child template","text":"<p>Once you have the base template figured out you are ready to create it's successors - child templates. A while back I showed you the <code>/templates/index.html</code> template where the following construct</p> <pre><code>{% extends 'base.html' %}\n{% block content %}\n &lt;!-- SOME HTML HERE --&gt;\n{% endblock %}\n</code></pre> <p>effectively told Flask to extend <code>base.html</code> content section with some code relevant to this particular <code>index.html</code> page.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#intermediate-templates-and-multiple-inheritances","title":"Intermediate templates and multiple inheritances","text":"<p>It is also possible to inherit more than once. See what I did for the pages with actual python scripts input and output forms:</p> <p></p> <p>As you will see shortly after - my user-facing scripts' page has some static sections like Description, Usage, Limitations, Author , etc. Normally, all of these sections will appear on every page thus it would be nice to move all this static and repetitious content to a separate template.</p> <p>That's how <code>content_template.html</code> was born. In this template I define blocks with names corresponding to the static sections.</p> <p>The last bit of this puzzle is the template <code>&lt;script_name&gt;.html</code> which extends <code>content_template.html</code> and fills in all the data into blocks defined in his parent template. This template will be spawned each time a new script will be added. In the example below I demo the template called <code>get_vmrc_links.html</code> that is used for a particular script.</p> <p><code>Content_template.html</code>:</p> <pre><code>{% extends 'base.html' %}\n{% block content %}\n\n&lt;h2 class=\"text-primary\"&gt;\n    {% block title %}{% endblock %}\n&lt;/h2&gt;\n\n&lt;p class=\"lead\" &gt;\n    {% block main_purpose %}{% endblock %}\n&lt;/p&gt;\n\n&lt;h3&gt;\n    Description\n&lt;/h3&gt;\n    &lt;p&gt;{% block descr %}{% endblock %}&lt;/p&gt;\n\n&lt;h3&gt;\n    Usage\n&lt;/h3&gt;\n    &lt;p&gt;{% block usage %}{% endblock %}&lt;/p&gt;\n\n\n&lt;h3&gt;\n    Limitations\n&lt;/h3&gt;\n    &lt;p&gt;{% block limitations %}{% endblock %}&lt;/p&gt;\n\n&lt;!-- Author and info block --&gt;\n&lt;small&gt;\n&lt;dl class=\"dl-horizontal\"&gt;\n  &lt;dt&gt;Author&lt;/dt&gt;\n  &lt;dd&gt;{% block author %}{% endblock %}&lt;/dd&gt;\n\n  &lt;dt&gt;Version&lt;/dt&gt;\n  &lt;dd&gt;{% block version %}{% endblock %}&lt;/dd&gt;\n\n  &lt;dt&gt;Tags&lt;/dt&gt;\n  &lt;dd&gt;{% block tags %}{% endblock %}&lt;/dd&gt;\n&lt;/dl&gt;\n&lt;/small&gt;\n\n    {% block script_content %}&lt;!-- Custom HTML for scrip --&gt;{% endblock %}\n\n{% endblock %}\n\n{% block added_js %}&lt;!-- Custom JavaScript for a script page--&gt;{% endblock %}&lt;/pre&gt;\n</code></pre> <p><code>templates/get_vmrc_links.html</code>:</p> <pre><code>{% extends 'content_template.html' %}\n{% block title %}\n    VMRC Links Composer\n{% endblock %}\n\n\n{% block main_purpose %}\nThis script composes clickable links for VMWare Remote Console (VMRC) standalone client along with basic info\nabout VMs on a ESXi host or vCenter.\n{% endblock %}\n\n\n{% block descr %}\nIt became quite a pain to get Web-based console working on ESXi hosts or vCenter servers (&lt;=v.5.5) with\n&lt;a href=\"http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;amp;cmd=displayKC&amp;amp;externalId=2114800\"&gt;deprecation of NPAPI&lt;/a&gt;\nplugins in modern browsers. An easy method to get a remote console access would be to use a standalone\n&lt;a href=\"http://kb.vmware.com/selfservice/microsites/search.do?language=en_US&amp;amp;cmd=displayKC&amp;amp;externalId=2091284\"&gt;Virtual Machine Remote Console client (VMRC)&lt;/a&gt;\nwhich is available for free for major OSes.\n&lt;p&gt;For standalone VMRC client to work a special link has to be composed - &lt;br/&gt;i.e. &lt;code&gt;vmrc://root@172.17.255.254:443/?moid=vm-13&lt;/code&gt;&lt;/p&gt;\nThis script generates such links and is based on an appropriate &lt;a href=\"http://noshut.ru/2016/01/getting-vmrc-links-with-python/\"&gt;console version&lt;/a&gt;.\n\n{% endblock %}\n\n\n{% block usage %}\nScript works both with independent ESXi hosts and managed by vCenter. Select desired entity from the\n&lt;code&gt;Known hosts&lt;/code&gt; select element or specify ESXi/vCenter IP address along with\ncredentials of the user with root privileges and you are good to go.\n{% endblock %}\n\n\n{% block limitations %}\nIn rare cases the script couldn't compose the whole link, in that case use the &lt;code&gt;moID&lt;/code&gt; value specified in the\noutput section to compose VMRC link manually.\n{% endblock %}\n\n\n{% block author %}\nRoman Dodin\n{% endblock %}\n\n\n{% block version %}\n0.1\n{% endblock %}\n\n\n{% block tags %}\nVMWare, VMRC\n{% endblock %}\n\n\n{% block script_content %}\n\n&lt;div class=\"container-fluid\"&gt;\n    &lt;div class=\"row\"&gt;\n        &lt;div class=\"col-md-12\"&gt;\n            &lt;div class=\"panel panel-default\"&gt;\n                &lt;div class=\"panel-heading\"&gt;\n                    &lt;h3 class=\"panel-title\"&gt;Inputs&lt;/h3&gt;\n                &lt;/div&gt;\n                &lt;div class=\"panel-body\"&gt;\n\n\n                    &lt;!-- FORM --&gt;\n                    &lt;form data-toggle=\"validator\" role=\"form\" method=\"post\"&gt; &lt;!-- validation form http://1000hz.github.io/bootstrap-validator/ --&gt;\n\n                        &lt;div class=\"form-group col-md-12\"&gt;\n                            &lt;label class=\"control-label\" for=\"known_hosts_select\"&gt;\n                                Known hosts\n                            &lt;/label&gt;\n                            &lt;select class=\"form-control\" id=\"known_hosts_select\"&gt;\n                              &lt;option&gt;-- List of known hosts --&lt;/option&gt;\n                              &lt;option ip=\"172.17.255.254\" login=\"root\" pass=\"test_pass\"&gt;vCenter (172.17.255.254) -- vCenter&lt;/option&gt;\n                              &lt;option ip=\"172.17.255.253\" login=\"root\" pass=\"test_pass\"&gt;ESXi (172.17.255.253) -- ESXi host&lt;/option&gt;\n                            &lt;/select&gt;\n                        &lt;/div&gt;\n\n                        &lt;div class=\"form-group col-md-12\"&gt;\n                            &lt;label class=\"control-label requiredField\" for=\"vmware_ip_addr\"&gt;\n                                ESXi/vCenter IP address\n                            &lt;/label&gt;\n                            &lt;input class=\"form-control\" type=\"text\"\n                                   pattern=\"^(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)$\"\n                                   id=\"vmware_ip_addr\" name=\"vmware_ip_addr\" placeholder=\"i.e. 172.17.255.255\"\n                                   data-error=\"How did you pass NRS I? it's not a valid IP address\" /&gt;\n                            &lt;div class=\"help-block with-errors\"&gt;&lt;/div&gt;\n                        &lt;/div&gt;\n                        &lt;div class=\"form-group col-md-6\"&gt;\n                            &lt;label class=\"control-label requiredField\" for=\"vmware_login\"&gt;Login\n                            &lt;/label&gt;\n                            &lt;input class=\"form-control\" id=\"vmware_login\" name=\"vmware_login\" placeholder=\"root\" type=\"text\" required/&gt;\n                        &lt;/div&gt;\n                        &lt;div class=\"form-group col-md-6\"&gt;\n                            &lt;label class=\"control-label requiredField\" for=\"vmware_pass\"&gt;Password\n                            &lt;/label&gt;\n                            &lt;input class=\"form-control\"  id=\"vmware_pass\" name=\"vmware_pass\"\n                                   placeholder=\"password\" type=\"password\" required/&gt;\n                        &lt;/div&gt;\n                        &lt;div class=\"form-group col-md-12\"&gt;\n                            &lt;div&gt; &lt;!-- RD: type=\"submit\" changed to button, since JQuery is in use --&gt;\n                                &lt;button class=\"btn btn-primary\" id=\"submit_form\" name=\"submit\" type=\"button\"&gt;\n                                    Submit\n                                &lt;/button&gt;\n                            &lt;/div&gt;\n                        &lt;/div&gt;\n                    &lt;/form&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;!-- OUTPUT SECTION --&gt;\n            &lt;div class=\"panel panel-default\"&gt;\n                &lt;div class=\"panel-heading\"&gt;\n                    &lt;h3 class=\"panel-title\"&gt;Output&lt;/h3&gt;\n                &lt;/div&gt;\n                &lt;div class=\"panel-body\" id=\"output_div\"&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;!-- END OF OUTPUT SECTION --&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n{% endblock %}\n{% block added_js %}\n&lt;script src=\"{{ url_for('get_vmrc_links.static', filename='get_vmrc_links.js') }}\"&gt;&lt;/script&gt;\n{% endblock %}\n</code></pre> <p>Rendered page: </p> <p>Dynamic template: http://codepen.io/hellt/pen/PNJwxq</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#flask-blueprints","title":"Flask Blueprints","text":"<p>Another project's major building block is Blueprint. Blueprints are important and actually making it possible to isolate various scripts in their appropriate sandboxes. And by sandbox I mean separate directory which hosts all the files linked to the script.</p> <p>Take a look inside <code>scripts_bank</code> directory which will host all the scripts-related files:</p> <pre><code>\u251c\u2500\u2500 scripts_bank\n\u2502   \u2514\u2500\u2500 vmware\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 get_vmrc_links\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 get_vmrc_links.py\n\u2502           \u251c\u2500\u2500 static\n\u2502           \u2502   \u2514\u2500\u2500 get_vmrc_links_scripts.js\n\u2502           \u2514\u2500\u2500 templates\n\u2502               \u2514\u2500\u2500 get_vmrc_links.html\n</code></pre> <p>It's the blueprints which allow us to modularize the app by storing some of it's components in the different directories and still be able to link them up to the main Flask app. See how elegantly JS code along with CSS styles needed only by this particular application <code>get_vmrc_links</code> found their's place in a separate directory - <code>/scripts_bank/vmware/get_vmrc_links/</code>!</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#blueprint-creation","title":"Blueprint creation","text":"<p>To create a Blueprint I placed this code in the <code>get_vmrc_links.py</code>:</p> <pre><code>get_vmrc_links_bp = Blueprint('get_vmrc_links', __name__, template_folder='templates', static_folder='static',\n                              static_url_path='/get_vmrc_links/static')\n\n@get_vmrc_links_bp.route('/get_vmrc_links', methods=['GET','POST'])\ndef get_vmrc_links():\n # some code\n</code></pre> <p>When I created a blueprint I defined it's <code>static_url_path</code> to <code>/get_vmrc_links/static</code> . But don't get confused if you don't see this path, I don't have it. That is because blueprints can be registered from a specific point and not directly from the project's root.</p> <p>Once we have Blueprint created we need to bind it to the route (line 4 in the snippet above). And again the route <code>/get_vmrc_links</code> will have it's root at the directory where Blueprint will be registered later.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#register-a-blueprint","title":"Register a blueprint","text":"<p>To register the blueprint navigate to the main <code>app.py</code> and add the following lines:</p> <pre><code>from scripts_bank.vmware.get_vmrc_links.get_vmrc_links import get_vmrc_links_bp\napp.register_blueprint(get_vmrc_links_bp, url_prefix='/vmware')\n</code></pre> <p>Registration is easy! Have you spotted the <code>url_prefix='/vmware'</code> part? This is the Blueprints root directory I was talking about! So now you can glue the parts in a whole picture.</p> <ol> <li>Blueprint's root directory is <code>/vmware</code></li> <li>It's static directory path is <code>/get_vmrc_links/static</code> which turns to <code>/vmware + /get_vmrc_links/static == /vmware/get_vmrc_links/static</code></li> <li>The Flask route <code>/get_vmrc_links</code> transforms to <code>/vmware/get_vmrc_links</code> and by following this URL the script's page will be rendered</li> </ol>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#front-end-back-end-data-exchange","title":"Front-end&lt;-&gt;back-end data exchange","text":"<p>To pass data back and forth between front-end and back-end we need to:</p> <ol> <li>(@front-end) serialize data from the input elements</li> <li>(@front-end) pass this data to the back-end</li> <li>(@back-end) receive data, make calculations, construct a response, send it</li> <li>(@front-end) receive a response and render it in the output form, handle errors</li> </ol>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#serializing-input-data","title":"Serializing input data","text":"<p>Serializing is not hard at all. Since it is a front-end's task it is done by the JS code which is also stored in a separate file unique to this particular script <code>/scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links.js</code>.</p> <p>This example shows you how you separate one script from another by maintaining all related files in a script's folder, in this example I'm working on <code>get_vmrc_links</code> script, so all the JS and specific HTML templates are stored under <code>/scripts_bank/vmware/get_vmrc_links/</code> directory.</p> <p>Take a look at <code>get_vmrc_links.js</code> and pay attention to <code>$('#submit_form').click(function()</code>. This function handles things occurring on <code>on-click</code> event to the <code>Submit</code> button.</p> <p><code>/scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js</code>:</p> <pre><code>// filling data to the input elements based on selection of predefined hosts\n$('#known_hosts_select').change(function () {\n    $(\"#vmware_ip_addr\").val($('#known_hosts_select option:selected').attr('ip'));\n    $(\"#vmware_login\").val($('#known_hosts_select option:selected').attr('login'));\n    $(\"#vmware_pass\").val($('#known_hosts_select option:selected').attr('pass'));\n});\n\n$(function() {\n    $('#submit_form').click(function() {\n        // start showing loading animation\n        $.LoadingOverlay(\"show\", {\n                        image       : \"\",\n                        fontawesome : \"fa fa-cog fa-spin\"\n                        })\n        $.ajax({\n            url: window.location.pathname, // url: /vmware/get_vmrc_links\n            data: $('form').serialize(),\n            type: 'POST',\n            success: function(response) {\n                $.LoadingOverlay(\"hide\");\n                if (response.error != \"\") {\n                    $('#output_div').HTML(response.error)\n                } else {\n                    $('#output_div').HTML(response.collected_vm_info)\n                }\n            }\n        });\n    });\n});\n</code></pre> <p>String <code>data: $('form').serialize()</code> produces a string of serialized data with all <code>&lt;form&gt;</code>'s input elements IDs and their values. Along with serialization task, this JS file contains additional things like showing \"Loading\" overlay and filling the <code>inputs</code> with the predefined data from <code>select</code> object.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#sending-serialized-data-to-the-back-end","title":"Sending serialized data to the back-end","text":"<p>Serialized data goes via <code>POST</code> method to the back-end via an <code>url</code> you specify.</p> <p>/scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js:</p> <pre><code>$.ajax({\n            url: newPathname + '/get_vmrc_links', // url: /vmware/get_vmrc_links\n            data: $('form').serialize(),\n            type: 'POST',\n</code></pre>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#receiving-data-by-the-back-end-and-processing","title":"Receiving data by the back-end and processing","text":"<p>To receive serialized data you need to create a <code>POST</code> requests handler:</p> <pre><code># /scripts_bank/vmware/get_vmrc_links/get_vmrc_links.py\n@get_vmrc_links_bp.route('/get_vmrc_links', methods=['GET','POST'])\ndef get_vmrc_links():\n    if request.method == 'GET':\n        return render_template('get_vmrc_links.html')\n\n    # handle POST method from JQuery\n    elif request.method == 'POST':\n        getvmrc_args = {'host': request.form['vmware_ip_addr'],\n                        'user': request.form['vmware_login'],\n                        'pass': request.form['vmware_pass']}\n</code></pre> <p>To get the contents arrived in <code>POST</code> I queried <code>form</code> data structure of the request object with appropriate keys. <code>form[]</code> object is an <code>ImmutableDict</code> data structure which contains all the data received in the <code>POST</code> method:</p> <pre><code>ImmutableMultiDict([('vmware_pass', 'mypass'), ('vmware_ip_addr', '172.17.255.253'), ('vmware_login', 'root')])\n</code></pre> <p>Once you received your inputs you pass it along to the main function of the chosen script to process. Here I should mention that you have two ways of generating output data:</p> <ol> <li>you could leave it in plain text and wrap it in the appropriate <code>HTML</code> tags with Flask</li> <li>or you could enclose scripts' output data in <code>HTML</code> tags during scripts execution process</li> </ol> <p>In this example with the <code>get_vmrc_links.py</code> script I chose the latter option and wrapped the whole output of the script (which normally would have found it's peace in <code>stdout</code>) with HTML tags:</p> <pre><code># /scripts_bank/vmware/get_vmrc_links/get_vmrc_links.py\n&lt;... omitted ...&gt;\n\nvmrc_links['collected_vm_info'] += \"&lt;p&gt;&lt;pre&gt;\" # opening paragraph and preformatted section\nvmrc_links['collected_vm_info'] += \"&lt;strong&gt;Name       : \" + vm_summary.config.name + \"&lt;/strong&gt;&lt;/br&gt;\"\nvmrc_links['collected_vm_info'] += \"Path       : \" + vm_summary.config.vmPathName + \"&lt;/strong&gt;&lt;/br&gt;\"\n\n&lt;... omitted ...&gt;\n</code></pre> <p>See these <code>&lt;pre&gt;</code>, <code>&lt;p&gt;</code> and <code>&lt;strong&gt;</code> tags I used? It'd done exactly to get rich formatting.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#passing-the-results-back-to-the-front-end","title":"Passing the results back to the front-end","text":"<p>One of the goals of this project was to make script's output look more readable. Thanks to modern front-end techniques and frameworks you could render whatever/however you like, your skills are the limit. At this time, my scripts produce just some text which I can render in various ways with HTML. But how do I actually pass this data to the front-end engine and in a what form?</p> <p>I pass it as <code>JSON</code>-formatted structure composed in a two-step process:</p> <p>Firstly, I collected scripts output data as a <code>dict</code> with the keys representing output data and errors (if any):  </p> <pre><code>vmrc_links = {'collected_vm_info': '',  ## collected results\n              'error': ''}              ## errors\n</code></pre> <p>Once I have a dict with results and errors to show I use Flask's jsonify function to represent my dict as JSON and compose a <code>Response</code> object to pass it further to the front-end:</p> <pre><code>    &lt;... omitted ...&gt;\n\n    elif request.method == 'POST':\n            getvmrc_args = {'host': request.form['vmware_ip_addr'],\n                            'user': request.form['vmware_login'],\n                            'pass': request.form['vmware_pass']}\n\n            global vmrc_links\n            vmrc_links = {'collected_vm_info': '',\n                        'error': ''}\n\n            vm_info = main(getvmrc_args)\n            return jsonify(vm_info)\n</code></pre> <p>And that's it. Now fast forward to the front-end and see how it processes received data:</p> <p>/scripts_bank/vmware/get_vmrc_links/static/get_vmrc_links_scripts.js:</p> <pre><code>$(function() {\n    $('#submit_form').click(function() {\n        // start showing loading animation\n        $.LoadingOverlay(\"show\", {\n                        image       : \"\",\n                        fontawesome : \"fa fa-cog fa-spin\"\n                        })\n        $.ajax({\n            url: window.location.pathname, // url: /vmware/get_vmrc_links\n            data: $('form').serialize(),\n            type: 'POST',\n            success: function(response) {\n                $.LoadingOverlay(\"hide\");\n                if (response.error != \"\") {\n                    $('#output_div').HTML(response.error)\n                } else {\n                    $('#output_div').HTML(response.collected_vm_info)\n                }\n            }\n        });\n    });\n});\n</code></pre> <p>On a successful return, I check if output has any errors and if it has - put an error message in the <code>#output_div</code> block. If things went smooth I put collected results in this block instead.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#adding-new-script-is-easy","title":"Adding new script is easy","text":"","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#create-the-file-structure","title":"Create the file structure","text":"<p>It's very easy to add a new script. Walk with me and see how easily I add completely separate script called <code>SAM-O XML API Tester</code>.</p> <p>At first, I created directories which represent sandbox for the script in a folder dedicated to storing scripts (<code>scripts_bank</code>). As I said, my directory structure follows my side-menu bar, that's why for the new script called <code>SAM-O_XML_API_Tester</code> I first off created root directory <code>5620sam</code> and then subdirectory <code>SAM-O_XML_API_Tester</code>. The latter dir will carry all files related to this particular script.</p> <p>Do not forget to create empty <code>__init.py__</code> files inside directories of the script to treat folders as python packages.</p> <pre><code>.\n\u251c\u2500\u2500 scripts_bank\n\u2502  \u251c\u2500\u2500 _5620sam\n\u2502  \u2502  \u251c\u2500\u2500 sam_xml_api_tester\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 __init__.py\n\u2502  \u2502  \u2502  \u251c\u2500\u2500 static\n\u2502  \u2502  \u2502  \u2514\u2500\u2500 templates\n\u2502  \u2502  \u2514\u2500\u2500 __init__.py\n\u2502  \u251c\u2500\u2500 __init__.py\n</code></pre>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#create-html-files","title":"Create HTML files","text":"<p>Now it's user-facing HTML template's turn. I created <code>sam-o_xml_api_tester.html</code> file in <code>SAM-O_XML_API_Tester/templates</code> dir leveraging sandbox environment. See, this makes individual script management very convenient, one directory stores em all.</p> <p>Following inheritance model this template inherits markup from the <code>content-template.html</code>. As I explained earlier it makes easier to fill in general text information (such as a name of the script, usage guide, author info, etc). Consider this as static or temporary layout for almost every new script.</p> <p>/scripts_bank/5620sam/SAM-O_XML_API_Tester/templates/sam-o_xml_api_tester.html:</p> <pre><code>{% extends 'content_template.html' %}\n{% block title %}\n    New script\n{% endblock %}\n\n{% block main_purpose %}\nThis is just demo for the blog.\n{% endblock %}\n\n{% block descr %}\nDescription TBD...\n{% endblock %}\n\n{% block usage %}\nIt does nothing, &lt;mark&gt;for now&lt;/mark&gt;\n{% endblock %}\n\n{% block limitations %}\nMy code does not have limitation!\n{% endblock %}\n\n{% block author %}\nRoman Dodin\n{% endblock %}\n\n{% block version %}\n0.1\n{% endblock %}\n\n{% block tags %}\nnoshut.ru\n{% endblock %}\n\n{% block script_content %}\n\n&lt;div class=\"container-fluid\"&gt;\n    &lt;div class=\"row\"&gt;\n        &lt;div class=\"col-md-12\"&gt;\n            &lt;div class=\"panel panel-default\"&gt;\n                &lt;div class=\"panel-heading\"&gt;\n                    &lt;h3 class=\"panel-title\"&gt;Inputs&lt;/h3&gt;\n                &lt;/div&gt;\n                &lt;div class=\"panel-body\"&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n\n            &lt;div class=\"panel panel-default\"&gt;\n                &lt;div class=\"panel-heading\"&gt;\n                    &lt;h3 class=\"panel-title\"&gt;Output&lt;/h3&gt;\n                &lt;/div&gt;\n                &lt;div class=\"panel-body\" id=\"output_div\"&gt;\n                &lt;/div&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;\n&lt;/div&gt;\n{% endblock %}\n{% block added_js %}\n&lt;!-- nothing here for the moment --&gt;\n{% endblock %}\n</code></pre>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#create-register-a-blueprint","title":"Create &amp; register a Blueprint","text":"<p>Now it's time to write few lines for back-end part. Create a python file which will hold blueprint for this script we've been adding and back-end activities:</p> <pre><code>#/scripts_bank/_5620sam/sam_xml_api_tester/sam_xml_api_tester.py\nfrom flask import render_template, request, Blueprint, jsonify\n\n###############\n#### FLASK ####\n###############\n\nsam_api_tester_bp = Blueprint('sam_api_tester', __name__, template_folder='templates', static_folder='static',\n                              static_url_path='/sam_xml_api_tester/static')\n\n@sam_api_tester_bp.route('/sam_xml_api_tester', methods=['GET','POST'])\ndef sam_api_tester():\n    if request.method == 'GET':\n        return render_template('sam_xml_api_tester.html')\n\n    # handle POST method from JQuery (will be filled later)\n    elif request.method == 'POST':\n        return 0\n</code></pre> <p>Register it in the main <code>app.py</code>:</p> <pre><code>from scripts_bank._5620sam.sam_xml_api_tester.sam_xml_api_tester import sam_api_tester_bp\napp.register_blueprint(sam_api_tester_bp, url_prefix='/5620sam')\n</code></pre> <p>And you are good to go!</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#how-to-test-plaza","title":"How to test PLAZA?","text":"<p>Apart from traditional way of cloning a repo and building a virtual environment, you can use a docker container.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/building-web-front-end-for-python-scripts-with-flask/#whats-next","title":"What's next?","text":"<p>Tons of useful things are missing at the moment - no search, no active tags, no login-based system, no tests, etc. I will probably add some of this features later, but you are welcome to suggest, blame, and pull-request. Yeah, the code <code>as is</code> can be grabbed from GitHub.</p>","tags":["bootstrap","flask","python"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/","title":"BGP VPLS deep dive. Nokia SR OS &amp; Juniper","text":"<p>It may very well be\u00a0that VPLS days are numbered\u00a0and EVPN is to blame. Nevertheless, it would be naive to expect VPLS extinction in the near future. With all its shortcomings VPLS is still\u00a0very well standardized, interop-proven and has a huge footprint in MPLS networks of various scale.</p> <p>In this post I will cover theory and configuration parts for one particular flavor of VPLS signalling - BGP VPLS (aka\u00a0Kompella VPLS) defined in RFC4761. I'll start with simple single home VPLS scenario while multi-homing techniques and some advanced\u00a0configurations might appear in a separate post later.</p> <p>In this topic the following SW releases were used:</p> <ul> <li>Nokia (Alcatel-Lucent) VSR\u00a014.0.R4</li> <li>Juniper vMX 14.1R1.10</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#bgp-vpls-basics","title":"BGP VPLS Basics","text":"<p>Virtual Private LAN Service (VPLS) is seen like an Ethernet LAN by the customers of a Service Provider. However, in a VPLS, not all of the customers are connected to a single LAN; they may be spread across a metro or wide area network. In essence, a VPLS glues together several individual LANs across a packet switched network to appear and function as a single LAN. This is accomplished by incorporating MAC address learning, flooding, and forwarding functions in the context of pseudowires that connect these individual LANs.</p> <p>The entire VPLS service behaves like a big switch with distributed MAC learning intelligence implemented on each PE, and as in a switch, MAC learning\u00a0happens in a dataplane.</p> <p>The following two types of interfaces are typical for VPLS:</p> <ol> <li>Attachment Circuits (AC) - circuits connecting\u00a0Customer Edge (CE) devices to\u00a0Provider Edge (PE) routers.\u00a0PE routers often called VPLS Edge (VE) devices in VPLS terminology.</li> <li>Pseudowires (PW) - circuits connecting PEs between each other</li> </ol> <p>In the context of a given VPLS instance, a PE can have one or more local ACs, and one or more PWs toward remote PEs. Full-mesh of transport tunnels between PEs is required.</p> <p>In Kompella\u00a0VPLS, BGP is a key enabler and is responsible for:</p> <ul> <li>Auto-discovery: process of finding all PE routers participating in a VPLS instance;</li> <li>Signalling:\u00a0the setup and tear-down of pseudowires (PW) that constitute the VPLS service.</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#auto-discovery","title":"Auto-discovery","text":"<p>Each PE \"discovers\" which other PEs are part of a given VPLS by means of BGP. This allows each PE's configuration to consist only of the identity of the VPLS instance established on this PE, not the identity of every other PE in that VPLS instance. Moreover, when the topology of a VPLS changes, only the affected PE's configuration changes; other PEs automatically find out about the change and adapt.</p> <p>The Route Target community is used to\u00a0identify members of a VPLS.\u00a0A PE announces that it belongs to <code>VPLS V</code> by annotating its NLRIs for <code>VPLS V</code> with Route Target <code>RT</code>, and acts on this by accepting NLRIs from other PEs that have Route Target <code>RT</code>. A PE announces that it no longer participates in <code>VPLS V</code> by withdrawing all NLRIs that it had advertised with Route Target <code>RT</code>.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#signalling","title":"Signalling","text":"<p>Once discovery is done, each pair of PEs in a VPLS must be able to establish (and tear down) pseudowires to each other, i.e., exchange (and withdraw) demultiplexors. This process is known as signaling. Signaling is also used to transmit certain characteristics of the pseudowires that a PE sets up for a given VPLS.</p> <p>BGP Update message carrying BGP VPLS NLRI (AFI:25, SAFI:65) is used to signal VPLS membership and multiplexors for a VPLS service:</p> <p></p> <p>Let's expand some of the fields of the BGP VPLS NLRI:</p> <ul> <li>Route Distinguisher - used to differentiate between customer NLRIs thus should be unique for every VPLS service.  </li> <li>VE ID - unique identifier (aka site-id), manually assigned to every VPLS Edge device.  </li> <li>VE Block Offset, VE Block Size and Label Base are used for calculating the service label (multiplexor).</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#label-blocks","title":"Label Blocks","text":"<p>Using a distinct BGP Update message to send a demultiplexor to each remote PE would require the originating PE to send N such messages for N remote PEs. In order to minimize the control plane load original standard introduced <code>Label Blocks</code> which drastically reduce the amount of BGP Update messages.\u00a0A label block is a set of demultiplexor labels used to reach a given VE ID.</p> <p>A single BGP VPLS NLRI signals a label block which consists of:</p> <ul> <li>VE ID - manually assigned to VE device identifier</li> <li>Label Base (LB)\u00a0- first label assigned to a label block</li> <li>VE Block Size (VBS) - number of labels assigned to a label block. Vendor-dependant value, Nokia and Juniper both use Block Size of 8.</li> <li>VE Block Offset (VBO) - first VE ID assigned to a label block</li> </ul> <p>A contiguous label block defined by <code>&lt;LB, VBO, VBS&gt;</code> is the set <code>{LB+VBO, LB+VBO+1, ..., LB+VBO+VBS-1}</code>. Thus, instead of a single large label block to cover all VE IDs in a VPLS, one can have several label blocks, each with a different label base.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#pseudowire-setup-process","title":"Pseudowire setup process","text":"<p>Section 3.2.3 of RFC4761\u00a0highlights the steps VE routers go through during PW setup/teardown.\u00a0Lets see by an example how PW setup takes places\u00a0in a BGP VPLS between routers VE1 and VE7.</p> <ol> <li> <p>Router VE1\u00a0is part of VPLS <code>BLUE</code> and has VPLS service configured with the following parameters:</p> <pre><code>VE ID: 1\nRT: 65000:10\nRD: 1.1.1.1:10\n</code></pre> <p>Upon a service config router VE1 sends BGP Update message to all its BGP peers with NLRI describing its label block. Since for this moment VE1 has no knowledge about any other router participating in <code>VPLS BLUE</code> service it sends only one NLRI. This NLRI covers a label block which has its own VE-ID (refer to the figure 40)</p> </li> <li> <p>When VE10 router boots with the same <code>VPLS BLUE</code> service configured using the following params:</p> <pre><code>VE ID: 10\nRT: 65000:10\nRD: 10.10.10.10:10\n</code></pre> <p>it sends BGP Update to all its peers. Again, since VE10 hasn't seen any VE routers yet, it will send only one NLRI with a label block which its VE-ID reside in.</p> </li> <li> <p>VE1 receive BGP Update send by VE10 and since route-target community is the same on both routers VE1 accepts it. Then VE1 performs a check whether the NLRI it received from VE10 can be used for PW setup (see Fig. 50, step 3). Since the result of the check returned false, VE1 can't use received NLRI from VE10 for PW setup towards it.      </p> </li> <li>By receiving an update from VE10, VE1 now has knowledge of VE10 existence\u00a0and checks if it has sent label block which contains a label for VE10. As stated in Fig. 40, VE1 sent label block containing information that can be used by routers with VE-ID from 1 to 8, thus VE10 can not setup PW using this label block. VE1 then sends another BGP Update with NLRI covering VE-IDs from 9 to 16 to satisfy VE10 needs.</li> <li>Router VE10 performs the same check against received NLRI. This time the check is passed and VE10 can calculate a label (multiplexor) which it should use for PW from VE10 to VE1.</li> <li>As in step 4, VE10 generates additional BGP Update with NLRI covering VE1 range.</li> </ol> <p>This enables PW to setup from VE1 to V10 and from VE10 to VE1 by using MPLS labels calculated on each router independently.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#layer-2-info-extended-community","title":"Layer 2 Info Extended community","text":"<p>Additional extended community is used\u00a0in\u00a0VPLS service establishment - originally <code>Layer 2 Info</code> extended community was defined in section 3.2.4. It is used to signal control information about the pseudowires to be setup for a given VPLS. Two additional\u00a0bits (D, F) later were introduced\u00a0by vpls-multihoming draft.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#mtu-considerations","title":"MTU considerations","text":"<p>One of the important fields in L2 Info community is the <code>Layer 2 MTU</code>. VE routers signal MTU which can be carried within VPLS service, moreover some router platforms will bring service into down state if the MTU values mismatch. Usually you can find configuration knobs which will turn off <code>MTU matching</code>, though it is better to keep MTU consistent between endpoints.</p> <p>Interesting fact is that Juniper routers (at least vMX 14.1) defaults to signal\u00a0<code>MTU=0</code> (as seen in Figure 80), which wont bring service down, because this means do not consider MTU value. Again, in vMX 14.1 there is no way to signal any particular MTU value for VPLS service, though starting with 15.1 it is possible with <code>mtu</code> keyword.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#vlan-tag-multiplexing","title":"VLAN tag multiplexing","text":"<p>To understand how ethernet frames are handled by PE and transported over PW we need to cover possible variations of frames on AC, as well as different modes of PW operation.</p> <p>Frames from the CE devices can be seen on\u00a0the attachment circuits in different flavours:</p> <ul> <li>untagged</li> <li>tagged (dot1q, q-in-q) by CE device itself</li> <li>additionally tagged by some SP's aggregation device (SVLAN put by L2 agg. device)</li> </ul> <p>When it comes to PW operation modes, RFC 4448\u00a0gives us two options: raw and tagged. Thus, we can distinguish two cases regarding tagged frames coming from AC:</p> <ul> <li> <p>The tag is service-delimiting.     This means that the tag was placed on the frame by some piece of service provider-operated equipment, and the tag is used by the service provider to distinguish the traffic. For example, LANs from different customers might be attached to the same service provider switch, which applies VLAN tags to distinguish one customer's traffic from another's, and then forwards the frames to the PE.</p> </li> <li> <p>The tag is not service-delimiting.     This means that the tag was placed in the frame by a piece of customer equipment, and is not meaningful to the PE.</p> </li> </ul> <p>RFC 4448 explains further possible scenarios actions:</p> <ul> <li>PW is operating in raw mode (aka Ether):<ul> <li>Service-delimiting tags are NEVER sent over the PW, if tag is present, it MUST be stripped before sending on PW</li> <li>When sending a frame on AC, PE may add service-delimiting tag, but can not strip or rewrite any existing tags present on a frame</li> </ul> </li> <li>PW is operating in tagged mode (aka VLAN):<ul> <li>PW MUST have a service-delimiting VLAN tag. If service-delimiting tag is not present, the PE must prepend the frame with a dummy VLAN tag before sending the frame on the PW</li> <li>When sending a frame on AC, PE may rewrite or strip tag entirely</li> </ul> </li> <li>Whether or not the tag is service-delimiting is determined by local configuration on the PE</li> <li>Service-delimiting tag have local to PE-CE interface significance</li> <li>Non-service-delimiting tags are passed transparently across the PW as part of the payload</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#vpls-data-plane","title":"VPLS data plane","text":"<p>This topic is focusing on VPLS\u00a0data\u00a0plane encapsulation, as defined in RFC 4448 - Encapsulation Methods for Transport of Ethernet over MPLS Networks.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#mac-learning","title":"MAC learning","text":"<p>VPLS is a multipoint service with a MAC learning on a data plane. This means that the entire Service Provider network should appear as a single logical learning bridge (Ethernet switch) for each VPLS that the SP network supports. The logical ports for the SP \"bridge\" are the AC as well as the PW\u00a0on a PE.\u00a0As a result of MAC learning, bridges populate a MAC table in which they keep track of the interface (or PW) where each unicast MAC is reachable.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#aging-flooding-bum-traffic","title":"Aging, Flooding, BUM traffic","text":"<p>VPLS PEs SHOULD have an aging mechanism to remove a MAC address associated with a logical port.\u00a0Aging reduces the size of a VPLS MAC table to just the active MAC addresses. When a bridge receives a packet to a destination that is not in its FIB, it floods the packet on all the other ports (process known as replication). Frames that should be flooded are Broadcast, Unknown unicast and Multicast</p> <ul> <li>Broadcast frames have destination MAC address <code>ff:ff:ff:ff:ff:ff</code>.</li> <li>Multicast frames have a destination MAC address whose first octet has its last bit set to one</li> </ul> <p>To avoid loops during replication process split-horizon rule should be honored:\u00a0A frame received on a PW is never sent back on the same or any other PW (default, but configurable behavior).</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#case-study-single-homed-vpls","title":"Case study: Single-homed VPLS","text":"<p>Enough with theory, time to practice\u00a0some VPLS! I will start with a simple case of two CE routers (CE1 and CE2) connected to a Service Provider's PE routers (R1, R2) configured with a VPLS service. Refer to Fig. 60 outlining lab topology for this case. It is assumed that ISIS and LDP are configured and operational.</p> <p>Refer to these baseline configurations:</p> <ul> <li>R1 (Nokia)</li> <li>R2 (Juniper)</li> <li>R3 Route reflector (Nokia)</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#bgp-configuration","title":"BGP configuration","text":"<p>This one is really simple. All we need is to configure MP-iBGP peering between PEs and RR with L2 VPN family enabled:</p> <p>R1 (Nokia)</p> <pre><code>*A:R1&gt;config&gt;router# info\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n## &lt; Output omitted for brevity &gt;\n\n        autonomous-system 65000\n#--------------------------------------------------\n\n\n*A:R1&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            connect-retry 1\n            min-route-advertisement 1\n            rapid-withdrawal\n            rapid-update l2-vpn\n            group \"RR\"\n                family l2-vpn\n                enable-peer-tracking\n                neighbor 3.3.3.3\n                    type internal\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>R3 Route reflector (Nokia)</p> <pre><code>*A:R3&gt;config&gt;router# info\n#--------------------------------------------------\necho \"IP Configuration\"\n#--------------------------------------------------\n## &lt; Output omitted for brevity &gt;\n\n        autonomous-system 65000\n#--------------------------------------------------\n\n\nA:R3&gt;config&gt;router&gt;bgp# info\n----------------------------------------------\n            family l2-vpn\n            connect-retry 1\n            min-route-advertisement 1\n            enable-peer-tracking\n            rapid-withdrawal\n            rapid-update l2-vpn\n            group \"RRC\"\n                cluster 3.3.3.3\n                neighbor 1.1.1.1\n                    type internal\n                exit\n                neighbor 2.2.2.2\n                    type internal\n                exit\n            exit\n            no shutdown\n----------------------------------------------\n</code></pre> <p>R2 (Juniper)</p> <pre><code>root@R2# show routing-options | grep auto\nautonomous-system 65000;\n\nroot@R2# show protocols bgp\ngroup RR {\n    type internal;\n    family l2vpn {\n        signaling;\n    }\n    neighbor 3.3.3.3;\n}\n</code></pre>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#interface-configuration","title":"Interface configuration","text":"<p>To demonstrate vlan-normalization methods I used two different vlans on attachment circuits connected to R1 and R2. Our CE1 and CE2 devices have simple\u00a0dot1q interfaces addressed in this way:</p> <ul> <li>CE1 has interface <code>toCE2</code> with address <code>192.168.1.1/24</code>, <code>VLAN 10</code></li> <li>CE1 has interface <code>toCE1</code> with address <code>192.168.1.2/24</code>, <code>VLAN 600</code></li> </ul> <p>Router R1 has vlan 10 on its AC, while R2 configured with vlan-id 600 (on Juniper vlan ids values for VPLS interfaces must be &gt; 512).</p> <p>Nokia routers do not differ if interface is going to be used in any particular service or in no service at all, therefore the configuration steps are obvious. The part which enables particular ethernet encapsulation (802.1q in this case) is done under port configuration:</p> <p>R1:</p> <pre><code>A:R1# configure port 1/1/2\nA:R1&gt;config&gt;port# info\n----------------------------------------------\n        ethernet\n            mode hybrid  ## hybrid means that port can act as an access &amp; network port\n            encap-type dot1q\n        exit\n        no shutdown\n----------------------------------------------\n</code></pre> <p>Configuration of <code>Vlan-id 10</code> attachment to a VPLS service will be done later in the VPLS configuration section.</p> <p>Note, that Ethernet MTU on Nokia routers includes\u00a0Ethernet header. This means that, for instance, interface with MTU 2000 will be able to put on wire exactly 2000 bytes, for example:</p> <pre><code>TOTAL 2000B == Frame 54: 2000 bytes on wire (16000 bits), 2000 bytes captured (16000 bits) on interface 0\n14B         == Ethernet II, Src: 50:01:00:04:00:01 (50:01:00:04:00:01), Dst: 50:01:00:05:00:01 (50:01:00:05:00:01)\n4B          == 802.1Q Virtual LAN, PRI: 7, CFI: 0, ID: 10\n20B         == Internet Protocol Version 4, Src: 192.168.1.1, Dst: 192.168.1.2\n1962B       == Internet Control Message Protocol\n</code></pre> <p>In contrast with Nokia, Juniper's configuration of interface\u00a0is done in a different way.</p> <pre><code>root@R2# show interfaces ge-0/0/1\nflexible-vlan-tagging;\nencapsulation flexible-ethernet-services;\nunit 600 {\n    encapsulation vlan-vpls;\n    vlan-id 600;\n}\n</code></pre> <p>In Juniper you have to specify\u00a0<code>encapsulation vlan-vpls</code>\u00a0under the interface's configuration for a logical unit you're going to use as an AC.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#service-configuration","title":"Service configuration","text":"<p>Now to the main course, service configuration. Both Nokia and Juniper has some vendor-specifics and different defaults worth mentioning.</p> <p>Let's take\u00a0one section at a time and discuss the details. On R1 I configured VPLS service with <code>id 10</code>. The same id I used for RD and RT.</p> <pre><code>A:R1# configure service vpls 10\nA:R1&gt;config&gt;service&gt;vpls# info\n----------------------------------------------\n            bgp\n                route-distinguisher 1.1.1.1:10\n                route-target export target:65000:10 import target:65000:10\n                pw-template-binding 11\n                exit\n            exit\n</code></pre> <p>BGP section of VPLS service has RD/RT values which used for route distinguishing between different VPLS services and auto-discovering based on Route Target. What is interesting here is <code>pw-template-binding</code> keyword. In SROS we use pseudowire templates to describe pseudowire characteristics. Based on this template PW will later bind to a VPLS service and established.</p> <pre><code>A:R1# configure service pw-template 11\nA:R1&gt;config&gt;service&gt;pw-template# info\n----------------------------------------------\n            controlword\n            force-vlan-vc-forwarding\n            split-horizon-group \"mesh\"\n            exit\n----------------------------------------------\n</code></pre> <p>The MPLS transport tunnel between PEs can be signaled using LDP or RSVP-TE. LDP based pseudowires can be automatically instantiated. RSVP-TE based SDPs have to be pre-provisioned. In this post I rely on automatically created LDP LSP. Using this mechanism SDPs will be auto-instantiated.</p> <p>A keyword <code>controlword</code> indicates that control word\u00a0must be used in dataplane. A pseudowire template is required containing a split horizon group. Each SDP created with this template is contained within a split horizon group so that traffic cannot be forwarded between them.</p> <p>By using\u00a0<code>force-vlan-vc-forwarding</code> command\u00a0we force a VE router to push service-delimiting vlan-id when sending data on PW (it is PW vlan-tagged mode). By default service-delimiting tag will be stripped off (equals to PW raw mode).</p> <p>Next section carries bgp-vpls specific commands:</p> <pre><code>bgp-vpls\n    max-ve-id 10\n    ve-name \"BLUE\"\n    ve-id 1\n    exit\n    no shutdown\nexit\n</code></pre> <p>In this section VPLS Edge name should be configured along with <code>ve-id</code>. What is specific to Nokia SR OS is the necessity to configure <code>max-ve-id</code> value. The choice of ve-id is crucial in ensuring efficient allocation of de-multiplexer labels. The most efficient choice is for ve-ids to be allocated starting at 1 and incrementing for each PE as the following section explains.</p> <p>The <code>max-ve-id</code> value determines the range of the ve-id value that can be configured. If a PE receives a BGP-VPLS update containing a ve-id with a greater value than the configured <code>max-ve-id</code>, then the update is dropped and no service labels are installed for this ve-id.</p> <p>Note: For Juniper it is not mandatory (but possible) to configure maximum VE devices in a service.</p> <p>The rest of the service configuration goes like this:</p> <pre><code>            stp\n                shutdown\n            exit\n            sap 1/1/2:10 create\n                no shutdown\n            exit\n            no shutdown\n</code></pre> <p>Important part here is <code>sap</code> (service access point) binding. Command <code>sap 1/1/2:10</code> basically tells that frames coming in <code>port 1/1/2</code> encapsulated with <code>vlan-id 10</code> will be attached to this VPLS service. Note, that <code>vlan-id 10</code> is a service delimiter. Read more about processing of vlan tags here.</p> <p>Since our topology is loop-free we do not need to run STP, thus it is\u00a0shutdowned.</p> <p>Juniper configuration of a service is as follows. A couple of lines need to be explained, while most of them are pretty standard:</p> <pre><code>root@R2# show routing-instances VPLS-BLUE\ninstance-type vpls;\nvlan-id 10;\ninterface ge-0/0/1.600;\nroute-distinguisher 2.2.2.2:10;\nvrf-target target:65000:10;\nprotocols {\n    vpls {\n        control-word;\n        no-tunnel-services;\n        site CE2 {\n            site-identifier 2;  ## VE-ID\n            interface ge-0/0/1.600;\n        }\n    }\n}\n</code></pre> <p>For VPLS service Juniper offers two types of routing instances: vpls and virtual-switch. Differences between them covered for example in the MPLS in the SDN Era book chapter 7. I used\u00a0a simple <code>vpls</code> instance.</p> <p>The reason for <code>no-tunnel-services</code> covered in this article.</p> <p><code>vlan-id 10</code> statement used for vlan-normalization, refer to the next section for the details.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#vlan-handling-in-vpls","title":"VLAN handling in VPLS","text":"<p>Most common problem with VPLS services are VLAN handling when vlan-id on attachment circuits differs. And our topology is\u00a0a perfect example of this, two ACs use different vlans (10 on R1 and 600 on R2). At the same time these vlan-ids are of service-delimiting purpose, meaning that they are not\u00a0CE provisioned vlans, but actually SPs assigned for multiplexing different customers/services on a single AC.</p> <p>Handling of service-delimiting vlans is different on Nokia and Juniper. Take a look at Fig. 70 and refer once again to\u00a0MPLS in the SDN Era book chapter 7 where each of VLAN modes discussed in details:</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#control-plane-walk-through","title":"Control plane walk through","text":"<p>To assemble all the pieces discussed earlier into a picture I will cover\u00a0things that happen in control and data planes during the VPLS service creation and operation.</p> <p>Step 1 For simplicity of discussion lets assume that router R1 has its VPLS service configured\u00a0first and send MP-BGP Update to Route Reflector. BGP Update consists of elements we have covered in previous sections of this post.</p> <p>Worth mentioning here is that R1 sends only one label block of size 8 (despite that <code>max-ve-id</code> has been configured for 10 VE devices). This is the result of optimization techniques, router does not send all the blocks, it will send ones that necessary once it receive an update message with CE-ID which is not part of blocks advertised by a router so far.</p> <p>Step 2 R2 receives update from R1 (via R3) and goes through PW setup process discussed earlier. At this step R2 calculates a service MPLS label it will use in the data plane.</p> <p>Step\u00a03 Now operator enables VPLS service on R2 and its R2's turn to send BGP Update\u00a0towards R1.</p> <p>Step\u00a04 R1 follows the\u00a0same procedure as R2 did in step 2. At the end of control plane\u00a0messages exchange we should have a pseudowire established with a certain characteristics and the service status should be healthy.\u00a0Lets ensure that from control plane perspective VPLS service is up and running on both platforms.</p> <p>Getting status of VPLS service with id 10:</p> <pre><code>A:R1# show service id 10 base \n\n===============================================================================\nService Basic Information\n===============================================================================\nService Id        : 10                  Vpn Id            : 0\nService Type      : VPLS                \nName              : (Not Specified)\nDescription       : (Not Specified)\nCustomer Id       : 1                   Creation Origin   : manual\nLast Status Change: 03/13/2016 07:59:09 \nLast Mgmt Change  : 03/13/2016 07:59:34 \nEtree Mode        : Disabled            \nAdmin State       : Up                  Oper State        : Up\nMTU               : 1514                Def. Mesh VC Id   : 10\nSAP Count         : 1                   SDP Bind Count    : 1\nSnd Flush on Fail : Disabled            Host Conn Verify  : Disabled\nSHCV pol IPv4     : None\nPropagate MacFlush: Disabled            Per Svc Hashing   : Disabled\nAllow IP Intf Bind: Disabled            \nFwd-IPv4-Mcast-To*: Disabled            Fwd-IPv6-Mcast-To*: Disabled\nDef. Gateway IP   : None                \nDef. Gateway MAC  : None                \nTemp Flood Time   : Disabled            Temp Flood        : Inactive\nTemp Flood Chg Cnt: 0                   \nSPI load-balance  : Disabled            \nTEID load-balance : Disabled            \nSrc Tep IP        : N/A                 \nVSD Domain        : &lt;none&gt;\n\n-------------------------------------------------------------------------------\nService Access &amp; Destination Points\n-------------------------------------------------------------------------------\nIdentifier Type         AdmMTU  OprMTU  Adm  Opr\n-------------------------------------------------------------------------------\nsap:1/1/2:10                             q-tag        8704    8704    Up   Up\nsdp:17407:4294967293 SB(2.2.2.2)         BgpVpls      0       8678    Up   Up\n===============================================================================\n</code></pre> <p>Basic information about a service shows us its Admin and Oper states. As well as <code>sap</code> and <code>sdp</code> this service\u00a0has along with their statuses. To see the service labels calculated on a Nokia router you need to query for SDP section of a service:</p> <pre><code>A:R1# show service id 10 sdp detail | match \"Egress Label\"\nIngress Label      : 131064                   Egress Label      : 262145\n</code></pre> <p>So R1 will use service label <code>262145</code> when will send traffic destined to a VPLS service configure on R2.</p> <p>For Juniper basic verification of VPLS service goes like this:</p> <pre><code>root@R2# run show vpls connections instance VPLS-BLUE \n\nLayer-2 VPN connections:\nInstance: VPLS-BLUE\n  Local site: CE2 (2)\n    Number of local interfaces: 1\n    Number of local interfaces up: 1\n    IRB interface present: no\n    ge-0/0/1.600       \n    lsi.1049088         1         Intf - vpls VPLS-BLUE local site 2 remote site 1\n    Label-base        Offset     Size  Range     Preference\n    262145            1          8      8         100   \n    connection-site           Type  St     Time last up          # Up trans\n    1                         rmt   Up     Nov 11 19:30:43 2016           1\n      Remote PE: 1.1.1.1, Negotiated control-word: Yes (Null)\n      Incoming label: 262145, Outgoing label: 131064\n      Local interface: lsi.1049088, Status: Up, Encapsulation: VPLS\n        Description: Intf - vpls VPLS-BLUE local site 2 remote site 1\n    Connection History:\n        Nov 11 19:30:43 2016  status update timer  \n        Nov 11 19:30:43 2016  loc intf up                  lsi.1049088\n        Nov 11 19:30:43 2016  PE route changed     \n        Nov 11 19:30:43 2016  Out lbl Update                    131064\n        Nov 11 19:30:43 2016  In lbl Update                     262145\n        Nov 11 19:30:43 2016  loc intf down\n</code></pre> <p>Both access (ge-0/0/1.600) and tunnel (lsi.1049088) interfaces are shown along with calculated service label values.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#data-plane-walk-through","title":"Data plane walk through","text":"<p>And now we are ready to explore data plane! In VPLS data plane is crucial as it is the only way how MAC addresses can be\u00a0learned. With this being said it is clear that since no MACs were learned during control plane messages exchange, CE devices have no ARP entries except for their own IP addresses.</p> <p>ARP handling To demonstrate data plane operations I will issue a ping from CE1 (<code>192.168.1.1/24</code>) to CE2 (<code>192.168.1.2/24</code>). Since ARP table on CE1 is empty it is necessary to\u00a0start with ARP request, but to keep this section a bit shorter I will omit ARP packets propagation since the same path will IP packets take.</p> <p>The only difference ARP packets have is that\u00a0they represent layer 2 broadcast traffic, therefore these traffic will be replicated by VPLS VE router and sent out from any local VPLS interface as well as out from every pseudowire of particular VPLS instance. During ARP packets propagation local MAC tables on R1 and R2 will be populated with MAC-IP pairs of CE routers.</p> <p>Known unicast Thanks to ARP process, a packet destined from CE1 to CE2 will be a known unicast by nature, since destination IP address of <code>192.168.1.2</code> is known in VPLS MAC tables of R1 and R2.</p> <p>Step 1 When a request to ping CE2 address arises on\u00a0CE1, the latter checks its ARP table to see if it has Layer 2 address corresponding to this IP. Since ARP process has already been done, CE1 has MAC address of CE2 resolved. Thus it can construct the whole frame and send it out via its interface towards R1.</p> <p>Since interface <code>toCE2</code> has particular <code>VLAN ID 10</code> assigned, the resulting packet will resemble the following structure\u00a0ICMP-IP-VlanID-Ethernet:</p> <p>Note, no MPLS encapsulation happens on the wire between CE1 and R1, it is plain IP packet with Ethernet on data layer.</p> <p>Step\u00a02 At\u00a0Step 2.1 R1 receives the data on the service access point <code>1/1/2:10</code> which is included in VPLS service with <code>id 10</code>. R1 queries with destination MAC address (<code>50:01:00:05:00:01</code>) VPLS's forwarding database (fdb or MAC table) and (thanks to previous ARP messages) matches a service distribution point (sdp or transport tunnel)\u00a0where this MAC was learned from.</p> <p>R1 knows what label (or demultiplexor) it should use when placing a customers packet on a pseudowire, this label were calculated based on the seed material received during control plane operations. Step 2.3 depicts how R1 assembles all the pieces to place a packet on a PW. It is worth to note that we see only service label on the wire, transport label is missing due to <code>implicit null</code> label received from R2.</p> <p>Step 3 When R2 receives a packet from R1 it strips Ethernet and VLAN headers and analyzes a MPLS label.\u00a0As was discussed, label values were\u00a0calculated during control plane convergency, therefore R2 has a proper label action for a label value of <code>262145</code> (Step 3.2).</p> <p>By using <code>no-tunnel-services</code> command on R2 we created a label-switched interface (LSI) to provide VPLS functionality. An LSI MPLS label is used as the inner label for VPLS. This label maps to a VPLS routing instance. On the PE router, the LSI label is stripped and then mapped to a logical LSI interface. The Layer 2 Ethernet frame is then forwarded using the LSI interface to the correct VPLS routing instance. Step 3.1 shows LSI interface and VPLS service binding.</p> <p>Since VPLS service behaves like a normal switch packet forwarding decisions are based on MAC table lookup. So far we decapsulated our packet up customer Ethernet header, which has <code>Dest MAC: 50:01:00:05:00:01</code>. R2 consults a MAC table inside VPLS service (Step 3.3) and finds out that destination MAC was\u00a0seen behind <code>ge-0/0/1.600</code> interface (this MAC was learned during ARP operation). This information\u00a0is enough for R2 to decide in what direction customers traffic should go further.</p> <p>Step 3.4 shows that R2 sends out of its <code>ge-0/0/0.600</code> interface a packet towards a recipient CE2 and uses the <code>VLAN ID 600</code> since this is the tag used by this attachment circuit. Note, that in step 2.3 we had <code>VLAN ID 10</code> placed by R1 because of <code>force-vlan-vc-forwarding</code> command on R1.\u00a0This VLAN has been recognized by R2 because\u00a0of this part in its service configuration:</p> <pre><code>root@R2# show routing-instances VPLS-BLUE\ninstance-type vpls;\nvlan-id 10;\ninterface ge-0/0/1.600;\nroute-distinguisher 2.2.2.2:10;\nvrf-target target:65000:10;\nprotocols {\n    vpls {\n        control-word;\n        no-tunnel-services;\n        site CE2 {\n            site-identifier 2;  ## VE-ID\n            interface ge-0/0/1.600;\n        }\n    }\n}\n</code></pre> <p>R2 sees that this is the VLAN ID used by VPLS and swaps it with VLAN ID used on its attachment circuit when sending traffic out of its <code>ge-0/0/0.600</code> interface.This is how you normalize VLAN IDs in data plane.</p> <p>Step 4 In the end CE2 receives a standard ICMP packet encapsulated within Ethernet 802.1q frame and processes it accordingly.</p> <p>Return traffic follows the same login in the opposite direction with a slight change in MPLS operation. Since Nokia vSR does not use implicit null label, you will see two labels in the data plane between R2 and R1. Top label will be LDP transport label, and bottom label will act as a service VPLS label.</p>","tags":["bgp","vpls","nokia","juniper"]},{"location":"2016/bgp-vpls-deep-dive-nokia-sr-os--juniper/#references-further-reading","title":"References &amp; further reading","text":"<ul> <li>RFC4761\u00a0Virtual Private LAN Service (VPLS) Using BGP for Auto-Discovery and Signaling</li> <li>RFC4448 Encapsulation Methods for Transport of Ethernet over MPLS Networks</li> <li>draft-vpls-multihoming BGP based Multi-homing in Virtual Private LAN Service</li> <li>Configuring VPLS on SR OS</li> <li>VPLS Services\u00a0in JunOS</li> <li>Configuring and troubleshooting BGP VPLS on Nokia SROS</li> <li>VPLS services explained (Nokia official doc)</li> <li>MPLS in the SDN Era</li> </ul>","tags":["bgp","vpls","nokia","juniper"]},{"location":"page/2/","title":"Index","text":""},{"location":"page/3/","title":"Index","text":""},{"location":"page/4/","title":"Index","text":""},{"location":"page/5/","title":"Index","text":""},{"location":"page/6/","title":"Index","text":""},{"location":"page/7/","title":"Index","text":""},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#alpine-linux","title":"alpine linux","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> </ul>"},{"location":"tags/#ansible","title":"ansible","text":"<ul> <li>SR Linux, JSON-RPC and Ansible</li> </ul>"},{"location":"tags/#apt","title":"apt","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#arista","title":"arista","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#aws","title":"aws","text":"<ul> <li>How to install python3 in Amazon Linux AMI</li> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Prepping up for and passing the AWS Certified Solution Architect - Associate</li> <li>Uploading multiple files to AWS S3 in parallel</li> </ul>"},{"location":"tags/#aws-lambda","title":"aws lambda","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> </ul>"},{"location":"tags/#bash","title":"bash","text":"<ul> <li>Formatting bash</li> </ul>"},{"location":"tags/#bgp","title":"bgp","text":"<ul> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#blogging","title":"blogging","text":"<ul> <li>Adding border to the logos</li> <li>Upscaling images</li> </ul>"},{"location":"tags/#bootstrap","title":"bootstrap","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#boto3","title":"boto3","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> </ul>"},{"location":"tags/#ceos","title":"ceos","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#certification","title":"certification","text":"<ul> <li>Prepping up for and passing the AWS Certified Solution Architect - Associate</li> <li>Prepping up for and passing the Certified Openstack Administrator Exam</li> </ul>"},{"location":"tags/#cisco","title":"cisco","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#cloudflare","title":"cloudflare","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> </ul>"},{"location":"tags/#containerlab","title":"containerlab","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>Test coverage for Go integration tests</li> </ul>"},{"location":"tags/#crpd","title":"crpd","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#dknog","title":"dknog","text":"<ul> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#docker","title":"docker","text":"<ul> <li>Flask application in a production-ready container</li> <li>Yang Explorer in a docker container based on Alpine</li> <li>NETCONF console in a docker container</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Using scrapligo with kubectl exec</li> <li>OpenStack Client Container Image</li> </ul>"},{"location":"tags/#documentation","title":"documentation","text":"<ul> <li>Projectdocs</li> </ul>"},{"location":"tags/#elk","title":"elk","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"tags/#eve-ng","title":"eve-ng","text":"<ul> <li>Using Wireshark remote capture with EVE-NG</li> </ul>"},{"location":"tags/#flask","title":"flask","text":"<ul> <li>Flask application in a production-ready container</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#frontend","title":"frontend","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> </ul>"},{"location":"tags/#frr","title":"frr","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#ftp","title":"ftp","text":"<ul> <li>Saturating the network with FTP</li> </ul>"},{"location":"tags/#fwd","title":"fwd","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#gcp","title":"gcp","text":"<ul> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#gcp-function","title":"gcp function","text":"<ul> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#gemfury","title":"gemfury","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#git","title":"git","text":"<ul> <li>How to count lines of code in a Git repo?</li> <li>Remove binaries and big files from Git repo</li> </ul>"},{"location":"tags/#gitlab","title":"gitlab","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> </ul>"},{"location":"tags/#gnmi","title":"gnmi","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMI Map</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>Easily exposing your local resources with ngrok and fwd</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> <li>Decoding gNMI with Wireshark</li> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#gnmic","title":"gnmic","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> <li>Decoding gNMI with Wireshark</li> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#go","title":"go","text":"<ul> <li>How to make VS Code Go extension to work in your cloud folder on different platforms?</li> <li>gNMIc - gNMI CLI client and collector</li> <li>Using scrapligo with kubectl exec</li> <li>Test coverage for Go integration tests</li> <li>Refreshing Go package index for your package</li> </ul>"},{"location":"tags/#goreleaser","title":"goreleaser","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"},{"location":"tags/#grpc","title":"grpc","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#guestfish","title":"guestfish","text":"<ul> <li>Using guestfish container image</li> </ul>"},{"location":"tags/#highlightjs","title":"highlightjs","text":"<ul> <li>How to add YAML highlight in Highlight.js?</li> </ul>"},{"location":"tags/#hugo","title":"hugo","text":"<ul> <li>Setting up a Hugo blog with GitLab and CloudFlare</li> <li>How to add YAML highlight in Highlight.js?</li> </ul>"},{"location":"tags/#javascript","title":"javascript","text":"<ul> <li>Creating a Bootstrap based front-end for your simple REST service</li> </ul>"},{"location":"tags/#json-rpc","title":"json-rpc","text":"<ul> <li>SR Linux, JSON-RPC and Ansible</li> </ul>"},{"location":"tags/#juniper","title":"juniper","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#kubernetes","title":"kubernetes","text":"<ul> <li>Using scrapligo with kubectl exec</li> </ul>"},{"location":"tags/#kvm","title":"kvm","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#l3vpn","title":"l3vpn","text":"<ul> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> </ul>"},{"location":"tags/#lacp","title":"lacp","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#ldp","title":"ldp","text":"<ul> <li>LDP. Ordered Label Distribution Control explained</li> </ul>"},{"location":"tags/#libvirt","title":"libvirt","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#logging","title":"logging","text":"<ul> <li>SR Linux logging with ELK</li> </ul>"},{"location":"tags/#m3u8","title":"m3u8","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#mkdocs-material","title":"mkdocs-material","text":"<ul> <li>Projectdocs</li> </ul>"},{"location":"tags/#mpls","title":"mpls","text":"<ul> <li>LDP. Ordered Label Distribution Control explained</li> </ul>"},{"location":"tags/#nanog","title":"nanog","text":"<ul> <li>gNMIc talks at DKNOG and NANOG</li> </ul>"},{"location":"tags/#netconf","title":"netconf","text":"<ul> <li>NETCONF console in a docker container</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Easily exposing your local resources with ngrok and fwd</li> <li>NETCONF subtree filtering by example</li> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#netrel","title":"netrel","text":"<ul> <li>Decoding gNMI with Wireshark</li> <li>DIY YANG Browser</li> </ul>"},{"location":"tags/#nginx","title":"nginx","text":"<ul> <li>Flask application in a production-ready container</li> </ul>"},{"location":"tags/#ngrok","title":"ngrok","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#nokia","title":"nokia","text":"<ul> <li>SR OS Rootifier or how to flatten 7750 SR config</li> <li>Nokia YANG tree and Path Browser</li> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> <li>Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#openconfig","title":"openconfig","text":"<ul> <li>Arista EOS gNMI Tutorial</li> <li>gNMI Map</li> <li>gNMIc got better with YANG-completions</li> <li>gNMIc - gNMI CLI client and collector</li> <li>gNMIc joins Openconfig \ud83d\ude80</li> </ul>"},{"location":"tags/#openstack","title":"openstack","text":"<ul> <li>Prepping up for and passing the Certified Openstack Administrator Exam</li> <li>OpenStack Client Container Image</li> </ul>"},{"location":"tags/#ospf","title":"ospf","text":"<ul> <li>OSPF. Neighbors on a \"point-to-broadcast\" network</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> </ul>"},{"location":"tags/#ovs","title":"ovs","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#paramiko","title":"paramiko","text":"<ul> <li>Waiting for SSH service to be ready with Paramiko</li> </ul>"},{"location":"tags/#pygments","title":"pygments","text":"<ul> <li>Creating a syntax highlighter for SR Linux CLI snippets</li> </ul>"},{"location":"tags/#python","title":"python","text":"<ul> <li>How to install python3 in Amazon Linux AMI</li> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Waiting for SSH service to be ready with Paramiko</li> <li>Creating Google Cloud Platform Function with Python and Serverless</li> <li>Building Web front end for Python scripts with Flask</li> </ul>"},{"location":"tags/#qemu","title":"qemu","text":"<ul> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#scrapli","title":"scrapli","text":"<ul> <li>Using scrapligo with kubectl exec</li> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#serverless","title":"serverless","text":"<ul> <li>Building AWS Lambda with Python, S3 and serverless</li> <li>Creating Google Cloud Platform Function with Python and Serverless</li> </ul>"},{"location":"tags/#sonic","title":"sonic","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> </ul>"},{"location":"tags/#sr-os","title":"sr os","text":"<ul> <li>SR OS Rootifier or how to flatten 7750 SR config</li> <li>Nokia YANG tree and Path Browser</li> <li>Nokia (Alcatel-Lucent) SROS OSPF configuration tutorial</li> <li>Nokia (Alcatel-Lucent). Configuring Packet (IP) Filters</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 1 - basic eBGP, iBGP</li> <li>Nokia (Alcatel-Lucent) BGP configuration tutorial. Part 2 - Communities</li> </ul>"},{"location":"tags/#srlinux","title":"srlinux","text":"<ul> <li>Containerlab - your network-centric labs with a Docker UX</li> <li>Nokia SR Linux goes public</li> <li>SR Linux, JSON-RPC and Ansible</li> <li>SR Linux logging with ELK</li> <li>Creating a syntax highlighter for SR Linux CLI snippets</li> </ul>"},{"location":"tags/#ssh","title":"ssh","text":"<ul> <li>Easily exposing your local resources with ngrok and fwd</li> </ul>"},{"location":"tags/#tc","title":"tc","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#testing","title":"testing","text":"<ul> <li>Test coverage for Go integration tests</li> </ul>"},{"location":"tags/#textfsm","title":"textfsm","text":"<ul> <li>Network automation options in Go with scrapligo</li> </ul>"},{"location":"tags/#ubuntu","title":"ubuntu","text":"<ul> <li>Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial</li> <li>How to patch Ubuntu 20.04 Focal Fossa with UKSM?</li> </ul>"},{"location":"tags/#uksm","title":"uksm","text":"<ul> <li>How to patch Ubuntu 20.04 Focal Fossa with UKSM?</li> </ul>"},{"location":"tags/#uwsgi","title":"uwsgi","text":"<ul> <li>Flask application in a production-ready container</li> </ul>"},{"location":"tags/#video","title":"video","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#virsh","title":"virsh","text":"<ul> <li>Changing Libvirt bridge attachment in a running domain aka on-the-fly</li> <li>Destroy and Undefine KVM VMs in a single run</li> </ul>"},{"location":"tags/#vpls","title":"vpls","text":"<ul> <li>BGP VPLS deep dive. Nokia SR OS &amp; Juniper</li> </ul>"},{"location":"tags/#vprn","title":"vprn","text":"<ul> <li>Basic L3VPN (BGP/MPLS VPN or VPRN) configuration on Nokia (Alcatel-Lucent) SROS &amp; Juniper MX</li> </ul>"},{"location":"tags/#vrnetlab","title":"vrnetlab","text":"<ul> <li>Transparently redirecting packets/frames between interfaces</li> </ul>"},{"location":"tags/#vs-code","title":"vs code","text":"<ul> <li>How to make VS Code Go extension to work in your cloud folder on different platforms?</li> </ul>"},{"location":"tags/#vsftpd","title":"vsftpd","text":"<ul> <li>Saturating the network with FTP</li> </ul>"},{"location":"tags/#wireshark","title":"wireshark","text":"<ul> <li>Using Wireshark remote capture with EVE-NG</li> </ul>"},{"location":"tags/#xrdp","title":"xrdp","text":"<ul> <li>Installing xrdp 0.9.1 on Ubuntu 16.04 Xenial</li> </ul>"},{"location":"tags/#yang","title":"yang","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> <li>Arista EOS gNMI Tutorial</li> <li>gNMIc got better with YANG-completions</li> <li>Getting XML data sample for a given leaf in a YANG model</li> <li>Nokia YANG tree and Path Browser</li> <li>DIY YANG Browser</li> </ul>"},{"location":"tags/#yang-explorer","title":"yang explorer","text":"<ul> <li>Yang Explorer in a docker container based on Alpine</li> </ul>"},{"location":"tags/#youtube-dl","title":"youtube-dl","text":"<ul> <li>How to download an M3U/M3U8 playlist (stream) without a special software</li> </ul>"},{"location":"tags/#yum","title":"yum","text":"<ul> <li>Building and publishing deb/rpm packages with goreleaser and gemfury</li> </ul>"}]}